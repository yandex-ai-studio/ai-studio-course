| OpenAlex ID | Title | Year | Venue | Type | Cited by | Open Access | Authors | Top concepts | Landing page | Abstract (reconstructed) |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| https://openalex.org/W4225672218 | Restormer: Efficient Transformer for High-Resolution Image Restoration | 2022 | 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) | article | 3060 | no | Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, Ming–Hsuan Yang | Deblurring, Computer science, Artificial intelligence, Image restoration, Computer vision, Transformer, +10 more | https://doi.org/10.1109/cvpr52688.2022.00564 | Since convolutional neural networks (CNNs) perform well at learning generalizable image priors from large-scale data, these models have been extensively applied to image restoration and related tasks. Recently, another class of neural architectures, Transformers, have shown significant performance gains on natural language and high-level vision tasks. While the Transformer model mitigates the shortcomings of CNNs (i.e., limited receptive field and inadaptability to input content), its computational complexity grows quadratically with the spatial resolution, therefore making it infeasible to apply to most image restoration tasks involving high-resolution images. In this work, we propose an efficient Transformer model by making several key designs in the building blocks (multi-head attention and feed-forward network) such that it can capture long-range pixel interactions, while still rema… |
| https://openalex.org/W2963854351 | Multi-Task Deep Neural Networks for Natural Language Understanding | 2019 |  | preprint | 1033 | yes | Xiaodong Liu, Pengcheng He, Weizhu Chen, Jianfeng Gao | Computer science, Domain adaptation, Transformer, Artificial intelligence, Benchmark (surveying), Natural language understanding, +20 more | https://doi.org/10.18653/v1/p19-1441 | In this paper, we present a Multi-Task Deep Neural Network (MT-DNN) for learning representations across multiple natural language understanding (NLU) tasks. MT-DNN not only leverages large amounts of cross-task data, but also benefits from a regularization effect that leads to more general representations to help adapt to new tasks and domains. MT-DNN extends the model proposed in Liu et al. (2015) by incorporating a pre-trained bidirectional transformer language model, known as BERT (Devlin et al., 2018). MT-DNN obtains new state-of-the-art results on ten NLU tasks, including SNLI, SciTail, and eight out of nine GLUE tasks, pushing the GLUE benchmark to 82.7% (2.2% absolute improvement) as of February 25, 2019 on the latest GLUE test set. We also demonstrate using the SNLI and SciTail datasets that the representations learned by MT-DNN allow domain adaptation with substantially fewer i… |
| https://openalex.org/W3096266342 | BERTimbau: Pretrained BERT Models for Brazilian Portuguese | 2020 | Lecture notes in computer science | book-chapter | 566 | no | Fábio Souza, Rodrigo Nogueira, Roberto Lotufo | Computer science, Natural language processing, Artificial intelligence, Transformer, Language model, Sentence, +15 more | https://doi.org/10.1007/978-3-030-61377-8_28 |  |
| https://openalex.org/W3136416617 | Incorporating Convolution Designs into Visual Transformers | 2021 | 2021 IEEE/CVF International Conference on Computer Vision (ICCV) | article | 539 | no | Kun Yuan, Shaopeng Guo, Ziwei Liu, Aojun Zhou, Fengwei Yu, Wei Wu | Transformer, Computer science, Convolutional neural network, Locality, Artificial intelligence, Encoder, +9 more | https://doi.org/10.1109/iccv48922.2021.00062 | Motivated by the success of Transformers in natural language processing (NLP) tasks, there emerge some attempts (e.g., ViT and DeiT) to apply Transformers to the vision domain. However, pure Transformer architectures often require a large amount of training data or extra supervision to obtain comparable performance with convolutional neural networks (CNNs). To overcome these limitations, we analyze the potential drawbacks when directly borrowing Transformer architectures from NLP. Then we propose a new Convolution-enhanced image Transformer (CeiT) which combines the advantages of CNNs in extracting low-level features, strengthening locality, and the advantages of Transformers in establishing long-range dependencies. Three modifications are made to the original Transformer: 1) instead of the straightforward tokenization from raw input images, we design an Image-to-Tokens (I2T) module tha… |
| https://openalex.org/W4384159609 | nnFormer: Volumetric Medical Image Segmentation via a 3D Transformer | 2023 | IEEE Transactions on Image Processing | article | 517 | no | Hong-Yu Zhou, Jiansen Guo, Yinghao Zhang, Xiaoguang Han, Lequan Yu, Liansheng Wang, Yizhou Yu | Computer science, Exploit, Transformer, Segmentation, Artificial intelligence, Convolutional neural network, +16 more | https://doi.org/10.1109/tip.2023.3293771 | Transformer, the model of choice for natural language processing, has drawn scant attention from the medical imaging community. Given the ability to exploit long-term dependencies, transformers are promising to help atypical convolutional neural networks to learn more contextualized visual representations. However, most of recently proposed transformer-based segmentation approaches simply treated transformers as assisted modules to help encode global context into convolutional representations. To address this issue, we introduce nnFormer (i.e., not-another transFormer), a 3D transformer for volumetric medical image segmentation. nnFormer not only exploits the combination of interleaved convolution and self-attention operations, but also introduces local and global volume-based self-attention mechanism to learn volume representations. Moreover, nnFormer proposes to use skip attention to… |
| https://openalex.org/W4313485929 | Large-scale chemical language representations capture molecular structure and properties | 2022 | Nature Machine Intelligence | article | 386 | no | Jerret Ross, Brian Belgodere, Vijil Chenthamarakshan, Inkit Padhi, Youssef Mroueh, Payel Das | Computer science, Chemical space, Language model, Artificial intelligence, Molecular graph, Transformer, +14 more | https://doi.org/10.1038/s42256-022-00580-7 |  |
| https://openalex.org/W2917128112 | BERT for Joint Intent Classification and Slot Filling | 2019 | arXiv (Cornell University) | preprint | 412 | yes | Chen Qian, Zhuo Zhu, Wen Wang | Joint (building), Computer science, History, Artificial intelligence, Engineering, Structural engineering | http://arxiv.org/abs/1902.10909 | Intent classification and slot filling are two essential tasks for natural language understanding. They often suffer from small-scale human-labeled training data, resulting in poor generalization capability, especially for rare words. Recently a new language representation model, BERT (Bidirectional Encoder Representations from Transformers), facilitates pre-training deep bidirectional representations on large-scale unlabeled corpora, and has created state-of-the-art models for a wide variety of natural language processing tasks after simple fine-tuning. However, there has not been much effort on exploring BERT for natural language understanding. In this work, we propose a joint intent classification and slot filling model based on BERT. Experimental results demonstrate that our proposed model achieves significant improvement on intent classification accuracy, slot filling F1, and sente… |
| https://openalex.org/W3174864715 | R-Drop: Regularized Dropout for Neural Networks | 2021 | arXiv (Cornell University) | preprint | 306 | yes | Xiaobo Liang, Lijun Wu, Juntao Li, Yue Wang, Qi Meng, Tao Qin, Wei Chen, Min Zhang, +1 more | Computer science, Machine translation, Transformer, Artificial neural network, Dropout (neural networks), Automatic summarization, +17 more | http://arxiv.org/abs/2106.14448 | Dropout is a powerful and widely used technique to regularize the training of deep neural networks. In this paper, we introduce a simple regularization strategy upon dropout in model training, namely R-Drop, which forces the output distributions of different sub models generated by dropout to be consistent with each other. Specifically, for each training sample, R-Drop minimizes the bidirectional KL-divergence between the output distributions of two sub models sampled by dropout. Theoretical analysis reveals that R-Drop reduces the freedom of the model parameters and complements dropout. Experiments on $\bf{5}$ widely used deep learning tasks ($\bf{18}$ datasets in total), including neural machine translation, abstractive summarization, language understanding, language modeling, and image classification, show that R-Drop is universally effective. In particular, it yields substantial imp… |
| https://openalex.org/W3206996142 | SSAST: Self-Supervised Audio Spectrogram Transformer | 2022 | Proceedings of the AAAI Conference on Artificial Intelligence | article | 233 | yes | Yuan Gong, Cheng-I Lai, Yu-An Chung, James Glass | Spectrogram, Computer science, Discriminative model, Speech recognition, Transformer, Artificial intelligence, +8 more | https://doi.org/10.1609/aaai.v36i10.21315 | Recently, neural networks based purely on self-attention, such as the Vision Transformer (ViT), have been shown to outperform deep learning models constructed with convolutional neural networks (CNNs) on various vision tasks, thus extending the success of Transformers, which were originally developed for language processing, to the vision domain. A recent study showed that a similar methodology can also be applied to the audio domain. Specifically, the Audio Spectrogram Transformer (AST) achieves state-of-the-art results on various audio classification benchmarks. However, pure Transformer models tend to require more training data compared to CNNs, and the success of the AST relies on supervised pretraining that requires a large amount of labeled data and a complex training pipeline, thus limiting the practical usage of AST. This paper focuses on audio and speech classification, and aim… |
| https://openalex.org/W2981857663 | Transformer-Based Acoustic Modeling for Hybrid Speech Recognition | 2020 | arXiv (Cornell University) | article | 253 | yes | Yongqiang Wang, Abdelrahman Mohamed, Dieu Ngan Le, Chunxi Liu, Alex Xiao, Jay Mahadeokar, Hongzhao Huang, Andros Tjandra, +5 more | Transformer, Computer science, Language model, Embedding, Speech recognition, Hidden Markov model, +9 more | http://arxiv.org/abs/1910.09799 | We propose and evaluate transformer-based acoustic models (AMs) for hybrid\nspeech recognition. Several modeling choices are discussed in this work,\nincluding various positional embedding methods and an iterated loss to enable\ntraining deep transformers. We also present a preliminary study of using\nlimited right context in transformer models, which makes it possible for\nstreaming applications. We demonstrate that on the widely used Librispeech\nbenchmark, our transformer-based AM outperforms the best published hybrid\nresult by 19% to 26% relative when the standard n-gram language model (LM) is\nused. Combined with neural network LM for rescoring, our proposed approach\nachieves state-of-the-art results on Librispeech. Our findings are also\nconfirmed on a much larger internal dataset.\n |
| https://openalex.org/W4401726555 | A Survey on Deep Neural Network Pruning: Taxonomy, Comparison, Analysis, and Recommendations | 2024 | IEEE Transactions on Pattern Analysis and Machine Intelligence | article | 266 | no | Hongrong Cheng, Miao Zhang, Qinfeng Shi | Artificial intelligence, Computer science, Taxonomy (biology), Artificial neural network, Machine learning, Pattern recognition (psychology), +2 more | https://doi.org/10.1109/tpami.2024.3447085 | Modern deep neural networks, particularly recent large language models, come with massive model sizes that require significant computational and storage resources. To enable the deployment of modern models on resource-constrained environments and to accelerate inference time, researchers have increasingly explored pruning techniques as a popular research direction in neural network compression. More than three thousand pruning papers have been published from 2020 to 2024. However, there is a dearth of up-to-date comprehensive review papers on pruning. To address this issue, in this survey, we provide a comprehensive review of existing research works on deep neural network pruning in a taxonomy of 1) universal/specific speedup, 2) when to prune, 3) how to prune, and 4) fusion of pruning and other compression techniques. We then provide a thorough comparative analysis of eight pairs of co… |
| https://openalex.org/W4298110867 | <scp>FinBERT</scp>: A Large Language Model for Extracting Information from Financial Text* | 2022 | Contemporary Accounting Research | article | 525 | yes | Allen Huang, Hui Wang, Yi Yang | Computer science, Artificial intelligence, Machine learning, Finance, Encoder, Natural language processing, +9 more | https://doi.org/10.1111/1911-3846.12832 | ABSTRACT We develop FinBERT, a state‐of‐the‐art large language model that adapts to the finance domain. We show that FinBERT incorporates finance knowledge and can better summarize contextual information in financial texts. Using a sample of researcher‐labeled sentences from analyst reports, we document that FinBERT substantially outperforms the Loughran and McDonald dictionary and other machine learning algorithms, including naïve Bayes, support vector machine, random forest, convolutional neural network, and long short‐term memory, in sentiment classification. Our results show that FinBERT excels in identifying the positive or negative sentiment of sentences that other algorithms mislabel as neutral, likely because it uses contextual information in financial text. We find that FinBERT's advantage over other algorithms, and Google's original bidirectional encoder representations from t… |
| https://openalex.org/W4391582407 | Revolutionizing Cyber Threat Detection With Large Language Models: A Privacy-Preserving BERT-Based Lightweight Model for IoT/IIoT Devices | 2024 | IEEE Access | article | 187 | yes | Mohamed Amine Ferrag, Mthandazo Ndhlovu, Norbert Tihanyi, Lucas C. Cordeiro, Mérouane Debbah, Thierry Lestable, Narinderjit Singh Thandi | Computer science, Deep learning, Artificial intelligence, Encoder, Convolutional neural network, Autoencoder, +12 more | https://doi.org/10.1109/access.2024.3363469 | The field of Natural Language Processing (NLP) is currently undergoing a revolutionary transformation driven by the power of pre-trained Large Language Models (LLMs) based on groundbreaking Transformer architectures. As the frequency and diversity of cybersecurity attacks continue to rise, the importance of incident detection has significantly increased. IoT devices are expanding rapidly, resulting in a growing need for efficient techniques to autonomously identify network-based attacks in IoT networks with both high precision and minimal computational requirements. This paper presents SecurityBERT, a novel architecture that leverages the Bidirectional Encoder Representations from Transformers (BERT) model for cyber threat detection in IoT networks. During the training of SecurityBERT, we incorporated a novel privacy-preserving encoding technique called Privacy-Preserving Fixed-Length E… |
| https://openalex.org/W3198035652 | nnFormer: Interleaved Transformer for Volumetric Segmentation | 2021 | arXiv (Cornell University) | preprint | 261 | yes | Hong-Yu Zhou, Jiansen Guo, Yinghao Zhang, Lequan Yu, Liansheng Wang, Yizhou Yu | Exploit, Computer science, Transformer, Segmentation, ENCODE, Convolutional neural network, +15 more | http://arxiv.org/abs/2109.03201 | Transformer, the model of choice for natural language processing, has drawn scant attention from the medical imaging community. Given the ability to exploit long-term dependencies, transformers are promising to help atypical convolutional neural networks to overcome their inherent shortcomings of spatial inductive bias. However, most of recently proposed transformer-based segmentation approaches simply treated transformers as assisted modules to help encode global context into convolutional representations. To address this issue, we introduce nnFormer, a 3D transformer for volumetric medical image segmentation. nnFormer not only exploits the combination of interleaved convolution and self-attention operations, but also introduces local and global volume-based self-attention mechanism to learn volume representations. Moreover, nnFormer proposes to use skip attention to replace the tradit… |
| https://openalex.org/W3212228063 | Restormer: Efficient Transformer for High-Resolution Image Restoration | 2021 | arXiv (Cornell University) | preprint | 191 | yes | Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, Ming–Hsuan Yang | Deblurring, Computer science, Artificial intelligence, Image restoration, Transformer, Computer vision, +9 more | http://arxiv.org/abs/2111.09881 | Since convolutional neural networks (CNNs) perform well at learning generalizable image priors from large-scale data, these models have been extensively applied to image restoration and related tasks. Recently, another class of neural architectures, Transformers, have shown significant performance gains on natural language and high-level vision tasks. While the Transformer model mitigates the shortcomings of CNNs (i.e., limited receptive field and inadaptability to input content), its computational complexity grows quadratically with the spatial resolution, therefore making it infeasible to apply to most image restoration tasks involving high-resolution images. In this work, we propose an efficient Transformer model by making several key designs in the building blocks (multi-head attention and feed-forward network) such that it can capture long-range pixel interactions, while still rema… |
| https://openalex.org/W4390919701 | Chatbots and Large Language Models in Radiology: A Practical Primer for Clinical and Research Applications | 2024 | Radiology | review | 259 | no | Rajesh Bhayana | Medicine, Transformative learning, Biomedicine, Artificial intelligence, Computer science, Bioinformatics, +3 more | https://doi.org/10.1148/radiol.232756 | Although chatbots have existed for decades, the emergence of transformer-based large language models (LLMs) has captivated the world through the most recent wave of artificial intelligence chatbots, including ChatGPT. Transformers are a type of neural network architecture that enables better contextual understanding of language and efficient training on massive amounts of unlabeled data, such as unstructured text from the internet. As LLMs have increased in size, their improved performance and emergent abilities have revolutionized natural language processing. Since language is integral to human thought, applications based on LLMs have transformative potential in many industries. In fact, LLM-based chatbots have demonstrated human-level performance on many professional benchmarks, including in radiology. LLMs offer numerous clinical and research applications in radiology, several of whi… |
| https://openalex.org/W2979860911 | Target-Dependent Sentiment Classification With BERT | 2019 | IEEE Access | article | 407 | yes | Zhengjie Gao, Ao Feng, Xinyu Song, Xi Wu | Computer science, Sentiment analysis, Feature engineering, Sentence, Artificial intelligence, Natural language processing, +19 more | https://doi.org/10.1109/access.2019.2946594 | Research on machine assisted text analysis follows the rapid development of digital media, and sentiment analysis is among the prevalent applications. Traditional sentiment analysis methods require complex feature engineering, and embedding representations have dominated leaderboards for a long time. However, the context-independent nature limits their representative power in rich context, hurting performance in Natural Language Processing (NLP) tasks. Bidirectional Encoder Representations from Transformers (BERT), among other pre-trained language models, beats existing best results in eleven NLP tasks (including sentence-level sentiment classification) by a large margin, which makes it the new baseline of text representation. As a more challenging task, fewer applications of BERT have been observed for sentiment classification at the aspect level. We implement three target-dependent va… |
| https://openalex.org/W4386290290 | A Survey of Controllable Text Generation Using Transformer-based Pre-trained Language Models | 2023 | ACM Computing Surveys | review | 254 | no | Hanqing Zhang, Haolin Song, Shaoyu Li, Ming Zhou, Dawei Song | Computer science, Interpretability, Transformer, Natural language generation, Artificial intelligence, Data science, +5 more | https://doi.org/10.1145/3617680 | Controllable Text Generation (CTG) is an emerging area in the field of natural language generation (NLG). It is regarded as crucial for the development of advanced text generation technologies that better meet the specific constraints in practical applications. In recent years, methods using large-scale pre-trained language models (PLMs), in particular the widely used Transformer-based PLMs, have become a new paradigm of NLG, allowing generation of more diverse and fluent text. However, due to the limited level of interpretability of deep neural networks, the controllability of these methods needs to be guaranteed. To this end, controllable text generation using Transformer-based PLMs has become a rapidly growing yet challenging new research hotspot. A diverse range of approaches have emerged in the past 3 to 4 years, targeting different CTG tasks that require different types of control… |
| https://openalex.org/W3047848469 | FTRANS | 2020 |  | article | 179 | no | Bingbing Li, Santosh Pandey, Haowen Fang, Yanjun Lyv, Ji Li, Jieyang Chen, Mimi Xie, Lipeng Wan, +2 more | Computer science, Field-programmable gate array, Transformer, Computation, Parallel computing, Recurrent neural network, +15 more | https://doi.org/10.1145/3370748.3406567 | In natural language processing (NLP), the "Transformer" architecture was proposed as the first transduction model replying entirely on self-attention mechanisms without using sequence-aligned recurrent neural networks (RNNs) or convolution, and it achieved significant improvements for sequence to sequence tasks. The introduced intensive computation and storage of these pre-trained language representations has impeded their popularity into computation and memory constrained devices. The field-programmable gate array (FPGA) is widely used to accelerate deep learning algorithms for its high parallelism and low latency. However, the trained models are still too large to accommodate to an FPGA fabric. In this paper, we propose an efficient acceleration framework, Ftrans, for transformer-based large scale language representations. Our framework includes enhanced block-circulant matrix (BCM)-b… |
| https://openalex.org/W2965570621 | Pre-training of Graph Augmented Transformers for Medication Recommendation | 2019 |  | preprint | 288 | yes | Junyuan Shang, Tengfei Ma, Cao Xiao, Jimeng Sun | Computer science, Encoder, Machine learning, Artificial intelligence, Transformer, Inductive bias, +16 more | https://doi.org/10.24963/ijcai.2019/825 | Medication recommendation is an important healthcare application. It is commonly formulated as a temporal prediction task. Hence, most existing works only utilize longitudinal electronic health records (EHRs) from a small number of patients with multiple visits ignoring a large number of patients with a single visit (selection bias). Moreover, important hierarchical knowledge such as diagnosis hierarchy is not leveraged in the representation learning process. Despite the success of deep learning techniques in computational phenotyping, most previous approaches have two limitations: task-oriented representation and ignoring hierarchies of medical codes. To address these challenges, we propose G-BERT, a new model to combine the power of Graph Neural Networks (GNNs) and BERT (Bidirectional Encoder Representations from Transformers) for medical code representation and medication recommendat… |
| https://openalex.org/W3095173472 | ContextNet: Improving Convolutional Neural Networks for Automatic Speech Recognition with Global Context | 2020 |  | article | 248 | no | Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung‐Cheng Chiu, James Qin, Anmol Gulati, Ruoming Pang, +1 more | Computer science, Convolutional neural network, Speech recognition, Context (archaeology), Artificial intelligence, Natural language processing, +2 more | https://doi.org/10.21437/interspeech.2020-2059 | Convolutional neural networks (CNN) have shown promising results for end-to-end speech recognition, albeit still behind RNN/transformer based models in performance.In this paper, we study how to bridge this gap and go beyond with a novel CNN-RNN-transducer architecture, which we call ContextNet.ContextNet features a fully convolutional encoder that incorporates global context information into convolution layers by adding squeeze-and-excitation modules.In addition, we propose a simple scaling method that scales the widths of Con-textNet that achieves good trade-off between computation and accuracy.We demonstrate that on the widely used Librispeech benchmark, ContextNet achieves a word error rate (WER) of 2.1%/4.6%without external language model (LM), 1.9%/4.1% with LM and 2.9%/7.0%with only 10M parameters on the clean/noisy LibriSpeech test sets.This compares to the best previously publi… |
| https://openalex.org/W2914526845 | Multi-Task Deep Neural Networks for Natural Language Understanding | 2019 | arXiv (Cornell University) | preprint | 220 | yes | Xiaodong Liu, Pengcheng He, Weizhu Chen, Jianfeng Gao | Computer science, Domain adaptation, Transformer, Artificial intelligence, Task (project management), Benchmark (surveying), +21 more | http://arxiv.org/abs/1901.11504 | In this paper, we present a Multi-Task Deep Neural Network (MT-DNN) for learning representations across multiple natural language understanding (NLU) tasks. MT-DNN not only leverages large amounts of cross-task data, but also benefits from a regularization effect that leads to more general representations in order to adapt to new tasks and domains. MT-DNN extends the model proposed in Liu et al. (2015) by incorporating a pre-trained bidirectional transformer language model, known as BERT (Devlin et al., 2018). MT-DNN obtains new state-of-the-art results on ten NLU tasks, including SNLI, SciTail, and eight out of nine GLUE tasks, pushing the GLUE benchmark to 82.7% (2.2% absolute improvement). We also demonstrate using the SNLI and SciTail datasets that the representations learned by MT-DNN allow domain adaptation with substantially fewer in-domain labels than the pre-trained BERT repres… |
| https://openalex.org/W4313453502 | Rapamycin in the context of Pascal’s Wager: generative pre-trained transformer perspective | 2022 | Oncoscience | article | 203 | yes | ChatGPT Generative Pre-trained Transformer, Alex Zhavoronkov | Pascal (unit), Computer science, Generative grammar, Transformer, Cognitive science, Artificial intelligence, +5 more | https://doi.org/10.18632/oncoscience.571 | Large language models utilizing transformer neural networks and other deep learning architectures demonstrated unprecedented results in many tasks previously accessible only to human intelligence. In this article, we collaborate with ChatGPT, an AI model developed by OpenAI to speculate on the applications of Rapamycin, in the context of Pascal's Wager philosophical argument commonly utilized to justify the belief in god. In response to the query "Write an exhaustive research perspective on why taking Rapamycin may be more beneficial than not taking Rapamycin from the perspective of Pascal's wager" ChatGPT provided the pros and cons for the use of Rapamycin considering the preclinical evidence of potential life extension in animals. This article demonstrates the potential of ChatGPT to produce complex philosophical arguments and should not be used for any off-label use of Rapamycin. |
| https://openalex.org/W3058507927 | Comparing deep learning architectures for sentiment analysis on drug reviews | 2020 | Journal of Biomedical Informatics | article | 173 | yes | Cristobal Colón-Ruíz, Isabel Segura-Bédmar | Computer science, Deep learning, Artificial intelligence, Natural language processing, Sentiment analysis, Machine learning, +1 more | https://doi.org/10.1016/j.jbi.2020.103539 |  |
| https://openalex.org/W2970641574 | Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks | 2019 |  | article | 9707 | yes | Nils Reimers, Iryna Gurevych | Sentence, Computer science, Artificial intelligence, Natural language processing | https://doi.org/10.18653/v1/d19-1410 | Nils Reimers, Iryna Gurevych. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019. |
| https://openalex.org/W4285586654 | jTrans: jump-aware transformer for binary code similarity detection | 2022 |  | article | 130 | yes | Hao Wang, Wenjie Qu, Gilad Katz, Wenyu Zhu, Zeyu Gao, Han Qiu, Jianwei Zhuge, Chao Zhang | Computer science, Transformer, Binary number, Binary code, Control flow, Source code, +14 more | https://doi.org/10.1145/3533767.3534367 | Binary code similarity detection (BCSD) has important applications in various fields such as vulnerabilities detection, software component analysis, and reverse engineering. Recent studies have shown that deep neural networks (DNNs) can comprehend instructions or control-flow graphs (CFG) of binary code and support BCSD. In this study, we propose a novel Transformer-based approach, namely jTrans, to learn representations of binary code. It is the first solution that embeds control flow information of binary code into Transformer-based language models, by using a novel jump-aware representation of the analyzed binaries and a newly-designed pre-training task. Additionally, we release to the community a newly-created large dataset of binaries, BinaryCorp, which is the most diverse to date. Evaluation results show that jTrans outperforms state-of-the-art (SOTA) approaches on this more chall… |
| https://openalex.org/W3204146347 | MIL-VT: Multiple Instance Learning Enhanced Vision Transformer for Fundus Image Classification | 2021 | Lecture notes in computer science | book-chapter | 128 | no | Shuang Yu, Kai Ma, Qi Bi, Cheng Bian, Munan Ning, Nanjun He, Yuexiang Li, Hanruo Liu, +1 more | Computer science, Artificial intelligence, Fundus (uterus), Computer vision, Transformer, Contextual image classification, +7 more | https://doi.org/10.1007/978-3-030-87237-3_5 |  |
| https://openalex.org/W4391514919 | Advantages of transformer and its application for medical image segmentation: a survey | 2024 | BioMedical Engineering OnLine | article | 88 | yes | Qiumei Pu, Zuoxin Xi, Shuai Yin, Zhe Zhao, Lina Zhao | Computer science, Segmentation, Transformer, Artificial intelligence, Image segmentation, Convolutional neural network, +5 more | https://doi.org/10.1186/s12938-024-01212-4 | Abstract Purpose Convolution operator-based neural networks have shown great success in medical image segmentation over the past decade. The U-shaped network with a codec structure is one of the most widely used models. Transformer, a technology used in natural language processing, can capture long-distance dependencies and has been applied in Vision Transformer to achieve state-of-the-art performance on image classification tasks. Recently, researchers have extended transformer to medical image segmentation tasks, resulting in good models. Methods This review comprises publications selected through a Web of Science search. We focused on papers published since 2018 that applied the transformer architecture to medical image segmentation. We conducted a systematic analysis of these studies and summarized the results. Results To better comprehend the benefits of convolutional neural networ… |
| https://openalex.org/W3101233295 | Text Graph Transformer for Document Classification | 2020 |  | article | 67 | yes | Haopeng Zhang, Jiawei Zhang | Computer science, Transformer, Artificial intelligence, Scalability, Graph, Text graph, +7 more | https://doi.org/10.18653/v1/2020.emnlp-main.668 | Text classification is a fundamental problem in natural language processing. Recent studies applied graph neural network (GNN) techniques to capture global word co-occurrence in a corpus. However, previous works are not scalable to large-sized corpus and ignore the heterogeneity of the text graph. To address these problems, we introduce a novel Transformer based heterogeneous graph neural network, namely Text Graph Transformer (TG-Transformer). Our model learns effective node representations by capturing structure and heterogeneity from the text graph. We propose a mini-batch text graph sampling method that significantly reduces computing and memory costs to handle large-sized corpus. Extensive experiments have been conducted on several benchmark datasets, and the results demonstrate that TG-Transformer outperforms state-of-the-art approaches on text classification task. |
| https://openalex.org/W4285018316 | Analyzing Transfer Learning of Vision Transformers for Interpreting Chest Radiography | 2022 | Journal of Digital Imaging | article | 82 | yes | Muhammad Usman, Tehseen Zia, Syed Ali Tariq | Radiography, Medical physics, Computer science, Artificial intelligence, Medicine, Computer vision, +1 more | https://doi.org/10.1007/s10278-022-00666-z | Limited availability of medical imaging datasets is a vital limitation when using "data hungry" deep learning to gain performance improvements. Dealing with the issue, transfer learning has become a de facto standard, where a pre-trained convolution neural network (CNN), typically on natural images (e.g., ImageNet), is finetuned on medical images. Meanwhile, pre-trained transformers, which are self-attention-based models, have become de facto standard in natural language processing (NLP) and state of the art in image classification due to their powerful transfer learning abilities. Inspired by the success of transformers in NLP and image classification, large-scale transformers (such as vision transformer) are trained on natural images. Based on these recent developments, this research aims to explore the efficacy of pre-trained natural image transformers for medical images. Specificall… |
| https://openalex.org/W3122838366 | Semantic Re-tuning with Contrastive Tension | 2021 | KTH Publication Database DiVA (KTH Royal Institute of Technology) | article | 86 | yes | Fredrik Carlsson, Magnus Sahlgren, Evangelia Gogoulou, Amaru Cuba Gyllensten, Erik Ylipää Hellqvist | Computer science, Transformer, Artificial intelligence, Natural language processing, Sentence, Feature extraction, +9 more | http://urn.kb.se/resolve?urn=urn:nbn:se:ri:diva-59816 | Extracting semantically useful natural language sentence representations frompre-trained deep neural networks such as Transformers remains a challenge. Wefirst demonstrate that pre-training objectives impose a significant task bias ontothe final layers of models, with a layer-wise survey of the Semantic Textual Similarity (STS) correlations for multiple common Transformer language models. Wethen propose a new self-supervised method called Contrastive Tension (CT) tocounter such biases. CT frames the training objective as a noise-contrastive taskbetween the final layer representations of two independent models, in turn makingthe final layer representations suitable for feature extraction. Results from multiple common unsupervised and supervised STS tasks indicate that CT outperformsprevious State Of The Art (SOTA), and when combining CT with supervised datawe improve upon previous SOTA r… |
| https://openalex.org/W2922565841 | Linguistic Knowledge and Transferability of Contextual Representations | 2019 |  | preprint | 57 | yes | Nelson F. Liu, Matt Gardner, Yonatan Belinkov, Matthew E. Peters, Noah A. Smith | Computer science, Transferability, Transformer, Natural language processing, Language model, Artificial intelligence, +11 more | https://doi.org/10.18653/v1/n19-1112 | Contextual word representations derived from large-scale neural language models are successful across a diverse set of NLP tasks, suggesting that they encode useful and transferable features of language. To shed light on the linguistic knowledge they capture, we study the representations produced by several recent pretrained contextualizers (variants of ELMo, the OpenAI transformer language model, and BERT) with a suite of seventeen diverse probing tasks. We find that linear models trained on top of frozen contextual representations are competitive with state-of-the-art task-specific models in many cases, but fail on tasks requiring fine-grained linguistic knowledge (e.g., conjunct identification). To investigate the transferability of contextual word representations, we quantify differences in the transferability of individual layers within contextualizers, especially between recurrent… |
| https://openalex.org/W4386434955 | Transformer for object detection: Review and benchmark | 2023 | Engineering Applications of Artificial Intelligence | article | 91 | yes | Yong Li, Naipeng Miao, Liangdi Ma, Feng Shuang, Xingwen Huang | Computer science, Transformer, Object detection, Convolutional neural network, Artificial intelligence, Benchmarking, +12 more | https://doi.org/10.1016/j.engappai.2023.107021 | Object detection is a crucial task in computer vision (CV). With the rapid advancement of Transformer-based models in natural language processing (NLP) and various visual tasks, Transformer structures are becoming increasingly prevalent in CV tasks. In recent years, numerous Transformer-based object detectors have been proposed, achieving performance comparable to mainstream convolutional neural network-based (CNN-based) approaches. To provide researchers with a comprehensive understanding of the development, advantages, disadvantages, and future potential of Transformer-based object detectors in Artificial Intelligence (AI), this paper systematically reviews the mainstream methods and analyzes the limitations and challenges encountered in their current applications, while also offering insights into future research directions. We have reviewed a large number of papers, selected the mos… |
| https://openalex.org/W4385800872 | Masked Vision Transformers for Hyperspectral Image Classification | 2023 |  | article | 72 | no | Linus Scheibenreif, Michael Mommert, Damian Borth | Hyperspectral imaging, Computer science, Artificial intelligence, Convolutional neural network, Transformer, Pattern recognition (psychology), +5 more | https://doi.org/10.1109/cvprw59228.2023.00210 | Transformer architectures have become state-of-the-art models in computer vision and natural language processing. To a significant degree, their success can be attributed to self-supervised pre-training on large scale unlabeled datasets. This work investigates the use of self-supervised masked image reconstruction to advance transformer models for hyperspectral remote sensing imagery. To facilitate self-supervised pre-training, we build a large dataset of unlabeled hyperspectral observations from the EnMAP satellite and systematically investigate modifications of the vision transformer architecture to optimally leverage the characteristics of hyperspectral data. We find significant improvements in accuracy on different land cover classification tasks over both standard vision and sequence transformers using (i) blockwise patch embeddings, (ii) spatialspectral self-attention, (iii) spect… |
| https://openalex.org/W4223591627 | Transformer-based deep learning models for the sentiment analysis of social media data | 2022 | Array | article | 179 | yes | Sayyida Tabinda Kokab, Sohail Asghar, Shehneela Naz | Computer science, Sentiment analysis, Artificial intelligence, Word2vec, Natural language processing, Transformer, +10 more | https://doi.org/10.1016/j.array.2022.100157 | Sentiment analysis (SA) is a widely used contextual mining technique for extracting useful and subjective information from text-based data. It applies on Natural Language Processing (NLP), text analysis, biometrics, and computational linguistics to identify, analyse, and extract responses, states, or emotions from the data. The features analysis technique plays a significant role in the development and improvement of a SA model. Recently, GloVe and Word2vec embedding models have been widely used for feature extractions. However, they overlook sentimental and contextual information of the text and need a large corpus of text data for training and generating exact vectors. These techniques generate vectors for just those words that are included in their vocabulary and ignore Out of Vocabulary Words (OOV), which can lead to information loss. Another challenge for the classification of sent… |
| https://openalex.org/W4386997414 | Semantic understanding and prompt engineering for large-scale traffic data imputation | 2023 | Information Fusion | article | 77 | no | Kunpeng Zhang, Feng Zhou, Lan Wu, Na Xie, Zhengbing He | Computer science, Imputation (statistics), Data mining, Missing data, Semantic data model, Artificial intelligence, +1 more | https://doi.org/10.1016/j.inffus.2023.102038 |  |
| https://openalex.org/W4390937897 | Ten years of generative adversarial nets (GANs): a survey of the state-of-the-art | 2024 | Machine Learning Science and Technology | article | 133 | yes | Tanujit Chakraborty, Ujjwal Reddy K S, Shraddha M. Naik, Madhurima Panja, Bayapureddy Manvitha | Generative grammar, Computer science, Discriminative model, Minimax, Generative adversarial network, Adversarial system, +12 more | https://doi.org/10.1088/2632-2153/ad1f77 | Abstract Generative adversarial networks (GANs) have rapidly emerged as powerful tools for generating realistic and diverse data across various domains, including computer vision and other applied areas, since their inception in 2014. Consisting of a discriminative network and a generative network engaged in a minimax game, GANs have revolutionized the field of generative modeling. In February 2018, GAN secured the leading spot on the ‘Top Ten Global Breakthrough Technologies List’ issued by the Massachusetts Science and Technology Review. Over the years, numerous advancements have been proposed, leading to a rich array of GAN variants, such as conditional GAN, Wasserstein GAN, cycle-consistent GAN, and StyleGAN, among many others. This survey aims to provide a general overview of GANs, summarizing the latent architecture, validation metrics, and application areas of the most widely rec… |
| https://openalex.org/W4206081398 | Sentiment analysis on Bangla text using extended lexicon dictionary and deep learning algorithms | 2022 | Array | article | 94 | yes | Nitish Ranjan Bhowmik, Mohammad Arifuzzaman, M. Rubaiyat Hossain Mondal | Computer science, Bengali, Word2vec, Artificial intelligence, Sentiment analysis, Word embedding, +12 more | https://doi.org/10.1016/j.array.2021.100123 | Sentiment analysis (SA) is a subset of natural language processing (NLP) research. In the case of categorical weighted based dictionary with rule-based sentiment score generation, no work in SA has been done yet in Bangla language using deep learning (DL) approaches. This paper proposes DL models for SA on Bangla text using an extended lexicon data dictionary (LDD). We implement the rule-based method Bangla text sentiment score (BTSC) algorithm for extracting polarity from large texts. These polarities are then fed into the neural network along with the preprocessed text as training samples. The preprocessed texts are formatted as a vectorization of words of unique numbers of pre-trained word embedding models. Word2Vec matrix with top highest probability word is applied on embedding layer as a weighted matrix to fit the DL models. This paper also presents a remarkably detailed analysis… |
| https://openalex.org/W3177318507 | Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting | 2021 | Proceedings of the AAAI Conference on Artificial Intelligence | article | 5207 | yes | Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, Wancai Zhang | Transformer, Computer science, Encoder, Sequence (biology), Dependency (UML), Algorithm, +7 more | https://doi.org/10.1609/aaai.v35i12.17325 | Many real-world applications require the prediction of long sequence time-series, such as electricity consumption planning. Long sequence time-series forecasting (LSTF) demands a high prediction capacity of the model, which is the ability to capture precise long-range dependency coupling between output and input efficiently. Recent studies have shown the potential of Transformer to increase the prediction capacity. However, there are several severe issues with Transformer that prevent it from being directly applicable to LSTF, including quadratic time complexity, high memory usage, and inherent limitation of the encoder-decoder architecture. To address these issues, we design an efficient transformer-based model for LSTF, named Informer, with three distinctive characteristics: (i) a ProbSparse self-attention mechanism, which achieves O(L log L) in time complexity and memory usage, and h… |
| https://openalex.org/W3157700644 | Adaptive Semiparametric Language Models | 2021 | Transactions of the Association for Computational Linguistics | article | 54 | yes | Dani Yogatama, Cyprien de Masson d’Autume, Lingpeng Kong | Computer science, Language model, Artificial intelligence, Parametric statistics, Transformer, Term (time), +8 more | https://doi.org/10.1162/tacl_a_00371 | Abstract We present a language model that combines a large parametric neural network (i.e., a transformer) with a non-parametric episodic memory component in an integrated architecture. Our model uses extended short-term context by caching local hidden states—similar to transformer-XL—and global long-term memory by retrieving a set of nearest neighbor tokens at each timestep. We design a gating function to adaptively combine multiple information sources to make a prediction. This mechanism allows the model to use either local context, short-term memory, or long-term memory (or any combination of them) on an ad hoc basis depending on the context. Experiments on word-based and character-based language modeling datasets demonstrate the efficacy of our proposed method compared to strong baselines. |
| https://openalex.org/W4401324765 | Data-driven stock forecasting models based on neural networks: A review | 2024 | Information Fusion | review | 63 | yes | Wuzhida Bao, Yuting Cao, Yin Yang, Hangjun Che, Junjian Huang, Shiping Wen | Computer science, Artificial neural network, Stock market, Artificial intelligence, Stock market prediction, Stock (firearms), +7 more | https://doi.org/10.1016/j.inffus.2024.102616 | As a core branch of financial forecasting, stock forecasting plays a crucial role for financial analysts, investors, and policymakers in managing risks and optimizing investment strategies, significantly enhancing the efficiency and effectiveness of economic decision-making. With the rapid development of information technology and computer science, data-driven neural network technologies have increasingly become the mainstream method for stock forecasting. Although recent review studies have provided a basic introduction to deep learning methods, they still lack detailed discussion on network architecture design and innovative details. Additionally, the latest research on emerging large language models and neural network structures has yet to be included in existing review literature. In light of this, this paper comprehensively reviews the literature on data-driven neural networks in t… |
| https://openalex.org/W4404584849 | In-context learning enables multimodal large language models to classify cancer pathology images | 2024 | Nature Communications | article | 101 | yes | Dyke Ferber, Georg Wölflein, Isabella C. Wiest, Marta Ligero, Srividhya Sainath, Narmin Ghaffari Laleh, Omar S. M. El Nahhas, Gustav Müller‐Franzes, +3 more | Computer science, Artificial intelligence, Machine learning, Deep learning, Subtyping, Context (archaeology), +5 more | https://doi.org/10.1038/s41467-024-51465-9 | Abstract Medical image classification requires labeled, task-specific datasets which are used to train deep learning networks de novo, or to fine-tune foundation models. However, this process is computationally and technically demanding. In language processing, in-context learning provides an alternative, where models learn from within prompts, bypassing the need for parameter updates. Yet, in-context learning remains underexplored in medical image analysis. Here, we systematically evaluate the model Generative Pretrained Transformer 4 with Vision capabilities (GPT-4V) on cancer image processing with in-context learning on three cancer histopathology tasks of high importance: Classification of tissue subtypes in colorectal cancer, colon polyp subtyping and breast tumor detection in lymph node sections. Our results show that in-context learning is sufficient to match or even outperform s… |
| https://openalex.org/W4308479898 | ViA: A Novel Vision-Transformer Accelerator Based on FPGA | 2022 | IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems | article | 108 | no | Teng Wang, Lei Gong, Chao Wang, Yang Yang, Yingxue Gao, Xuehai Zhou, Huaping Chen | Computer science, Field-programmable gate array, Transformer, Hardware acceleration, Locality, Computation, +11 more | https://doi.org/10.1109/tcad.2022.3197489 | Since Google proposed Transformer in 2017, it has made significant natural language processing (NLP) development. However, the increasing cost is a large amount of calculation and parameters. Previous researchers designed and proposed some accelerator structures for transformer models in field-programmable gate array (FPGA) to deal with NLP tasks efficiently. Now, the development of Transformer has also affected computer vision (CV) and has rapidly surpassed convolution neural networks (CNNs) in various image tasks. And there are apparent differences between the image data used in CV and the sequence data in NLP. The details in the models contained with transformer units in these two fields are also different. The difference in terms of data brings about the problem of the locality. The difference in the model structure brings about the problem of path dependence, which is not noticed i… |
| https://openalex.org/W3196329608 | Transformer for Handwritten Text Recognition Using Bidirectional Post-decoding | 2021 | Lecture notes in computer science | book-chapter | 48 | no | Christoph Wick, J. Marius Zöllner, Tobias Grüning | Computer science, Transformer, Decoding methods, Encoder, Connectionism, Convolutional neural network, +11 more | https://doi.org/10.1007/978-3-030-86334-0_8 |  |
| https://openalex.org/W4360612299 | Personalized Prompt Learning for Explainable Recommendation | 2023 | ACM Transactions on Information Systems | article | 131 | no | Lei Li, Yongfeng Zhang, Li Chen | Computer science, Recommender system, Artificial intelligence, Identifier, Regularization (linguistics), Transformer, +5 more | https://doi.org/10.1145/3580488 | Providing user-understandable explanations to justify recommendations could help users better understand the recommended items, increase the system’s ease of use, and gain users’ trust. A typical approach to realize it is natural language generation. However, previous works mostly adopt recurrent neural networks to meet the ends, leaving the potentially more effective pre-trained Transformer models under-explored. In fact, user and item IDs, as important identifiers in recommender systems, are inherently in different semantic space as words that pre-trained models were already trained on. Thus, how to effectively fuse IDs into such models becomes a critical issue. Inspired by recent advancement in prompt learning, we come up with two solutions: find alternative words to represent IDs (called discrete prompt learning) and directly input ID vectors to a pre-trained model (termed continuou… |
| https://openalex.org/W3120502588 | Evaluating Deep Learning Approaches for Covid19 Fake News Detection | 2021 | Communications in computer and information science | book-chapter | 131 | yes | Apurva Wani, Isha Joshi, Snehal Khandve, Vedangi Wagh, Raviraj Joshi | Computer science, Social media, Convolutional neural network, Fake news, Key (lock), Coronavirus disease 2019 (COVID-19), +13 more | http://arxiv.org/abs/2101.04012 |  |
| https://openalex.org/W4382632527 | Combining protein sequences and structures with transformers and equivariant graph neural networks to predict protein function | 2023 | Bioinformatics | article | 65 | yes | Frimpong Boadu, Hongyuan Cao, Jianlin Cheng | Computer science, Protein function prediction, Leverage (statistics), Artificial neural network, Artificial intelligence, Transformer, +16 more | https://doi.org/10.1093/bioinformatics/btad208 | Abstract Motivation Millions of protein sequences have been generated by numerous genome and transcriptome sequencing projects. However, experimentally determining the function of the proteins is still a time consuming, low-throughput, and expensive process, leading to a large protein sequence-function gap. Therefore, it is important to develop computational methods to accurately predict protein function to fill the gap. Even though many methods have been developed to use protein sequences as input to predict function, much fewer methods leverage protein structures in protein function prediction because there was lack of accurate protein structures for most proteins until recently. Results We developed TransFun—a method using a transformer-based protein language model and 3D-equivariant graph neural networks to distill information from both protein sequences and structures to predict pr… |
| https://openalex.org/W3155682407 | Is Supervised Syntactic Parsing Beneficial for Language Understanding Tasks? An Empirical Investigation | 2021 |  | article | 53 | yes | Goran Glavaš, Ivan Vulić | Computer science, Parsing, Natural language processing, Transformer, Artificial intelligence, S-attributed grammar, +6 more | https://doi.org/10.18653/v1/2021.eacl-main.270 | Traditional NLP has long held (supervised) syntactic parsing necessary for successful higher-level semantic language understanding (LU). The recent advent of end-to-end neural models, self-supervised via language modeling (LM), and their success on a wide range of LU tasks, however, questions this belief. In this work, we empirically investigate the usefulness of supervised parsing for semantic LU in the context of LM-pretrained transformer networks. Relying on the established fine-tuning paradigm, we first couple a pretrained transformer with a biaffine parsing head, aiming to infuse explicit syntactic knowledge from Universal Dependencies treebanks into the transformer. We then fine-tune the model for LU tasks and measure the effect of the intermediate parsing training (IPT) on downstream LU task performance. Results from both monolingual English and zero-shot language transfer experi… |
| https://openalex.org/W4391808470 | Predictive Analytics in Mental Health Leveraging LLM Embeddings and Machine Learning Models for Social Media Analysis | 2024 | International Journal of Web Services Research | article | 62 | yes | Ahmad Radwan, Mohannad Amarneh, Hussam Alawneh, Huthaifa I. Ashqar, Anas Alsobeh, Aws A. Magableh | Machine learning, Computer science, Support vector machine, Social media, Artificial intelligence, Transformer, +13 more | https://doi.org/10.4018/ijwsr.338222 | The prevalence of stress-related disorders has increased significantly in recent years, necessitating scalable methods to identify affected individuals. This paper proposes a novel approach utilizing large language models (LLMs), with a focus on OpenAI's generative pre-trained transformer (GPT-3) embeddings and machine learning (ML) algorithms to classify social media posts as indicative or not of stress disorders. The aim is to create a preliminary screening tool leveraging online textual data. GPT-3 embeddings transformed posts into vector representations capturing semantic meaning and linguistic nuances. Various models, including support vector machines, random forests, XGBoost, KNN, and neural networks, were trained on a dataset of &amp;gt;10,000 labeled social media posts. The top model, a support vector machine, achieved 83% accuracy in classifying posts displaying signs of stress. |
| https://openalex.org/W3034689697 | Transform and Tell: Entity-Aware News Image Captioning | 2020 |  | article | 84 | no | Alasdair Tran, A. P. Mathews, Lexing Xie | Closed captioning, Computer science, Transformer, Artificial intelligence, Word (group theory), Language model, +20 more | https://doi.org/10.1109/cvpr42600.2020.01305 | We propose an end-to-end model which generates captions for images embedded in news articles. News images present two key challenges: they rely on real-world knowledge, especially about named entities; and they typically have linguistically rich captions that include uncommon words. We address the first challenge by associating words in the caption with faces and objects in the image, via a multi-modal, multi-head attention mechanism. We tackle the second challenge with a state-of-the-art transformer language model that uses byte-pair-encoding to generate captions as a sequence of word parts. On the GoodNews dataset, our model outperforms the previous state of the art by a factor of four in CIDEr score (13 to 54). This performance gain comes from a unique combination of language models, word representation, image embeddings, face embeddings, object embeddings, and improvements in neural… |
| https://openalex.org/W4388821834 | On reading and interpreting black box deep neural networks | 2023 | International Journal of Digital Humanities | article | 42 | yes | James E. Dobson | Computer science, Artificial intelligence, Deep learning, Black box, Transformer, Criticism, +13 more | https://doi.org/10.1007/s42803-023-00075-w | Abstract The deep neural networks used in computer vision and in recent large language models are widely recognized as black boxes, a term that describes their complicated architectures and opaque decision-making mechanisms. This essay outlines several different strategies through which humanist researchers and critics of machine learning might better understand and interpret the class of deep learning methods known as Transformers. These strategies expose different aspects of what might be “learned” as Transformers are trained and used in the analysis of language and can help critics at least partially open the black box of machine learning. They are also especially useful for digital humanists using these models as part of a research program informed by tool criticism in which the use of computational tools is conceived of as a metainterpretive act. |
| https://openalex.org/W4393078503 | Large language models and multimodal foundation models for precision oncology | 2024 | npj Precision Oncology | article | 45 | yes | Daniel Truhn, Jan‐Niklas Eckardt, Dyke Ferber, Jakob Nikolas Kather | Computer science, Artificial neural network, Paradigm shift, Artificial intelligence, Data science, Epistemology, +1 more | https://doi.org/10.1038/s41698-024-00573-2 |  |
| https://openalex.org/W4394650770 | An Overview of Neural Network Compression | 2020 | arXiv (Cornell University) | preprint | 52 | yes | James O’ Neill | Deep learning, Computer science, Artificial neural network, Tensor decomposition, Artificial intelligence, Convolutional neural network, +9 more | http://arxiv.org/abs/2006.03669 | Overparameterized networks trained to convergence have shown impressive performance in domains such as computer vision and natural language processing. Pushing state of the art on salient tasks within these domains corresponds to these models becoming larger and more difficult for machine learning practitioners to use given the increasing memory and storage requirements, not to mention the larger carbon footprint. Thus, in recent years there has been a resurgence in model compression techniques, particularly for deep convolutional neural networks and self-attention based networks such as the Transformer. Hence, this paper provides a timely overview of both old and current compression techniques for deep neural networks, including pruning, quantization, tensor decomposition, knowledge distillation and combinations thereof. We assume a basic familiarity with deep learning architectures\fo… |
| https://openalex.org/W3198282417 | Automatic ICD-10 Coding and Training System: Deep Neural Network Based on Supervised Learning | 2021 | JMIR Medical Informatics | article | 64 | yes | Pei‐Fu Chen, Ssu-Ming Wang, Wei-Chih Liao, Lu-Cheng Kuo, Kuan‐Chih Chen, Yu‐Cheng Lin, Chi-Yu Yang, Chi-Hao Chiu, +2 more | Computer science, Coding (social sciences), Artificial intelligence, Deep learning, Encoder, Artificial neural network, +15 more | https://doi.org/10.2196/23230 | Background The International Classification of Diseases (ICD) code is widely used as the reference in medical system and billing purposes. However, classifying diseases into ICD codes still mainly relies on humans reading a large amount of written material as the basis for coding. Coding is both laborious and time-consuming. Since the conversion of ICD-9 to ICD-10, the coding task became much more complicated, and deep learning– and natural language processing–related approaches have been studied to assist disease coders. Objective This paper aims at constructing a deep learning model for ICD-10 coding, where the model is meant to automatically determine the corresponding diagnosis and procedure codes based solely on free-text medical notes to improve accuracy and reduce human effort. Methods We used diagnosis records of the National Taiwan University Hospital as resources and apply nat… |
| https://openalex.org/W2946232455 | Improving Neural Language Modeling via Adversarial Training | 2019 | arXiv (Cornell University) | article | 55 | yes | Dilin Wang, Chengyue Gong, Qiang Liu | Computer science, Adversarial system, Language model, Overfitting, Machine translation, Artificial intelligence, +16 more | http://arxiv.org/abs/1906.03805 | Recently, substantial progress has been made in language modeling by using deep neural networks. However, in practice, large scale neural language models have been shown to be prone to overfitting. In this paper, we present a simple yet highly effective adversarial training mechanism for regularizing neural language models. The idea is to introduce adversarial noise to the output embedding layer while training the models. We show that the optimal adversarial noise yields a simple closed-form solution, thus allowing us to develop a simple and time efficient algorithm. Theoretically, we show that our adversarial mechanism effectively encourages the diversity of the embedding vectors, helping to increase the robustness of models. Empirically, we show that our method improves on the single model state-of-the-art results for language modeling on Penn Treebank (PTB) and Wikitext-2, achieving… |
| https://openalex.org/W3140854437 | Review of deep learning: concepts, CNN architectures, challenges, applications, future directions | 2021 | Journal Of Big Data | article | 6953 | yes | Laith Alzubaidi, Jinglan Zhang, Amjad J. Humaidi, Ayad Q. Al-Dujaili, Ye Duan, Omran Al-Shamma, José Santamaría, Mohammed A. Fadhel, +2 more | Computer science, Artificial intelligence, Field (mathematics), Deep learning, Convolutional neural network, Machine learning, +7 more | https://doi.org/10.1186/s40537-021-00444-8 |  |
| https://openalex.org/W4229053887 | Transformer-Based Graph Convolutional Network for Sentiment Analysis | 2022 | Applied Sciences | article | 45 | yes | Barakat AlBadani, Ronghua Shi, Jian Dong, Raeed Al-Sabri, Babatoundé Moctard Olouladé | Computer science, Artificial intelligence, Sentiment analysis, Graph, Convolutional neural network, Transformer, +6 more | https://doi.org/10.3390/app12031316 | Sentiment Analysis is an essential research topic in the field of natural language processing (NLP) and has attracted the attention of many researchers in the last few years. Recently, deep neural network (DNN) models have been used for sentiment analysis tasks, achieving promising results. Although these models can analyze sequences of arbitrary length, utilizing them in the feature extraction layer of a DNN increases the dimensionality of the feature space. More recently, graph neural networks (GNNs) have achieved a promising performance in different NLP tasks. However, previous models cannot be transferred to a large corpus and neglect the heterogeneity of textual graphs. To overcome these difficulties, we propose a new Transformer-based graph convolutional network for heterogeneous graphs called Sentiment Transformer Graph Convolutional Network (ST-GCN). To the best of our knowledge… |
| https://openalex.org/W2948130861 | Playing the lottery with rewards and multiple languages: lottery tickets in RL and NLP | 2019 | arXiv (Cornell University) | preprint | 77 | yes | Haonan Yu, Sergey Edunov, Yuandong Tian, Ari S. Morcos | Lottery, Computer science, Artificial intelligence, Natural language processing, Economics, Microeconomics | http://arxiv.org/abs/1906.02768 | The lottery ticket hypothesis proposes that over-parameterization of deep neural networks (DNNs) aids training by increasing the probability of a "lucky" sub-network initialization being present rather than by helping the optimization process (Frankle &amp; Carbin, 2019). Intriguingly, this phenomenon suggests that initialization strategies for DNNs can be improved substantially, but the lottery ticket hypothesis has only previously been tested in the context of supervised learning for natural image tasks. Here, we evaluate whether "winning ticket" initializations exist in two different domains: natural language processing (NLP) and reinforcement learning (RL).For NLP, we examined both recurrent LSTM models and large-scale Transformer models (Vaswani et al., 2017). For RL, we analyzed a number of discrete-action space tasks, including both classic control and pixel control. Consistent w… |
| https://openalex.org/W2955260208 | Multi-source data fusion for aspect-level sentiment classification | 2019 | Knowledge-Based Systems | article | 64 | no | Fang Chen, Zhigang Yuan, Yongfeng Huang | Computer science, Artificial intelligence, Laptop, Sentiment analysis, Sentence, Natural language processing, +11 more | https://doi.org/10.1016/j.knosys.2019.07.002 |  |
| https://openalex.org/W4387885883 | Vision Transformer Model for Predicting the Severity of Diabetic Retinopathy in Fundus Photography-Based Retina Images | 2023 | IEEE Access | article | 94 | yes | Waleed Nazih, Ahmad O. Aseeri, Osama Youssef Atallah, Shaker El–Sappagh | Computer science, Artificial intelligence, Convolutional neural network, Diabetic retinopathy, Deep learning, Fundus photography, +11 more | https://doi.org/10.1109/access.2023.3326528 | Diabetic Retinopathy (DR) is a result of prolonged diabetes with poor blood sugar management. It causes vision problems and blindness due to the deformation of the human retina. Recently, DR has become a crucial medical problem that affects the health and life of people. Diagnosis of DR can be done manually by ophthalmologists, but this is cumbersome and time consuming especially in the current overloaded physician&#x2019;s environment. The early detection and prevention of DR, a severe complication of diabetes that can lead to blindness, require an automatic, accurate, and personalized machine learning-based method. Various deep learning algorithms, particularly convolutional neural networks (CNNs), have been investigated for detecting different stages of DR. Recently, transformers have proved their capabilities in natural language processing. Vision transformers (ViTs) are extensions… |
| https://openalex.org/W4313124045 | Dual-Tasks Siamese Transformer Framework for Building Damage Assessment | 2022 | IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium | article | 51 | no | Hongruixuan Chen, Edoardo Nemni, S. Vallecorsa, Xi Li, Chen Wu, Lars Bromley | Computer science, Transformer, Encoder, Architecture, Artificial intelligence, Convolutional neural network, +6 more | https://doi.org/10.1109/igarss46834.2022.9883139 | Accurate and fine-grained information about the extent of damage to buildings is essential for humanitarian relief and disaster response. However, as the most commonly used architecture in remote sensing interpretation tasks, Convolutional Neural Networks (CNNs) have limited ability to model the non-local relationship between pixels. Recently, Transformer architecture first proposed for modeling long-range dependency in natural language processing has shown promising results in computer vision tasks. Considering the frontier advances of Transformer architecture in the computer vision field, in this paper, we present a Transformer-based damage assessment architecture (DamFormer). In DamFormer, a siamese Transformer encoder is first constructed to extract non-local and representative deep features from input multitemporal image-pairs. Then, a multitemporal fusion module is designed to fus… |
| https://openalex.org/W2964110616 | Transformer-XL: Attentive Language Models beyond a Fixed-Length Context | 2019 |  | preprint | 3072 | yes | Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov | Perplexity, Computer science, Language model, Transformer, Treebank, Artificial intelligence, +6 more | https://doi.org/10.18653/v1/p19-1285 | Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billi… |
| https://openalex.org/W4295942986 | The AI‐IP: Minimizing the guesswork of personality scale item development through artificial intelligence | 2022 | Personnel Psychology | article | 38 | yes | Ivan Hernandez, Weiwen Nie | Artificial intelligence, Construct (python library), Computer science, Construct validity, Transformer, Artificial neural network, +13 more | https://doi.org/10.1111/peps.12543 | Abstract We propose a framework for integrating various modern natural language processing (NLP) models to assist researchers with developing valid psychological scales. Transformer‐based deep neural networks offer state‐of‐the‐art performance on various natural language tasks. This project adapts the transformer model GPT‐2 to learn the structure of personality items, and generate the largest openly available pool of personality items, consisting of one million new items. We then use that artificial intelligence‐based item pool (AI‐IP) to provide a subset of potential scale items for measuring a desired construct. To better recommend construct‐related items, we train a paired neural network‐based classification BERT model to predict the observed correlation between personality items using only their text. We also demonstrate how zero‐shot models can help balance desired content domains… |
| https://openalex.org/W4306362399 | Automatic Correction of Indonesian Grammatical Errors Based on Transformer | 2022 | Applied Sciences | article | 29 | yes | Ahmad Musyafa, Ying Gao, Aiman Solyman, Chaojie Wu, Siraj Khan | Computer science, Indonesian, Transformer, Natural language processing, Artificial intelligence, Grammar, +9 more | https://doi.org/10.3390/app122010380 | Grammatical error correction (GEC) is one of the major tasks in natural language processing (NLP) which has recently attracted great attention from researchers. The performance of universal languages such as English and Chinese in the GEC system has improved significantly. This could be attributed to the large number of powerful applications supported by neural network models and pretrained language models. Referring to the satisfactory results of the universal language in the GEC task and the lack of research on the GEC task for low-resource languages, especially Indonesian, this paper proposes an automatic model for Indonesian grammar correction based on the Transformer architecture which can be applied to other low-resource language texts. Furthermore, we build a large corpus of the Indonesian language that can be utilized for evaluating the next Indonesian GEC task. We evaluate the… |
| https://openalex.org/W4400074175 | AtomGPT: Atomistic Generative Pretrained Transformer for Forward and Inverse Materials Design | 2024 | The Journal of Physical Chemistry Letters | article | 51 | no | Kamal Choudhary | Generative grammar, Transformer, Inverse, Computer science, Generative Design, Materials science, +8 more | https://doi.org/10.1021/acs.jpclett.4c01126 | Large language models (LLMs) such as generative pretrained transformers (GPTs) have shown potential for various commercial applications, but their applicability for materials design remains underexplored. In this Letter, AtomGPT is introduced as a model specifically developed for materials design based on transformer architectures, demonstrating capabilities for both atomistic property prediction and structure generation. This study shows that a combination of chemical and structural text descriptions can efficiently predict material properties with accuracy comparable to graph neural network models, including formation energies, electronic bandgaps from two different methods, and superconducting transition temperatures. Furthermore, AtomGPT can generate atomic structures for tasks such as designing new superconductors, with the predictions validated through density functional theory ca… |
| https://openalex.org/W2919207648 | Tensorized Embedding Layers for Efficient Model Compression | 2019 | arXiv (Cornell University) | preprint | 50 | yes | Valentin Khrulkov, Oleksii Hrinchuk, Leyla Mirvakhabova, Ivan Oseledets, Oseledets, Ivan | Embedding, Compression (physics), Computer science, Materials science, Artificial intelligence, Composite material | http://arxiv.org/abs/1901.10787 | The embedding layers transforming input words into real vectors are the key components of deep neural networks used in natural language processing. However, when the vocabulary is large, the corresponding weight matrices can be enormous, which precludes their deployment in a limited resource setting. We introduce a novel way of parametrizing embedding layers based on the Tensor Train (TT) decomposition, which allows compressing the model significantly at the cost of a negligible drop or even a slight gain in performance. We evaluate our method on a wide range of benchmarks in natural language processing and analyze the trade-off between performance and compression ratios for a wide range of architectures, from MLPs to LSTMs and Transformers. |
| https://openalex.org/W4312097792 | Integrating unsupervised language model with triplet neural networks for protein gene ontology prediction | 2022 | PLoS Computational Biology | article | 59 | yes | Yiheng Zhu, Chengxin Zhang, Dong‐Jun Yu, Yang Zhang | Computer science, Artificial intelligence, Discriminative model, Gene ontology, Artificial neural network, Protein function prediction, +11 more | https://doi.org/10.1371/journal.pcbi.1010793 | Accurate identification of protein function is critical to elucidate life mechanisms and design new drugs. We proposed a novel deep-learning method, ATGO, to predict Gene Ontology (GO) attributes of proteins through a triplet neural-network architecture embedded with pre-trained language models from protein sequences. The method was systematically tested on 1068 non-redundant benchmarking proteins and 3328 targets from the third Critical Assessment of Protein Function Annotation (CAFA) challenge. Experimental results showed that ATGO achieved a significant increase of the GO prediction accuracy compared to the state-of-the-art approaches in all aspects of molecular function, biological process, and cellular component. Detailed data analyses showed that the major advantage of ATGO lies in the utilization of pre-trained transformer language models which can extract discriminative function… |
| https://openalex.org/W2969696767 | Neural Machine Translation With Sentence-Level Topic Context | 2019 | IEEE/ACM Transactions on Audio Speech and Language Processing | article | 51 | no | Kehai Chen, Rui Wang, Masao Utiyama, Eiichiro Sumita, Tiejun Zhao | Machine translation, Computer science, Sentence, Natural language processing, Example-based machine translation, Artificial intelligence, +15 more | https://doi.org/10.1109/taslp.2019.2937190 | Traditional neural machine translation (NMT) methods use the word-level context to predict target language translation while neglecting the sentence-level context, which has been shown to be beneficial for translation prediction in statistical machine translation. This paper represents the sentence-level context as latent topic representations by using a convolution neural network, and designs a topic attention to integrate source sentence-level topic context information into both attention-based and Transformer-based NMT. In particular, our method can improve the performance of NMT by modeling source topics and translations jointly. Experiments on the large-scale LDC Chinese-to-English translation tasks and WMT'14 English-to-German translation tasks show that the proposed approach can achieve significant improvements compared with baseline systems. |
| https://openalex.org/W2970401203 | Large Memory Layers with Product Keys | 2019 | HAL (Le Centre pour la Communication Scientifique Directe) | article | 50 | yes | Guillaume Lample, Alexandre Sablayrolles, Marc’Aurelio Ranzato, Ludovic Denoyer, Hervé Jeǵou | Computer science, Transformer, Computation, Inference, Architecture, Artificial neural network, +12 more | https://doi.org/10.48550/arxiv.1907.05242 | This paper introduces a structured memory which can be easily integrated into a neural network. The memory is very large by design and significantly increases the capacity of the architecture, by up to a billion parameters with a negligible computational overhead. Its design and access pattern is based on product keys, which enable fast and exact nearest neighbor search. The ability to increase the number of parameters while keeping the same computational budget lets the overall system strike a better trade-off between prediction accuracy and computation efficiency both at training and test time. This memory layer allows us to tackle very large scale language modeling tasks. In our experiments we consider a dataset with up to 30 billion words, and we plug our memory layer in a state-of-the-art transformer-based architecture. In particular, we found that a memory augmented model with onl… |
| https://openalex.org/W4210271265 | Benchmarking machine learning methods for modeling physical properties of ionic liquids | 2022 | Journal of Molecular Liquids | article | 64 | no | Igor I. Baskin, A. E. Épshtein, Yair Ein‐Eli | Quantitative structure–activity relationship, Artificial neural network, Computer science, Benchmarking, Artificial intelligence, Machine learning, +9 more | https://doi.org/10.1016/j.molliq.2022.118616 |  |
| https://openalex.org/W4388593316 | Assessing the Strengths and Weaknesses of Large Language Models | 2023 | Journal of Logic Language and Information | article | 34 | yes | Shalom Lappin | Sentience, Skepticism, Transformer, Artificial intelligence, Cognitive science, Computer science, +9 more | https://doi.org/10.1007/s10849-023-09409-x | Abstract The transformers that drive chatbots and other AI systems constitute large language models (LLMs). These are currently the focus of a lively discussion in both the scientific literature and the popular media. This discussion ranges from hyperbolic claims that attribute general intelligence and sentience to LLMs, to the skeptical view that these devices are no more than “stochastic parrots”. I present an overview of some of the weak arguments that have been presented against LLMs, and I consider several of the more compelling criticisms of these devices. The former significantly underestimate the capacity of transformers to achieve subtle inductive inferences required for high levels of performance on complex, cognitively significant tasks. In some instances, these arguments misconstrue the nature of deep learning. The latter criticisms identify significant limitations in the wa… |
| https://openalex.org/W4226163610 | Transformer-Based Deep Neural Language Modeling for Construct-Specific Automatic Item Generation | 2021 | Psychometrika | article | 49 | yes | Björn Erik Hommel, Franz-Josef M. Wollang, Veronika Kotova, Hannes Zacher, Stefan C. Schmukle | Computer science, Artificial intelligence, Construct (python library), Cognition, Natural language processing, Transformer, +8 more | https://doi.org/10.1007/s11336-021-09823-9 | Algorithmic automatic item generation can be used to obtain large quantities of cognitive items in the domains of knowledge and aptitude testing. However, conventional item models used by template-based automatic item generation techniques are not ideal for the creation of items for non-cognitive constructs. Progress in this area has been made recently by employing long short-term memory recurrent neural networks to produce word sequences that syntactically resemble items typically found in personality questionnaires. To date, such items have been produced unconditionally, without the possibility of selectively targeting personality domains. In this article, we offer a brief synopsis on past developments in natural language processing and explain why the automatic generation of construct-specific items has become attainable only due to recent technological progress. We propose that pre-… |
| https://openalex.org/W3098480005 | Attention-based Bidirectional Long Short-Term Memory Networks for Relation Classification Using Knowledge Distillation from BERT | 2020 |  | article | 27 | no | Zihan Wang, Bo Yang | Computer science, Inference, Artificial intelligence, Transformer, Relation (database), Artificial neural network, +12 more | https://doi.org/10.1109/dasc-picom-cbdcom-cyberscitech49142.2020.00100 | Relation classification is an important task in the field of natural language processing. Today the best-performing models often use huge, transformer-based neural architectures like BERT and XLNet and have hundreds of millions of network parameters. These large neural networks have led to the belief that the shallow neural networks of the previous generation for relation classification are obsolete. However, due to large network size and low inference speed, these models may be impractical in on-line real-time systems or resource-restricted systems. To address this issue, we try to accelerate these well-performing language models by compressing them. Specifically, we distill knowledge for relation classification from a huge, transformer-based language model, BERT, into an Attention-Based Bidirectional Long Short-Term Memory Network. We run our model on the SemEval-2010 relation classif… |
| https://openalex.org/W4388717695 | To Transformers and Beyond: Large Language Models for the Genome | 2023 | arXiv (Cornell University) | preprint | 31 | yes | Micaela Elisa Consens, C Dufault, Michael Wainberg, Duncan Forster, Mehran Karimzadeh, Hani Goodarzi, Fabian J. Theis, Alan M Moses, +1 more | Transformative learning, Genomics, Transformer, Architecture, Data science, Computer science, +12 more | http://arxiv.org/abs/2311.07621 | In the rapidly evolving landscape of genomics, deep learning has emerged as a useful tool for tackling complex computational challenges. This review focuses on the transformative role of Large Language Models (LLMs), which are mostly based on the transformer architecture, in genomics. Building on the foundation of traditional convolutional neural networks and recurrent neural networks, we explore both the strengths and limitations of transformers and other LLMs for genomics. Additionally, we contemplate the future of genomic modeling beyond the transformer architecture based on current trends in research. The paper aims to serve as a guide for computational biologists and computer scientists interested in LLMs for genomic data. We hope the paper can also serve as an educational introduction and discussion for biologists to a fundamental shift in how we will be analyzing genomic data in… |
| https://openalex.org/W4387904013 | Vision Transformers and Transfer Learning Approaches for Arabic Sign Language Recognition | 2023 | Applied Sciences | article | 32 | yes | Nojood M. Alharthi, Salha M. Alzahrani | Computer science, Transformer, Artificial intelligence, Transfer of learning, Convolutional neural network, Deep learning, +8 more | https://doi.org/10.3390/app132111625 | Sign languages are complex, but there are ongoing research efforts in engineering and data science to recognize, understand, and utilize them in real-time applications. Arabic sign language recognition (ArSL) has been examined and applied using various traditional and intelligent methods. However, there have been limited attempts to enhance this process by utilizing pretrained models and large-sized vision transformers designed for image classification tasks. This study aimed to create robust transfer learning models trained on a dataset of 54,049 images depicting 32 alphabets from an ArSL dataset. The goal was to accurately classify these images into their corresponding Arabic alphabets. This study included two methodological parts. The first one was the transfer learning approach, wherein we utilized various pretrained models namely MobileNet, Xception, Inception, InceptionResNet, Den… |
| https://openalex.org/W3122317902 | GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding | 2021 | International Conference on Learning Representations | article | 49 | no | Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Fırat, Yanping Huang, Maxim Krikun, Noam Shazeer, +1 more | Computer science, Computation, Machine translation, Artificial intelligence, Compiler, Transformer, +13 more | https://openreview.net/pdf?id=qrwe7XHTmYb | Neural network scaling has been critical for improving the model quality in many real-world machine learning applications with vast amounts of training data and compute. Although this trend of scaling is affirmed to be a sure-fire approach for better model quality, there are challenges on the path such as the computation cost,ease of programming, and efficient implementation on parallel devices. In this paper we demonstrate conditional computation as a remedy to the above mentioned impediments, and demonstrate its efficacy and utility. We make extensive use of GShard, a module composed of a set of lightweight annotation APIs and an extension to the XLA compiler to enable large scale models with up to trillions of parameters. GShard and conditional computation enable us to scale up multilingual neural machine translation Transformer model with Sparsely-Gated Mixture-of-Experts. We demons… |
| https://openalex.org/W3202105401 | GraphFormers: GNN-nested Transformers for Representation Learning on Textual Graph | 2021 | arXiv (Cornell University) | preprint | 38 | yes | Junhan Yang, Zheng Liu, Shitao Xiao, Chaozhuo Li, Defu Lian, Sanjay Agrawal, Amit Singh, Guangzhong Sun, +1 more | Computer science, Transformer, Graph, Workflow, Architecture, Feature learning, +10 more | http://arxiv.org/abs/2105.02605 | The representation learning on textual graph is to generate low-dimensional embeddings for the nodes based on the individual textual features and the neighbourhood information. Recent breakthroughs on pretrained language models and graph neural networks push forward the development of corresponding techniques. The existing works mainly rely on the cascaded model architecture: the textual features of nodes are independently encoded by language models at first; the textual embeddings are aggregated by graph neural networks afterwards. However, the above architecture is limited due to the independent modeling of textual features. In this work, we propose GraphFormers, where layerwise GNN components are nested alongside the transformer blocks of language models. With the proposed architecture, the text encoding and the graph aggregation are fused into an iterative workflow, {making} each no… |
| https://openalex.org/W3142085127 | On the Adversarial Robustness of Visual Transformers | 2021 | arXiv (Cornell University) | preprint | 50 | yes | Rulin Shao, Zhouxing Shi, Jinfeng Yi, Pin‐Yu Chen, Cho‐Jui Hsieh | Robustness (evolution), Adversarial system, Computer science, Transformer, Artificial intelligence, Convolutional neural network, +10 more | http://arxiv.org/pdf/2103.15670.pdf | Following the success in advancing natural language processing and understanding, transformers are expected to bring revolutionary changes to computer vision. This work provides the first and comprehensive study on the robustness of vision transformers (ViTs) against adversarial perturbations. Tested on various white-box and transfer attack settings, we find that ViTs possess better adversarial robustness when compared with convolutional neural networks (CNNs). We summarize the following main observations contributing to the improved robustness of ViTs: 1) Features learned by ViTs contain less low-level information and are more generalizable, which contributes to superior robustness against adversarial perturbations. 2) Introducing convolutional or tokens-to-token blocks for learning low-level features in ViTs can improve classification accuracy but at the cost of adversarial robustness… |
| https://openalex.org/W4210742602 | Generative Pre-Training from Molecules | 2021 |  | preprint | 23 | yes | Sanjar Adilov | Transformer, Computer science, Notation, Artificial intelligence, Generative grammar, Natural language processing, +11 more | https://doi.org/10.26434/chemrxiv-2021-5fwjd | SMILES is a line notation for entering and representing molecules. Being inherently a language construct, it allows estimating molecular data in a self-supervised fashion by employing machine learning methods for natural language processing (NLP). The recent success of attention-based neural networks in NLP has made large-corpora transformer pretraining a de facto standard for learning representations and transferring knowledge to downstream tasks. In this work, we attempt to adapt transformer capabilities to a large SMILES corpus by constructing a GPT-2-like language model. We experimentally show that a pretrained causal transformer captures general knowledge that can be successfully transferred to such downstream tasks as focused molecule generation and single-/multi-output molecular-property prediction. For each task, we freeze model parameters and attach trainable lightweight networ… |
| https://openalex.org/W4288368497 | Shared functional specialization in transformer-based language models and the human brain | 2022 |  | preprint | 31 | yes | Sreejan Kumar, Theodore R. Sumers, Takateru Yamakoshi, Ariel Goldstein, Uri Hasson, Kenneth A. Norman, Thomas L. Griffiths, Robert D. Hawkins, +1 more | Computer science, Transformer, Computation, Language model, Artificial intelligence, Cognitive science, +11 more | https://doi.org/10.1101/2022.06.08.495348 | Abstract Humans use complex linguistic structures to transmit ideas to one another. The brain is thought to deploy specialized computations to process these structures. Recently, a new class of artificial neural networks based on the Transformer architecture has revolutionized the field of language modeling, attracting attention from neuroscientists seeking to understand the neurobiology of language in silico. Transformers integrate information across words via multiple layers of structured circuit computations, forming increasingly contextualized representations of linguistic content. Prior work has focused on the internal representations (the “embeddings”) generated by these circuits. In this paper, we instead analyze the circuit computations directly: we deconstruct these computations into functionally-specialized “transformations” to provide a complementary window onto linguistic co… |
| https://openalex.org/W2970771982 | SciBERT: A Pretrained Language Model for Scientific Text | 2019 |  | article | 2871 | yes | Iz Beltagy, Kyle Lo, Arman Cohan | Computer science, Natural language processing, Language model, Artificial intelligence, Programming language | https://doi.org/10.18653/v1/d19-1371 | Iz Beltagy, Kyle Lo, Arman Cohan. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019. |
| https://openalex.org/W3045332379 | Highly accurate classification of chest radiographic reports using a deep learning natural language model pre-trained on 3.8 million text reports | 2020 | Bioinformatics | article | 69 | no | Keno K. Bressem, Lisa C. Adams, Robert Gaudin, Daniel Tröltzsch, Bernd Hamm, Marcus R. Makowski, Chan-Yong Schüle, Janis L. Vahldiek, +1 more | Computer science, Artificial intelligence, Deep learning, Workflow, Source code, Encoder, +11 more | https://doi.org/10.1093/bioinformatics/btaa668 | Abstract Motivation The development of deep, bidirectional transformers such as Bidirectional Encoder Representations from Transformers (BERT) led to an outperformance of several Natural Language Processing (NLP) benchmarks. Especially in radiology, large amounts of free-text data are generated in daily clinical workflow. These report texts could be of particular use for the generation of labels in machine learning, especially for image classification. However, as report texts are mostly unstructured, advanced NLP methods are needed to enable accurate text classification. While neural networks can be used for this purpose, they must first be trained on large amounts of manually labelled data to achieve good results. In contrast, BERT models can be pre-trained on unlabelled data and then only require fine tuning on a small amount of manually labelled data to achieve even better results.… |
| https://openalex.org/W4388798253 | XAI Transformer based Approach for Interpreting Depressed and Suicidal User Behavior on Online Social Networks | 2023 | Cognitive Systems Research | article | 43 | no | Anshu Malhotra, Rajni Jindal | Computer science, Machine learning, Artificial intelligence, Deep learning, Transformer, Health care, +14 more | https://doi.org/10.1016/j.cogsys.2023.101186 |  |
| https://openalex.org/W3116085416 | Complaint Identification in Social Media with Transformer Networks | 2020 |  | article | 24 | yes | Mali Jin, Νικόλαος Αλέτρας | Computer science, Complaint, Transformer, Social media, Artificial neural network, Artificial intelligence, +15 more | https://doi.org/10.18653/v1/2020.coling-main.157 | Complaining is a speech act extensively used by humans to communicate a negative inconsistency between reality and expectations. Previous work on automatically identifying complaints in social media has focused on using feature-based and task-specific neural network models. Adapting state-of-the-art pre-trained neural language models and their combinations with other linguistic information from topics or sentiment for complaint prediction has yet to be explored. In this paper, we evaluate a battery of neural models underpinned by transformer networks which we subsequently combine with linguistic information. Experiments on a publicly available data set of complaints demonstrate that our models outperform previous state-of-the-art methods by a large margin achieving a macro F1 up to 87. |
| https://openalex.org/W3081796931 | Text Augmentation Using BERT for Image Captioning | 2020 | Applied Sciences | article | 31 | yes | Viktar Atliha, Dmitrij Šešok | Computer science, Closed captioning, Transformer, Encoder, Artificial intelligence, Language model, +9 more | https://doi.org/10.3390/app10175978 | Image captioning is an important task for improving human-computer interaction as well as for a deeper understanding of the mechanisms underlying the image description by human. In recent years, this research field has rapidly developed and a number of impressive results have been achieved. The typical models are based on a neural networks, including convolutional ones for encoding images and recurrent ones for decoding them into text. More than that, attention mechanism and transformers are actively used for boosting performance. However, even the best models have a limit in their quality with a lack of data. In order to generate a variety of descriptions of objects in different situations you need a large training set. The current commonly used datasets although rather large in terms of number of images are quite small in terms of the number of different captions per one image. We exp… |
| https://openalex.org/W2933138175 | fairseq: A Fast, Extensible Toolkit for Sequence Modeling | 2019 |  | article | 2476 | yes | Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, Michael Auli | Computer science, Computational linguistics, Programming language, Sequence (biology), Art history, Library science, +4 more | https://doi.org/10.18653/v1/n19-4009 | Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, Michael Auli. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations). 2019. |
| https://openalex.org/W4287758766 | Leap-Of-Thought: Teaching Pre-Trained Models to Systematically Reason\n Over Implicit Knowledge | 2020 | arXiv (Cornell University) | preprint | 49 | yes | Alon Talmor, Oyvind Tafjord, Peter Clark, Yoav Goldberg, Jonathan Berant | Computer science, Chaining, Inference, Artificial intelligence, Forward chaining, Backward chaining, +14 more | http://arxiv.org/abs/2006.06609 | To what extent can a neural network systematically reason over symbolic\nfacts? Evidence suggests that large pre-trained language models (LMs) acquire\nsome reasoning capacity, but this ability is difficult to control. Recently, it\nhas been shown that Transformer-based models succeed in consistent reasoning\nover explicit symbolic facts, under a "closed-world" assumption. However, in an\nopen-domain setup, it is desirable to tap into the vast reservoir of implicit\nknowledge already encoded in the parameters of pre-trained LMs. In this work,\nwe provide a first demonstration that LMs can be trained to reliably perform\nsystematic reasoning combining both implicit, pre-trained knowledge and\nexplicit natural language statements. To do this, we describe a procedure for\nautomatically generating datasets that teach a model new reasoning skills, and\ndemonstrate that models learn to effect… |
| https://openalex.org/W4297812065 | FP8 Formats for Deep Learning | 2022 | arXiv (Cornell University) | preprint | 49 | yes | Paulius Micikevicius, Dušan Stošić, Neil Burgess, Marius Cornea, Pradeep Dubey, Richard Grisenthwaite, Sangwon Ha, Alexander Heinecke, +7 more | Computer science, Artificial intelligence, Deep learning, Hyperparameter, Artificial neural network, Binary number, +4 more | http://arxiv.org/abs/2209.05433 | FP8 is a natural progression for accelerating deep learning training inference beyond the 16-bit formats common in modern processors. In this paper we propose an 8-bit floating point (FP8) binary interchange format consisting of two encodings - E4M3 (4-bit exponent and 3-bit mantissa) and E5M2 (5-bit exponent and 2-bit mantissa). While E5M2 follows IEEE 754 conventions for representatio of special values, E4M3's dynamic range is extended by not representing infinities and having only one mantissa bit-pattern for NaNs. We demonstrate the efficacy of the FP8 format on a variety of image and language tasks, effectively matching the result quality achieved by 16-bit training sessions. Our study covers the main modern neural network architectures - CNNs, RNNs, and Transformer-based models, leaving all the hyperparameters unchanged from the 16-bit baseline training sessions. Our training expe… |
| https://openalex.org/W4391062620 | The Ethics of Artificial Intelligence in the Era of Generative AI | 2023 | Journal of systemics, cybernetics, and informatics/Journal of systemics cyberne… | article | 41 | yes | Vassilka Kirova, Cyril S. Ku, Joseph R. Laracy, Thomas J. Marlowe | Generative grammar, Computer science, Context (archaeology), Artificial intelligence, Engineering ethics, Applications of artificial intelligence, +7 more | https://doi.org/10.54808/jsci.21.04.42 | In the early 2020s, advances in transformer-based deep neural networks enabled the development and growth of a number of generative artificial intelligence (GenAI) systems notable for accepting natural language prompts as input. These include large language model chatbots such as ChatGPT, Bard, and others. GenAI has applications across a wide range of industries, including art, writing, software development, product design, healthcare, finance, gaming, and more. In this paper, we place these recent advances in a historical, cybernetic context. We analyze ethical issues that arise in the area of software engineering and cyber-physical systems. In addition, we explore AI-based challenges in healthcare and medicine, including a number involving GenAI. This research shows the importance of rigorous ethical analysis and resulting safeguards to address the emerging issues with AI. |
| https://openalex.org/W4320516905 | What Can Transformers Learn In-Context? A Case Study of Simple Function Classes | 2022 | arXiv (Cornell University) | preprint | 59 | yes | Shivam Garg, Dimitris Tsipras, Percy Liang, Gregory Valiant | Computer science, Inference, Artificial intelligence, Machine learning, Estimator, Context model, +9 more | http://arxiv.org/abs/2208.01066 | In-context learning refers to the ability of a model to condition on a prompt sequence consisting of in-context examples (input-output pairs corresponding to some task) along with a new query input, and generate the corresponding output. Crucially, in-context learning happens only at inference time without any parameter updates to the model. While large language models such as GPT-3 exhibit some ability to perform in-context learning, it is unclear what the relationship is between tasks on which this succeeds and what is present in the training data. To make progress towards understanding in-context learning, we consider the well-defined problem of training a model to in-context learn a function class (e.g., linear functions): that is, given data derived from some functions in the class, can we train a model to in-context learn "most" functions from this class? We show empirically that… |
| https://openalex.org/W4312762144 | Cross-modal Clinical Graph Transformer for Ophthalmic Report Generation | 2022 | 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) | article | 53 | no | Mingjie Li, Wenjia Cai, Karin Verspoor, Shirui Pan, Xiaodan Liang, Xiaojun Chang | Computer science, Transformer, Knowledge base, Artificial intelligence, Natural language processing, Relationship extraction, +8 more | https://doi.org/10.1109/cvpr52688.2022.02000 | Automatic generation of ophthalmic reports using data-driven neural networks has great potential in clinical practice. When writing a report, ophthalmologists make inferences with prior clinical knowledge. This knowledge has been neglected in prior medical report generation methods. To endow models with the capability of incorporating expert knowledge, we propose a Cross-modal clinical Graph Transformer (CGT) for ophthalmic report generation (ORG), in which clinical relation triples are injected into the visual features as prior knowledge to drive the decoding procedure. However, two major common Knowledge Noise (KN) issues may affect models' effectiveness. 1) Existing general biomedical knowledge bases such as the UMLS may not align meaningfully to the specific context and language of the report, limiting their utility for knowledge injection. 2) Incorporating too much knowledge may di… |
| https://openalex.org/W4381233128 | X-Former: In-Memory Acceleration of Transformers | 2023 | IEEE Transactions on Very Large Scale Integration (VLSI) Systems | article | 47 | no | Shrihari Sridharan, Jacob R. Stevens, Kaushik Roy, Anand Raghunathan | Computer science, Interleaved memory, Computer hardware, Operand, Dram, Massively parallel, +12 more | https://doi.org/10.1109/tvlsi.2023.3282046 | Transformers have achieved great success in a wide variety of natural language processing (NLP) tasks due to the self-attention mechanism, which assigns an importance score for every word relative to other words in a sequence. However, these models are very large, often reaching hundreds of billions of parameters, and therefore require a large number of dynamic random access memory (DRAM) accesses. Hence, traditional deep neural network (DNN) accelerators such as graphical processing units (GPUs) and tensor processing units (TPUs) face limitations in processing Transformers efficiently. In-memory accelerators based on nonvolatile memory (NVM) promise to be an effective solution to this challenge, since they provide high storage density while performing massively parallel matrix–vector multiplications (MVMs) within memory arrays. However, attention score computations, which are frequentl… |
| https://openalex.org/W3185341429 | Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing | 2022 | ACM Computing Surveys | review | 3325 | yes | Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig | Computer science, Variety (cybernetics), Set (abstract data type), Notation, Cover (algebra), Artificial intelligence, +18 more | https://doi.org/10.1145/3560815 | This article surveys and organizes research works in a new paradigm in natural language processing, which we dub “prompt-based learning.” Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P (y\|x), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x′ that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string x̂, from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: It allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-sho… |
| https://openalex.org/W4322759345 | SpikeGPT: Generative Pre-trained Language Model with Spiking Neural Networks | 2023 | arXiv (Cornell University) | preprint | 52 | yes | Ruijie Zhu, Qihang Zhao, Jason K. Eshraghian, Eshraghian, Jason K. | Computer science, Spiking neural network, Artificial intelligence, Language model, Machine learning, Leverage (statistics), +4 more | http://arxiv.org/abs/2302.13939 | As the size of large language models continue to scale, so does the computational resources required to run it. Spiking Neural Networks (SNNs) have emerged as an energy-efficient approach to deep learning that leverage sparse and event-driven activations to reduce the computational overhead associated with model inference. While they have become competitive with non-spiking models on many computer vision tasks, SNNs have also proven to be more challenging to train. As a result, their performance lags behind modern deep learning, and we are yet to see the effectiveness of SNNs in language generation. In this paper, inspired by the Receptance Weighted Key Value (RWKV) language model, we successfully implement `SpikeGPT', a generative language model with binary, event-driven spiking activation units. We train the proposed model on two model variants: 45M and 216M parameters. To the best of… |
| https://openalex.org/W4360620450 | Opinion Paper: “So what if ChatGPT wrote it?” Multidisciplinary perspectives on opportunities, challenges and implications of generative conversational AI for research, practice and policy | 2023 | International Journal of Information Management | article | 3187 | yes | Yogesh K. Dwivedi, Nir Kshetri, Laurie Hughes, Emma Slade, Anand Jeyaraj, Arpan Kumar Kar, Abdullah M. Baabdullah, Alex Koohang, +65 more | Transformative learning, Generative grammar, Knowledge management, Hospitality, Engineering ethics, Sociology, +8 more | https://doi.org/10.1016/j.ijinfomgt.2023.102642 | Transformative artificially intelligent tools, such as ChatGPT, designed to generate sophisticated text indistinguishable from that produced by a human, are applicable across a wide range of contexts. The technology presents opportunities as well as, often ethical and legal, challenges, and has the potential for both positive and negative impacts for organisations, society, and individuals. Offering multi-disciplinary insight into some of these, this article brings together 43 contributions from experts in fields such as computer science, marketing, information systems, education, policy, hospitality and tourism, management, publishing, and nursing. The contributors acknowledge ChatGPT's capabilities to enhance productivity and suggest that it is likely to offer significant gains in the banking, hospitality and tourism, and information technology industries, and enhance business activit… |
| https://openalex.org/W3098605233 | CodeBERT: A Pre-Trained Model for Programming and Natural Languages | 2020 |  | article | 2203 | yes | Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, +3 more | Computer science, Programming language, Computational linguistics, Natural language processing, Natural language, Artificial intelligence, +5 more | https://doi.org/10.18653/v1/2020.findings-emnlp.139 | Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, Ming Zhou. Findings of the Association for Computational Linguistics: EMNLP 2020. 2020. |
| https://openalex.org/W3103522166 | Tensorized Embedding Layers | 2020 |  | article | 24 | yes | Oleksii Hrinchuk, Valentin Khrulkov, Leyla Mirvakhabova, Elena Orlova, Ivan Oseledets | Computer science, Embedding, Software deployment, Artificial neural network, Deep neural networks, Transformer, +10 more | https://doi.org/10.18653/v1/2020.findings-emnlp.436 | The embedding layers transforming input words into real vectors are the key components of deep neural networks used in natural language processing. However, when the vocabulary is large, the corresponding weight matrices can be enormous, which precludes their deployment in a limited resource setting. We introduce a novel way of parameterizing embedding layers based on the Tensor Train decomposition, which allows compressing the model significantly at the cost of a negligible drop or even a slight gain in performance. We evaluate our method on a wide range of benchmarks in natural language processing and analyze the trade-off between performance and compression ratios for a wide range of architectures, from MLPs to LSTMs and Transformers. |
| https://openalex.org/W3133511908 | Exploring Video Captioning Techniques: A Comprehensive Survey on Deep Learning Methods | 2021 | SN Computer Science | article | 34 | no | Saiful Islam, Aurpan Dash, Ashek Seum, Amir Hossain Raj, Tonmoy Hossain, Faisal Muhammad Shah | Closed captioning, Computer science, Residual neural network, Deep learning, Artificial intelligence, Transformer, +23 more | https://doi.org/10.1007/s42979-021-00487-x |  |
| https://openalex.org/W4386566474 | A Survey on Dynamic Neural Networks for Natural Language Processing | 2023 |  | article | 15 | yes | Canwen Xu, Julian McAuley | Computer science, Artificial neural network, Computation, Inference, Scaling, Artificial intelligence, +10 more | https://doi.org/10.18653/v1/2023.findings-eacl.180 | Effectively scaling large Transformer models is a main driver of recent advances in natural language processing. Dynamic neural networks, as an emerging research direction, are capable of scaling up neural networks with sub-linear increases in computation and time by dynamically adjusting their computational path based on the input. Dynamic neural networks could be a promising solution to the growing parameter numbers of pretrained language models, allowing both model pretraining with trillions of parameters and faster inference on mobile devices. In this survey, we summarize the progress of three types of dynamic neural networks in NLP: skimming, mixture of experts, and early exit. We also highlight current challenges in dynamic neural networks and directions for future research. |
| https://openalex.org/W4393352610 | Applications of Large Language Models in Pathology | 2024 | Bioengineering | review | 29 | yes | Jerome Cheng | Computer science, Pathology, Artificial intelligence, Medicine | https://doi.org/10.3390/bioengineering11040342 | Large language models (LLMs) are transformer-based neural networks that can provide human-like responses to questions and instructions. LLMs can generate educational material, summarize text, extract structured data from free text, create reports, write programs, and potentially assist in case sign-out. LLMs combined with vision models can assist in interpreting histopathology images. LLMs have immense potential in transforming pathology practice and education, but these models are not infallible, so any artificial intelligence generated content must be verified with reputable sources. Caution must be exercised on how these models are integrated into clinical practice, as these models can produce hallucinations and incorrect results, and an over-reliance on artificial intelligence may lead to de-skilling and automation bias. This review paper provides a brief history of LLMs and highlig… |
| https://openalex.org/W4287117648 | Large-Scale Chemical Language Representations Capture Molecular Structure and Properties | 2021 | arXiv (Cornell University) | preprint | 31 | yes | Jerret Ross, Brian Belgodere, Vijil Chenthamarakshan, Inkit Padhi, Youssef Mroueh, Payel Das | Computer science, Chemical space, Artificial intelligence, Language model, Machine learning, Transformer, +18 more | http://arxiv.org/abs/2106.09553 | Models based on machine learning can enable accurate and fast molecular property predictions, which is of interest in drug discovery and material design. Various supervised machine learning models have demonstrated promising performance, but the vast chemical space and the limited availability of property labels make supervised learning challenging. Recently, unsupervised transformer-based language models pretrained on a large unlabelled corpus have produced state-of-the-art results in many downstream natural language processing tasks. Inspired by this development, we present molecular embeddings obtained by training an efficient transformer encoder model, MoLFormer, which uses rotary positional embeddings. This model employs a linear attention mechanism, coupled with highly distributed training, on SMILES sequences of 1.1 billion unlabelled molecules from the PubChem and ZINC datasets.… |
| https://openalex.org/W2970231061 | LXMERT: Learning Cross-Modality Encoder Representations from Transformers | 2019 |  | article | 2183 | yes | Hao Tan, Mohit Bansal | Modality (human–computer interaction), Encoder, Transformer, Computer science, Artificial intelligence, Natural language processing, +4 more | https://doi.org/10.18653/v1/d19-1514 | Hao Tan, Mohit Bansal. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019. |
| https://openalex.org/W4312186383 | Transformer-Based Weed Segmentation for Grass Management | 2022 | Sensors | article | 40 | yes | Kan Jiang, Usman Afzaal, Joonwhoan Lee | Weed, Transformer, Computer science, Artificial intelligence, Deep learning, Segmentation, +12 more | https://doi.org/10.3390/s23010065 | Weed control is among the most challenging issues for crop cultivation and turf grass management. In addition to hosting various insects and plant pathogens, weeds compete with crop for nutrients, water and sunlight. This results in problems such as the loss of crop yield, the contamination of food crops and disruption in the field aesthetics and practicality. Therefore, effective and efficient weed detection and mapping methods are indispensable. Deep learning (DL) techniques for the rapid recognition and localization of objects from images or videos have shown promising results in various areas of interest, including the agricultural sector. Attention-based Transformer models are a promising alternative to traditional constitutional neural networks (CNNs) and offer state-of-the-art results for multiple tasks in the natural language processing (NLP) domain. To this end, we exploited th… |
| https://openalex.org/W3212386989 | Attention mechanisms in computer vision: A survey | 2022 | Computational Visual Media | article | 2164 | yes | Meng-Hao Guo, Tian-Xing Xu, Jiangjiang Liu, Zheng-Ning Liu, Peng-Tao Jiang, Tai‐Jiang Mu, Song–Hai Zhang, Ralph R. Martin, +2 more | Computer science, Categorization, Artificial intelligence, Computer graphics, Process (computing), Salient, +8 more | https://doi.org/10.1007/s41095-022-0271-y | Humans can naturally and effectively find salient regions in complex scenes.\nMotivated by this observation, attention mechanisms were introduced into\ncomputer vision with the aim of imitating this aspect of the human visual\nsystem. Such an attention mechanism can be regarded as a dynamic weight\nadjustment process based on features of the input image. Attention mechanisms\nhave achieved great success in many visual tasks, including image\nclassification, object detection, semantic segmentation, video understanding,\nimage generation, 3D vision, multi-modal tasks and self-supervised learning. In\nthis survey, we provide a comprehensive review of various attention mechanisms\nin computer vision and categorize them according to approach, such as channel\nattention, spatial attention, temporal attention and branch attention; a\nrelated repository https://github.com/MenghaoGuo/Awesome-Vis… |
| https://openalex.org/W4221141417 | A Survey of Controllable Text Generation using Transformer-based Pre-trained Language Models | 2022 | arXiv (Cornell University) | preprint | 28 | yes | Hanqing Zhang, Haolin Song, Shaoyu Li, Ming Zhou, Dawei Song | Interpretability, Computer science, Transformer, Artificial intelligence, Natural language generation, Data science, +5 more | http://arxiv.org/abs/2201.05337 | Controllable Text Generation (CTG) is emerging area in the field of natural language generation (NLG). It is regarded as crucial for the development of advanced text generation technologies that better meet the specific constraints in practical applications. In recent years, methods using large-scale pre-trained language models (PLMs), in particular the widely used transformer-based PLMs, have become a new paradigm of NLG, allowing generation of more diverse and fluent text. However, due to the limited level of interpretability of deep neural networks, the controllability of these methods need to be guaranteed. To this end, controllable text generation using transformer-based PLMs has become a rapidly growing yet challenging new research hotspot. A diverse range of approaches have emerged in the recent 3-4 years, targeting different CTG tasks that require different types of controlled c… |
| https://openalex.org/W4377969438 | Deep Learning in ChatGPT - A Survey | 2023 |  | article | 23 | no | A.M. Jasmine Hashana, P. Brundha, Mohamed Uvaze Ahamed Ayoobkhan, S Fazila | Computer science, Deep learning, Artificial intelligence, Transformer, Natural language, Artificial neural network, +5 more | https://doi.org/10.1109/icoei56765.2023.10125852 | Abstract-As a subset of machine learning, deep learning makes use of multiple-layer neural networks to learn with available data and make decisions or predictions. A large language model called ChatGPT is based on deep learning, specifically a type of neural network called a transformer. ChatGPT's transformer architecture uses attention mechanisms to focus on the most important parts of the input, allowing it to process and comprehend a large amount of text data. In order for the model to comprehend the context and meaning of natural language text, it is trained on a huge database of text, including articles and books. One of the main importance of using deep learning in ChatGPT is its intelligence to understand relationships and patterns from the input text and generate or predict new text that is homogeneous to the input/training data. Because of this, ChatGPT is able to respond to qu… |
| https://openalex.org/W4367000142 | Spikingformer: A Key Foundation Model for Spiking Neural Networks | 2023 | arXiv (Cornell University) | preprint | 35 | yes | Chenlin Zhou, Liutao Yu, Zhaokun Zhou, Han Zhang, Zhengyu Ma, Huihui Zhou, Yonghong Tian, Tian, Yonghong | Spiking neural network, Computer science, Spike (software development), Computation, Residual, Transformer, +9 more | http://arxiv.org/abs/2304.11954 | Spiking neural networks (SNNs) offer a promising energy-efficient alternative to artificial neural networks, due to their event-driven spiking computation. However, some foundation SNN backbones (including Spikformer and SEW ResNet) suffer from non-spike computations (integer-float multiplications) caused by the structure of their residual connections. These non-spike computations increase SNNs' power consumption and make them unsuitable for deployment on mainstream neuromorphic hardware. In this paper, we analyze the spike-driven behavior of the residual connection methods in SNNs. We then present Spikingformer, a novel spiking transformer backbone that merges the MS Residual connection with Self-Attention in a biologically plausible way to address the non-spike computation challenge in Spikformer while maintaining global modeling capabilities. We evaluate Spikingformer across 13 datas… |
| https://openalex.org/W4382203079 | Are Transformers Effective for Time Series Forecasting? | 2023 | Proceedings of the AAAI Conference on Artificial Intelligence | article | 2117 | yes | Ailing Zeng, Muxi Chen, Lei Zhang, Qiang Xu | Transformer, Computer science, Data mining, Time series, Artificial intelligence, Machine learning, +3 more | https://doi.org/10.1609/aaai.v37i9.26317 | Recently, there has been a surge of Transformer-based solutions for the long-term time series forecasting (LTSF) task. Despite the growing performance over the past few years, we question the validity of this line of research in this work. Specifically, Transformers is arguably the most successful solution to extract the semantic correlations among the elements in a long sequence. However, in time series modeling, we are to extract the temporal relations in an ordered set of continuous points. While employing positional encoding and using tokens to embed sub-series in Transformers facilitate preserving some ordering information, the nature of the permutation-invariant self-attention mechanism inevitably results in temporal information loss. To validate our claim, we introduce a set of embarrassingly simple one-layer linear models named LTSF-Linear for comparison. Experimental results on… |
| https://openalex.org/W4391765649 | Domain-specific language models pre-trained on construction management systems corpora | 2024 | Automation in Construction | article | 40 | yes | Yunshun Zhong, Sebastian D. Goodfellow | Computer science, Domain (mathematical analysis), Natural language processing, Artificial intelligence, Domain-specific language, Construction management, +5 more | https://doi.org/10.1016/j.autcon.2024.105316 | The rising demand for automated methods in the Construction Management Systems (CMS) sector highlights opportunities for the Transformer architecture, which enables pre-training Deep Learning models on large, unlabeled datasets for Natural Language Processing (NLP) tasks, outperforming traditional Recurrent Neural Network models. However, their potential in the CMS domain remains underexplored. Therefore, this research produced the first CMS domain corpora from academic papers and introduced an end-to-end pipeline for pre-training and fine-tuning domain-specific Pre-trained Language Models. Four corpora were constructed and transfer learning was employed to pre-train BERT and RoBERTa using the corpora. The best-performing models were then fine-tuned and outperformed models pre-trained on general corpora. In two key NLP tasks, text classification using an infrastructure condition predict… |
| https://openalex.org/W4392292170 | Artificial intelligence: reflecting on the past and looking towards the next paradigm shift | 2024 | Journal of Experimental & Theoretical Artificial Intelligence | article | 37 | yes | Petar Radanliev | Computer science, Paradigm shift, Data science, Artificial intelligence, Human–computer interaction, Epistemology, +1 more | https://doi.org/10.1080/0952813x.2024.2323042 | Artificial intelligence (AI) has undergone major advances over the past decades, propelled by key innovations in machine learning and the availability of big data and computing power. This paper surveys the historical progress of AI from its origins in logic-based systems like the Logic Theorist to recent deep learning breakthroughs like Bidirectional Encoder Representations from Transformers (BERT), Generative Pretrained Transformer 3 (GPT-3) and Large Language Model Meta AI (LLaMA). The early rule-based systems using handcrafted expertise gave way to statistical learning techniques and neural networks trained on large datasets. Milestones like AlexNet and AlphaGo established deep learning as a dominant AI approach. Transfer learning enabled models pre-trained on diverse corpora to excel at specialised downstream tasks. The scope of AI expanded from niche applications like playing ches… |
| https://openalex.org/W4309791143 | End-to-End Protein Normal Mode Frequency Predictions Using Language and Graph Models and Application to Sonification | 2022 | ACS Nano | article | 24 | no | Yiwen Hu, Markus J. Buehler | Transformer, Computer science, End-to-end principle, Graph, Artificial neural network, Artificial intelligence, +4 more | https://doi.org/10.1021/acsnano.2c07681 | The prediction of mechanical and dynamical properties of proteins is an important frontier, especially given the greater availability of proteins structures. Here we report a series of models that provide end-to-end predictions of nanodynamical properties of proteins, focused on high-throughput normal mode predictions directly from the amino acid sequence. Using neural network models within the family of Natural Language Processing and graph-based methods, we offer atomistically based mechanistic predictions of key protein mechanical features. The models include an end-to-end long short-term memory (LSTM) model, an end-to-end transformer model, a graph-based transformer model, and an equivariant graph neural network. All four models show exceptional performance, with the graph-based transformer architecture offering the best results but at the cost of requiring a graph structure as inpu… |
| https://openalex.org/W4321636216 | Predicting Generalized Anxiety Disorder From Impromptu Speech Transcripts Using Context-Aware Transformer-Based Neural Networks: Model Evaluation Study | 2023 | JMIR Mental Health | article | 23 | yes | Bazen Gashaw Teferra, Jonathan Rose | Impromptu, Psychology, Anxiety, Logistic regression, Receiver operating characteristic, Artificial intelligence, +11 more | https://doi.org/10.2196/44325 | Background The ability to automatically detect anxiety disorders from speech could be useful as a screening tool for an anxiety disorder. Prior studies have shown that individual words in textual transcripts of speech have an association with anxiety severity. Transformer-based neural networks are models that have been recently shown to have powerful predictive capabilities based on the context of more than one input word. Transformers detect linguistic patterns and can be separately trained to make specific predictions based on these patterns. Objective This study aimed to determine whether a transformer-based language model can be used to screen for generalized anxiety disorder from impromptu speech transcripts. Methods A total of 2000 participants provided an impromptu speech sample in response to a modified version of the Trier Social Stress Test (TSST). They also completed the Gene… |
| https://openalex.org/W3156636935 | SimCSE: Simple Contrastive Learning of Sentence Embeddings | 2021 | Proceedings of the 2021 Conference on Empirical Methods in Natural Language Pro… | article | 2377 | yes | Tianyu Gao, Xingcheng Yao, Danqi Chen | Artificial intelligence, Computer science, Natural language processing, Sentence, Simple (philosophy), Feature learning, +4 more | https://doi.org/10.18653/v1/2021.emnlp-main.552 | This paper presents SimCSE, a simple contrastive learning framework that greatly advances the state-of-the-art sentence embeddings. We first describe an unsupervised approach, which takes an input sentence and predicts itself in a contrastive objective, with only standard dropout used as noise. This simple method works surprisingly well, performing on par with previous supervised counterparts. We find that dropout acts as minimal data augmentation and removing it leads to a representation collapse. Then, we propose a supervised approach, which incorporates annotated pairs from natural language inference datasets into our contrastive learning framework, by using “entailment” pairs as positives and “contradiction” pairs as hard negatives. We evaluate SimCSE on standard semantic textual similarity (STS) tasks, and our unsupervised and supervised models using BERT base achieve an average of… |
| https://openalex.org/W4286801089 | Fighting hate speech from bilingual hinglish speaker’s perspective, a transformer- and translation-based approach. | 2022 | Social Network Analysis and Mining | article | 21 | yes | Shankar Biradar, Sunil Saumya, Arun Chauhan | Computer science, Transformer, Interpreter, Code-switching, Natural language processing, Artificial intelligence, +14 more | https://doi.org/10.1007/s13278-022-00920-w |  |
| https://openalex.org/W3011207117 | Neural Machine Translation for the Bangla-English Language Pair | 2019 |  | article | 24 | no | Md. Arid Hasan, Firoj Alam, Shammur Absar Chowdhury, Naira Khan | Machine translation, Computer science, Bengali, Transformer, Artificial intelligence, Natural language processing, +10 more | https://doi.org/10.1109/iccit48885.2019.9038381 | Due to the rapid advancement of different neural network architectures, the task of automated translation from one language to another is now in a new era of Machine Translation (MT) research. In the last few years, Neural Machine Translation (NMT) architectures have proven to be successful for resource-rich languages, trained on a large dataset of translated sentences, with variations of NMT algorithms used to train the model. In this study, we explore different NMT algorithms - Bidirectional Long Short Term Memory (LSTM) and Transformer based NMT, to translate the Bangla to English language pair. For the experiments, we used different datasets and our experimental results outperform the existing performance by a large margin on different datasets. We also investigated the factors affecting the data quality and how they influence the performance of the models. It shows a promising rese… |
| https://openalex.org/W4308245819 | On the Adversarial Robustness of Vision Transformers | 2021 | arXiv (Cornell University) | preprint | 50 | yes | Rulin Shao, Zhouxing Shi, Jinfeng Yi, Pin‐Yu Chen, Cho‐Jui Hsieh | Robustness (evolution), Computer science, Adversarial system, Artificial intelligence, Convolutional neural network, Transformer, +9 more | http://arxiv.org/abs/2103.15670 | Following the success in advancing natural language processing and understanding, transformers are expected to bring revolutionary changes to computer vision. This work provides a comprehensive study on the robustness of vision transformers (ViTs) against adversarial perturbations. Tested on various white-box and transfer attack settings, we find that ViTs possess better adversarial robustness when compared with MLP-Mixer and convolutional neural networks (CNNs) including ConvNeXt, and this observation also holds for certified robustness. Through frequency analysis and feature visualization, we summarize the following main observations contributing to the improved robustness of ViTs: 1) Features learned by ViTs contain less high-frequency patterns that have spurious correlation, which helps explain why ViTs are less sensitive to high-frequency perturbations than CNNs and MLP-Mixer, and… |
| https://openalex.org/W4319598896 | The neural machine translation models for the low-resource Kazakh–English language pair | 2023 | PeerJ Computer Science | article | 18 | yes | Vladislav Karyukin, Diana Rakhimova, Aidana Karibayeva, Aliya Turganbayeva, Asem Turarbek | Machine translation, Computer science, Artificial intelligence, Transformer, Natural language processing, Example-based machine translation, +8 more | https://doi.org/10.7717/peerj-cs.1224 | The development of the machine translation field was driven by people’s need to communicate with each other globally by automatically translating words, sentences, and texts from one language into another. The neural machine translation approach has become one of the most significant in recent years. This approach requires large parallel corpora not available for low-resource languages, such as the Kazakh language, which makes it difficult to achieve the high performance of the neural machine translation models. This article explores the existing methods for dealing with low-resource languages by artificially increasing the size of the corpora and improving the performance of the Kazakh–English machine translation models. These methods are called forward translation, backward translation, and transfer learning. Then the Sequence-to-Sequence (recurrent neural network and bidirectional re… |
| https://openalex.org/W4406431845 | Artificial Intelligence Scribe and Large Language Model Technology in Healthcare Documentation: Advantages, Limitations, and Recommendations | 2025 | Plastic & Reconstructive Surgery Global Open | article | 45 | yes | Sarah A. Mess, Alison Mackey, David Yarowsky | Documentation, Computer science, Health care, Natural language processing, Data science, Artificial intelligence, +4 more | https://doi.org/10.1097/gox.0000000000006450 | Summary: Artificial intelligence (AI) scribe applications in the healthcare community are in the early adoption phase and offer unprecedented efficiency for medical documentation. They typically use an application programming interface with a large language model (LLM), for example, generative pretrained transformer 4. They use automatic speech recognition on the physician–patient interaction, generating a full medical note for the encounter, together with a draft follow-up e-mail for the patient and, often, recommendations, all within seconds or minutes. This provides physicians with increased cognitive freedom during medical encounters due to less time needed interfacing with electronic medical records. However, careful proofreading of the AI-generated language by the physician signing the note is essential. Insidious and potentially significant errors of omission, fabrication, or sub… |
| https://openalex.org/W4386214355 | VulDetect: A novel technique for detecting software vulnerabilities using Language Models | 2023 |  | article | 34 | no | Marwan Omar, Stavros Shiaeles | Computer science, Deep learning, Convolutional neural network, Benchmark (surveying), Software deployment, Code (set theory), +16 more | https://doi.org/10.1109/csr57506.2023.10224924 | Recently, deep learning techniques have garnered substantial attention for their ability to identify vulnerable code patterns accurately. However, current state-of-the-art deep learning models, such as Convolutional Neural Networks (CNN), and Long Short-Term Memories (LSTMs) require substantial computational resources. This results in a level of overhead that makes their implementation unfeasible for deployment in realtime settings. This study presents a novel transformer-based vulnerability detection framework, referred to as VulDetect, which is achieved through the fine-tuning of a pretrained large language model, (GPT) on various benchmark datasets of vulnerable code. Our empirical findings indicate that our framework is capable of identifying vulnerable software code with an accuracy of up to 92.65%. Our proposed technique outperforms SyseVR and VuIDeBERT, two state-of-the-art vulne… |
| https://openalex.org/W3032960712 | An Overview of Neural Network Compression | 2020 | arXiv (Cornell University) | preprint | 23 | yes | James O’Neill | Deep learning, Computer science, Tensor decomposition, Artificial neural network, Artificial intelligence, Convolutional neural network, +9 more | http://export.arxiv.org/pdf/2006.03669 | Overparameterized networks trained to convergence have shown impressive performance in domains such as computer vision and natural language processing. Pushing state of the art on salient tasks within these domains corresponds to these models becoming larger and more difficult for machine learning practitioners to use given the increasing memory and storage requirements, not to mention the larger carbon footprint. Thus, in recent years there has been a resurgence in model compression techniques, particularly for deep convolutional neural networks and self-attention based networks such as the Transformer. Hence, this paper provides a timely overview of both old and current compression techniques for deep neural networks, including pruning, quantization, tensor decomposition, knowledge distillation and combinations thereof. We assume a basic familiarity with deep learning architectures\fo… |
| https://openalex.org/W4390783938 | Advancing bioinformatics with large language models: components, applications and perspectives | 2024 | PubMed | preprint | 26 | yes | Jiajia Liu, Mengyuan Yang, Yankai Yu, Haixia Xu, Kang Li, Xiaobo Zhou, Zhou, Xiaobo | Computer science, Artificial intelligence, Language model, Natural language processing, Natural language, Focus (optics), +4 more | http://arxiv.org/abs/2401.04155 | Large language models (LLMs) are a class of artificial intelligence models based on deep learning, which have great performance in various tasks, especially in natural language processing (NLP). Large language models typically consist of artificial neural networks with numerous parameters, trained on large amounts of unlabeled input using self-supervised or semi-supervised learning. However, their potential for solving bioinformatics problems may even exceed their proficiency in modeling human language. In this review, we will provide a comprehensive overview of the essential components of large language models (LLMs) in bioinformatics, spanning genomics, transcriptomics, proteomics, drug discovery, and single-cell analysis. Key aspects covered include tokenization methods for diverse data types, the architecture of transformer models, the core attention mechanism, and the pre-training… |
| https://openalex.org/W3168770049 | A Review of On-Device Fully Neural End-to-End Automatic Speech Recognition Algorithms | 2020 |  | review | 25 | no | Chanwoo Kim, Dhananjaya Gowda, Dongsoo Lee, Jiyeon Kim, Ankur Kumar, Sung-Soo Kim, Abhinav Garg, Changwoo Han | Computer science, Speech recognition, End-to-end principle, Language model, Artificial neural network, Recurrent neural network, +11 more | https://doi.org/10.1109/ieeeconf51394.2020.9443456 | In this paper, we review various end-to-end automatic speech recognition algorithms and their optimization techniques for on-device applications. Conventional speech recognition systems comprise a large number of discrete components such as an acoustic model, a language model, a pronunciation model, a text-normalizer, an inverse-text normalizer, a decoder based on a Weighted Finite State Transducer (WFST), and so on. To obtain sufficiently high speech recognition accuracy with such conventional speech recognition systems, a very large language model (up to 100 GB) is usually needed. Hence, the corresponding WFST size becomes enormous, which prohibits their on-device implementation. Recently, fully neural network end-to-end speech recognition algorithms have been proposed. Examples include speech recognition systems based on Connectionist Temporal Classification (CTC), Recurrent Neural N… |
| https://openalex.org/W3103017991 | Everybody Sign Now: Translating Spoken Language to Photo Realistic Sign Language Video | 2020 | arXiv (Cornell University) | preprint | 36 | yes | Ben Saunders, Necati Cihan Camgöz, Richard Bowden | Sign language, Computer science, Inference, American Sign Language, Artificial intelligence, Interpreter, +10 more | http://arxiv.org/abs/2011.09846 | To be truly understandable and accepted by Deaf communities, an automatic Sign Language Production (SLP) system must generate a photo-realistic signer. Prior approaches based on graphical avatars have proven unpopular, whereas recent neural SLP works that produce skeleton pose sequences have been shown to be not understandable to Deaf viewers. In this paper, we propose SignGAN, the first SLP model to produce photo-realistic continuous sign language videos directly from spoken language. We employ a transformer architecture with a Mixture Density Network (MDN) formulation to handle the translation from spoken language to skeletal pose. A pose-conditioned human synthesis model is then introduced to generate a photo-realistic sign language video from the skeletal pose sequence. This allows the photo-realistic production of sign videos directly translated from written text. We further propos… |
| https://openalex.org/W4200350226 | What Does a Language-And-Vision Transformer See: The Impact of Semantic Information on Visual Representations | 2021 | Frontiers in Artificial Intelligence | article | 23 | yes | Nikolai Ilinykh, Simon Dobnik | Computer science, Closed captioning, Transformer, Artificial intelligence, Visual Objects, Natural language processing, +13 more | https://doi.org/10.3389/frai.2021.767971 | Neural networks have proven to be very successful in automatically capturing the composition of language and different structures across a range of multi-modal tasks. Thus, an important question to investigate is how neural networks learn and organise such structures. Numerous studies have examined the knowledge captured by language models (LSTMs, transformers) and vision architectures (CNNs, vision transformers) for respective uni-modal tasks. However, very few have explored what structures are acquired by multi-modal transformers where linguistic and visual features are combined. It is critical to understand the representations learned by each modality, their respective interplay, and the task’s effect on these representations in large-scale architectures. In this paper, we take a multi-modal transformer trained for image captioning and examine the structure of the self-attention patt… |
| https://openalex.org/W3102985954 | End to End Binarized Neural Networks for Text Classification | 2020 |  | article | 26 | yes | Kumar Shridhar, Harshil Jain, Akshat Agarwal, Denis Kleyko | Computer science, Artificial neural network, End-to-end principle, Artificial intelligence, Classifier (UML), Transformer, +12 more | https://doi.org/10.18653/v1/2020.sustainlp-1.4 | Deep neural networks have demonstrated their superior performance in almost every Natural Language Processing task, however, their increasing complexity raises concerns. A particular concern is that these networks pose high requirements for computing hardware and training budgets. The state-of-the-art transformer models are a vivid example. Simplifying the computations performed by a network is one way of addressing the issue of the increasing complexity. In this paper, we propose an end to end binarized neural network for the task of intent and text classification. In order to fully utilize the potential of end to end binarization, both the input representations (vector embeddings of tokens statistics) and the classifier are binarized. We demonstrate the efficiency of such a network on the intent classification of short texts over three datasets and text classification with a larger da… |
| https://openalex.org/W4366462849 | Molecular language models: RNNs or transformer? | 2023 | Briefings in Functional Genomics | article | 26 | no | Yangyang Chen, Zixu Wang, Xiangxiang Zeng, Yayang Li, Pengyong Li, Xiucai Ye, Tetsuya Sakurai | Biology, Transformer, Computational biology, Artificial intelligence, Computer science, Engineering, +2 more | https://doi.org/10.1093/bfgp/elad012 | Abstract Language models have shown the capacity to learn complex molecular distributions. In the field of molecular generation, they are designed to explore the distribution of molecules, and previous studies have demonstrated their ability to learn molecule sequences. In the early times, recurrent neural networks (RNNs) were widely used for feature extraction from sequence data and have been used for various molecule generation tasks. In recent years, the attention mechanism for sequence data has become popular. It captures the underlying relationships between words and is widely applied to language models. The Transformer-Layer, a model based on a self-attentive mechanism, also shines the same as the RNN-based model. In this research, we investigated the difference between RNNs and the Transformer-Layer to learn a more complex distribution of molecules. For this purpose, we experimen… |
| https://openalex.org/W4383955291 | Transformer Architecture-Based Transfer Learning for Politeness Prediction in Conversation | 2023 | Sustainability | article | 17 | yes | Shakir Khan, Mohd Fazil, Agbotiname Lucky Imoize, Bayan Alabduallah, Bader M. Albahlal, Saad Abdullah Alajlan, Abrar Almjally, Tamanna Siddiqui | Politeness, Computer science, Transformer, Artificial intelligence, Softmax function, Language model, +13 more | https://doi.org/10.3390/su151410828 | Politeness is an essential part of a conversation. Like verbal communication, politeness in textual conversation and social media posts is also stimulating. Therefore, the automatic detection of politeness is a significant and relevant problem. The existing literature generally employs classical machine learning-based models like naive Bayes and Support Vector-based trained models for politeness prediction. This paper exploits the state-of-the-art (SOTA) transformer architecture and transfer learning for respectability prediction. The proposed model employs the strengths of context-incorporating large language models, a feed-forward neural network, and an attention mechanism for representation learning of natural language requests. The trained representation is further classified using a softmax function into polite, impolite, and neutral classes. We evaluate the presented model employi… |
| https://openalex.org/W4292508613 | FieldPerceiver: Domain agnostic transformer model to predict multiscale physical fields and nonlinear material properties through neural ologs | 2022 | Materials Today | article | 45 | no | Markus J. Buehler | Computer science, Nonlinear system, Artificial neural network, Convolutional neural network, Transformer, Physical system, +9 more | https://doi.org/10.1016/j.mattod.2022.05.020 |  |
| https://openalex.org/W4382463801 | Improving Dynamic HDR Imaging with Fusion Transformer | 2023 | Proceedings of the AAAI Conference on Artificial Intelligence | article | 21 | yes | Rufeng Chen, Bolun Zheng, Hua Zhang, Quan Chen, Chenggang Yan, Greg Slabaugh, Shanxin Yuan | Ghosting, High-dynamic-range imaging, Computer science, Artificial intelligence, High dynamic range, Computer vision, +9 more | https://doi.org/10.1609/aaai.v37i1.25107 | Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets. |
| https://openalex.org/W4401843926 | Generative artificial intelligence in smart manufacturing | 2024 | Journal of Intelligent Manufacturing | article | 22 | yes | Andrew Kusiak | Generative grammar, Artificial intelligence, Computer science, Production (economics), Smart manufacturing, Engineering, +3 more | https://doi.org/10.1007/s10845-024-02480-6 |  |
| https://openalex.org/W4401357474 | A Large Dataset to Enhance Skin Cancer Classification With Transformer-Based Deep Neural Networks | 2024 | IEEE Access | article | 20 | yes | Mirco Gallazzi, Sara Biavaschi, Alessandro Bulgheroni, Tommaso M. Gatti, Silvia Corchs, Ignazio Gallo | Computer science, Deep learning, Artificial intelligence, Transformer, Artificial neural network, Machine learning, +9 more | https://doi.org/10.1109/access.2024.3439365 | The advent of Deep Learning methodologies has revolutionized the field of medical image analysis, particularly in skin lesion diagnosis and classification. This paper proposes an explorative approach utilizing Transformer-based deep neural networks to classify multiclass skin lesion datasets. Initially introduced for natural language processing tasks, Transformers have remarkably succeeded in capturing long-range dependencies in sequential data. However, their application to image data, especially in medical imaging, remains relatively unexplored. Our proposed framework leverages the self-attention mechanism of Transformer models to effectively capture spatial dependencies across image regions without relying on handcrafted features or extensive pre-processing. We present a comprehensive evaluation of several Deep Learning models on skin imaging reference datasets for various types of s… |
| https://openalex.org/W4391043254 | A comparative study of cross-lingual sentiment analysis | 2024 | Expert Systems with Applications | article | 26 | yes | Pavel Přibáň, Jakub Šmíd, Josef Steinberger, Adam Mištera | Computer science, Sentiment analysis, Natural language processing, Artificial intelligence | https://doi.org/10.1016/j.eswa.2024.123247 | This paper presents a detailed comparative study of the zero-shot cross-lingual sentiment analysis. Namely, we use modern multilingual Transformer-based models and linear transformations combined with CNN and LSTM neural networks. We evaluate their performance in Czech, French, and English. We aim to compare and assess the models' ability to transfer knowledge across languages and discuss the trade-off between their performance and training/inference speed. We build strong monolingual baselines comparable with the current SotA approaches, achieving state-of-the-art results in Czech (96.0% accuracy) and French (97.6% accuracy). Next, we compare our results with the latest large language models (LLMs), i.e., Llama 2 and ChatGPT. We show that the large multilingual Transformer-based XLM-R model consistently outperforms all other cross-lingual approaches in zero-shot cross-lingual sentiment… |
| https://openalex.org/W4366823741 | Automated labelling of radiology reports using natural language processing: Comparison of traditional and newer methods | 2023 | Health care science | review | 17 | yes | Seo Yi Chng, Paul Jie Wen Tern, Rui Xian Matthew Kan, Lionel Tim‐Ee Cheng | Computer science, Artificial intelligence, Labelling, Preprocessor, Lexical analysis, Machine learning, +13 more | https://doi.org/10.1002/hcs2.40 | Abstract Automated labelling of radiology reports using natural language processing allows for the labelling of ground truth for large datasets of radiological studies that are required for training of computer vision models. This paper explains the necessary data preprocessing steps, reviews the main methods for automated labelling and compares their performance. There are four main methods of automated labelling, namely: (1) rules‐based text‐matching algorithms, (2) conventional machine learning models, (3) neural network models and (4) Bidirectional Encoder Representations from Transformers (BERT) models. Rules‐based labellers perform a brute force search against manually curated keywords and are able to achieve high F1 scores. However, they require proper handling of negative words. Machine learning models require preprocessing that involves tokenization and vectorization of text in… |
| https://openalex.org/W4384282814 | Inferring cancer disease response from radiology reports using large language models with data augmentation and prompting | 2023 | Journal of the American Medical Informatics Association | article | 35 | yes | Ryan Tan, Qian Lin, Guat Hwa Low, Ruixi Lin, Tzer Chew Goh, Christopher Chu En Chang, Fung Fung Lee, Wei Yin Chan, +16 more | Computer science, Test set, Machine learning, Response Evaluation Criteria in Solid Tumors, Artificial intelligence, Language model, +7 more | https://doi.org/10.1093/jamia/ocad133 | Abstract Objective To assess large language models on their ability to accurately infer cancer disease response from free-text radiology reports. Materials and Methods We assembled 10 602 computed tomography reports from cancer patients seen at a single institution. All reports were classified into: no evidence of disease, partial response, stable disease, or progressive disease. We applied transformer models, a bidirectional long short-term memory model, a convolutional neural network model, and conventional machine learning methods to this task. Data augmentation using sentence permutation with consistency loss as well as prompt-based fine-tuning were used on the best-performing models. Models were validated on a hold-out test set and an external validation set based on Response Evaluation Criteria in Solid Tumors (RECIST) classifications. Results The best-performing model was the Gat… |
| https://openalex.org/W4213060092 | Transformer-Based Abstractive Summarization for Reddit and Twitter: Single Posts vs. Comment Pools in Three Languages | 2022 | Future Internet | article | 23 | yes | Ivan S. Blekanov, Nikita Tarasov, Svetlana S. Bodrunova | Automatic summarization, Computer science, Social media, Transformer, Artificial intelligence, Information retrieval, +6 more | https://doi.org/10.3390/fi14030069 | Abstractive summarization is a technique that allows for extracting condensed meanings from long texts, with a variety of potential practical applications. Nonetheless, today’s abstractive summarization research is limited to testing the models on various types of data, which brings only marginal improvements and does not lead to massive practical employment of the method. In particular, abstractive summarization is not used for social media research, where it would be very useful for opinion and topic mining due to the complications that social media data create for other methods of textual analysis. Of all social media, Reddit is most frequently used for testing new neural models of text summarization on large-scale datasets in English, without further testing on real-world smaller-size data in various languages or from various other platforms. Moreover, for social media, summarizing… |
| https://openalex.org/W2970476646 | Language Models as Knowledge Bases? | 2019 |  | article | 1685 | yes | Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander Miller | Miller, Computer science, Natural language processing, Cognitive science, Natural language, Artificial intelligence, +5 more | https://doi.org/10.18653/v1/d19-1250 | Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander Miller. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019. |
| https://openalex.org/W4392153289 | Deep Learning Approaches for Feature Extraction in Big Data Analytics | 2023 |  | article | 26 | no | Amol Dattatray Dhaygude, Raj Varma, Poonam Yerpude, Suman Kumar Swarnkar, Rupesh Kumar Jindal, Fazle Rabbi | Computer science, Big data, Analytics, Artificial intelligence, Feature extraction, Deep learning, +6 more | https://doi.org/10.1109/upcon59197.2023.10434607 | In the context of big data analytics, this study examines the use of algorithms based on deep learning for feature extraction. Traditional methods usually have trouble sifting through the complexity and volume of data to find the important elements. We investigate the application of auto encoders, transformer-based models, convolutional neural networks (CNNs), and recurrent neural networks (RNNs) to address this problem. Our comprehensive review of existing literature compares these deep learning techniques with traditional methods and highlights their adaptability to large-scale datasets. The efficacy and precision of these methodologies are demonstrated by empirical investigations conducted on authentic datasets across a range of disciplines, including but not limited to time-series analysis, picture identification, and natural language processing. Despite challenges like computationa… |
| https://openalex.org/W4221158409 | Personalized Prompt Learning for Explainable Recommendation | 2022 | arXiv (Cornell University) | preprint | 29 | yes | Lei Li, Yongfeng Zhang, Li Chen | Computer science, Recommender system, Artificial intelligence, Identifier, Regularization (linguistics), Transformer, +6 more | http://arxiv.org/abs/2202.07371 | Providing user-understandable explanations to justify recommendations could help users better understand the recommended items, increase the system's ease of use, and gain users' trust. A typical approach to realize it is natural language generation. However, previous works mostly adopt recurrent neural networks to meet the ends, leaving the potentially more effective pre-trained Transformer models under-explored. In fact, user and item IDs, as important identifiers in recommender systems, are inherently in different semantic space as words that pre-trained models were already trained on. Thus, how to effectively fuse IDs into such models becomes a critical issue. Inspired by recent advancement in prompt learning, we come up with two solutions: find alternative words to represent IDs (called discrete prompt learning), and directly input ID vectors to a pre-trained model (termed continuo… |
| https://openalex.org/W3175394187 | LexFit: Lexical Fine-Tuning of Pretrained Language Models | 2021 |  | article | 22 | yes | Ivan Vulić, Edoardo Maria Ponti, Anna Korhonen, Goran Glavaš | Computer science, Göran, Computational linguistics, Natural language processing, Joint (building), Volume (thermodynamics), +13 more | https://doi.org/10.18653/v1/2021.acl-long.410 | Transformer-based language models (LMs) pretrained on large text collections implicitly store a wealth of lexical semantic knowledge, but it is non-trivial to extract that knowledge effectively from their parameters. Inspired by prior work on semantic specialization of static word embedding (WE) models, we show that it is possible to expose and enrich lexical knowledge from the LMs, that is, to specialize them to serve as effective and universal ``decontextualized'' word encoders even when fed input words ``in isolation'' (i.e., without any context). Their transformation into such word encoders is achieved through a simple and efficient lexical fine-tuning procedure (termed LexFit) based on dual-encoder network structures. Further, we show that LexFit can yield effective word encoders even with limited lexical supervision and, via cross-lingual transfer, in different languages without a… |
| https://openalex.org/W4382322607 | Revolutionizing Cyber Threat Detection with Large Language Models: A privacy-preserving BERT-based Lightweight Model for IoT/IIoT Devices | 2023 | arXiv (Cornell University) | preprint | 21 | yes | Mohamed Amine Ferrag, Mthandazo Ndhlovu, Norbert Tihanyi, Lucas C. Cordeiro, Mérouane Debbah, Thierry Lestable, Thandi, Narinderjit Singh | Computer science, Deep learning, Encoder, Artificial intelligence, Software deployment, Convolutional neural network, +16 more | http://arxiv.org/abs/2306.14263 | The field of Natural Language Processing (NLP) is currently undergoing a revolutionary transformation driven by the power of pre-trained Large Language Models (LLMs) based on groundbreaking Transformer architectures. As the frequency and diversity of cybersecurity attacks continue to rise, the importance of incident detection has significantly increased. IoT devices are expanding rapidly, resulting in a growing need for efficient techniques to autonomously identify network-based attacks in IoT networks with both high precision and minimal computational requirements. This paper presents SecurityBERT, a novel architecture that leverages the Bidirectional Encoder Representations from Transformers (BERT) model for cyber threat detection in IoT networks. During the training of SecurityBERT, we incorporated a novel privacy-preserving encoding technique called Privacy-Preserving Fixed-Length E… |
| https://openalex.org/W4380986416 | Deep learning for opinion mining and topic classification of course reviews | 2023 | Education and Information Technologies | article | 34 | yes | Anna Koufakou | Computer science, Artificial intelligence, Sentiment analysis, Machine learning, Support vector machine, Macro, +4 more | https://doi.org/10.1007/s10639-023-11736-2 | Abstract Student opinions for a course are important to educators and administrators, regardless of the type of the course or the institution. Reading and manually analyzing open-ended feedback becomes infeasible for massive volumes of comments at institution level or online forums. In this paper, we collected and pre-processed a large number of course reviews publicly available online. We applied machine learning techniques with the goal to gain insight into student sentiments and topics. Specifically, we utilized current Natural Language Processing (NLP) techniques, such as word embeddings and deep neural networks, and state-of-the-art BERT (Bidirectional Encoder Representations from Transformers), RoBERTa (Robustly optimized BERT approach) and XLNet (Generalized Auto-regression Pre-training). We performed extensive experimentation to compare these techniques versus traditional approa… |
| https://openalex.org/W4387424400 | BERT models for Brazilian Portuguese: Pretraining, evaluation and tokenization analysis | 2023 | Applied Soft Computing | article | 23 | no | Fábio C. de Souza, Rodrigo Nogueira, Roberto Lotufo | Computer science, Artificial intelligence, Natural language processing, Language model, Transformer, Lexical analysis, +13 more | https://doi.org/10.1016/j.asoc.2023.110901 |  |
| https://openalex.org/W4313257365 | Attentive deep neural networks for legal document retrieval | 2022 | Artificial Intelligence and Law | article | 36 | yes | Ha-Thanh Nguyen, Manh-Kien Phi, Xuan-Bach Ngo, Vu Tran, Le-Minh Nguyen, Minh-Phuong Tu | Computer science, Information retrieval, Artificial intelligence, Question answering, Natural language processing, Document retrieval, +6 more | http://arxiv.org/abs/2212.13899 |  |
| https://openalex.org/W4403980797 | The Evolution of Artificial Intelligence in Medical Imaging: From Computer Science to Machine and Deep Learning | 2024 | Cancers | review | 32 | yes | Michele Avanzo, Joseph Stancanello, G. Pirrone, Annalisa Drigo, Alessandra Retico | Artificial intelligence, Computer science, Deep learning, Medical imaging, Machine learning | https://doi.org/10.3390/cancers16213702 | Artificial intelligence (AI), the wide spectrum of technologies aiming to give machines or computers the ability to perform human-like cognitive functions, began in the 1940s with the first abstract models of intelligent machines. Soon after, in the 1950s and 1960s, machine learning algorithms such as neural networks and decision trees ignited significant enthusiasm. More recent advancements include the refinement of learning algorithms, the development of convolutional neural networks to efficiently analyze images, and methods to synthesize new images. This renewed enthusiasm was also due to the increase in computational power with graphical processing units and the availability of large digital databases to be mined by neural networks. AI soon began to be applied in medicine, first through expert systems designed to support the clinician’s decision and later with neural networks for t… |
| https://openalex.org/W3008587939 | End-to-End Training of a Large Vocabulary End-to-End Speech Recognition System | 2019 | 2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU) | article | 35 | no | Chanwoo Kim, Sung-Soo Kim, Kwangyoun Kim, Mehul Kumar, Jiyeon Kim, Kyungmin Lee, Changwoo Han, Abhinav Garg, +5 more | End-to-end principle, Computer science, Vocabulary, Speech recognition, End user, Training (meteorology), +6 more | https://doi.org/10.1109/asru46091.2019.9003976 | In this paper, we present an end-to-end training framework for building state-of-the-art end-to-end speech recognition systems. Our training system utilizes a cluster of Central Processing Units (CPUs) and Graphics Processing Units (GPUs). The entire data reading, large scale data augmentation, neural network parameter updates are all performed "on-the-fly". We use vocal tract length perturbation [1] and an acoustic simulator [2] for data augmentation. The processed features and labels are sent to the GPU cluster. The Horovod allreduce approach is employed to train neural network parameters. We evaluated the effectiveness of our system on the standard Librispeech corpus [3] and the 10,000-hr anonymized Bixby English dataset. Our end-to-end speech recognition system built using this training infrastructure showed a 2.44 % WER on test-clean of the LibriSpeech test set after applying shall… |
| https://openalex.org/W4394967854 | Potential of Large Language Models in Health Care: Delphi Study | 2024 | Journal of Medical Internet Research | article | 75 | yes | Kerstin Denecke, Richard May, Octavio Rivera-Romero, Octavio Rivera Romero | Health care, Health informatics, Strengths and weaknesses, Context (archaeology), Documentation, Delphi method, +13 more | https://doi.org/10.2196/52399 | Background A large language model (LLM) is a machine learning model inferred from text data that captures subtle patterns of language use in context. Modern LLMs are based on neural network architectures that incorporate transformer methods. They allow the model to relate words together through attention to multiple words in a text sequence. LLMs have been shown to be highly effective for a range of tasks in natural language processing (NLP), including classification and information extraction tasks and generative applications. Objective The aim of this adapted Delphi study was to collect researchers’ opinions on how LLMs might influence health care and on the strengths, weaknesses, opportunities, and threats of LLM use in health care. Methods We invited researchers in the fields of health informatics, nursing informatics, and medical NLP to share their opinions on LLM use in health car… |
| https://openalex.org/W3105966348 | TinyBERT: Distilling BERT for Natural Language Understanding | 2020 |  | article | 1534 | yes | Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Dong Chen, Linlin Li, Fang Wang, Qun Liu | Computer science, Transformer, Distillation, Inference, Benchmark (surveying), Language model, +15 more | https://doi.org/10.18653/v1/2020.findings-emnlp.372 | Language model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks. However, pre-trained language models are usually computationally expensive, so it is difficult to efficiently execute them on resource-restricted devices. To accelerate inference and reduce model size while maintaining accuracy, we first propose a novel Transformer distillation method that is specially designed for knowledge distillation (KD) of the Transformer-based models. By leveraging this new KD method, the plenty of knowledge encoded in a large “teacher” BERT can be effectively transferred to a small “student” TinyBERT. Then, we introduce a new two-stage learning framework for TinyBERT, which performs Transformer distillation at both the pre-training and task-specific learning stages. This framework ensures that TinyBERT can capture the general-domain a… |
| https://openalex.org/W2747680751 | Natural language processing: state of the art, current trends and challenges | 2022 | Multimedia Tools and Applications | article | 1600 | yes | Diksha Khurana, Aditya Koli, Kiran Khatter, Sukhdev Singh | Automatic summarization, Computer science, Natural language processing, Machine translation, Artificial intelligence, Question answering, +10 more | https://doi.org/10.1007/s11042-022-13428-4 |  |
| https://openalex.org/W2970419734 | Text Summarization with Pretrained Encoders | 2019 |  | article | 1561 | yes | Yang Liu, Mirella Lapata | Automatic summarization, Computer science, Encoder, Natural language processing, Artificial intelligence, Information retrieval, +1 more | https://doi.org/10.18653/v1/d19-1387 | Yang Liu, Mirella Lapata. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019. |
| https://openalex.org/W3176923149 | Text Data Augmentation for Deep Learning | 2021 | Journal Of Big Data | article | 1630 | yes | Connor Shorten, Taghi M. Khoshgoftaar, Borko Furht | Computer science, Computational Science and Engineering, Deep learning, Artificial intelligence, Data science, Machine learning | https://doi.org/10.1186/s40537-021-00492-0 |  |
| https://openalex.org/W4224282638 | Optimization of English Machine Translation by Deep Neural Network under Artificial Intelligence | 2022 | Computational Intelligence and Neuroscience | article | 15 | yes | Xiaohua Guo | Computer science, Machine translation, Artificial intelligence, Artificial neural network, Example-based machine translation, Word error rate, +14 more | https://doi.org/10.1155/2022/2003411 | To improve the function of machine translation to adapt to global language translation, the work takes deep neural network (DNN) as the basic theory, carries out transfer learning and neural network translation modeling, and optimizes the word alignment function in machine translation performance. First, the work implements a deep learning translation network model for English translation. On this basis, the neural machine translation model is designed under transfer learning. The random shielding method is introduced to implement the language training model, and the machine translation is slightly adjusted as the goal of transfer learning, thereby improving the semantic understanding ability in translation performance. Meanwhile, the work design introduces the method of word alignment optimization and optimizes the performance of word alignment in the transformer system by using word c… |
| https://openalex.org/W2971296908 | EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks | 2019 |  | article | 1845 | yes | Jason Wei, Kai Zou | Boosting (machine learning), Computer science, Artificial intelligence, Machine learning | https://doi.org/10.18653/v1/d19-1670 | Jason Wei, Kai Zou. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019. |
| https://openalex.org/W3160987270 | Multitask Learning and Joint Optimization for Transformer-RNN-Transducer Speech Recognition | 2021 |  | article | 14 | no | Jae-Jin Jeon, Eesung Kim | Computer science, Transformer, Recurrent neural network, Decoding methods, Language model, Speech recognition, +12 more | https://doi.org/10.1109/icassp39728.2021.9414911 | Recently, several types of end-to-end speech recognition methods named transformer-transducer were introduced. According to those kinds of methods, transcription networks are generally modeled by transformer-based neural networks, while prediction networks could be modeled by either transformers or recurrent neural networks (RNN). In this paper, we propose novel multitask learning, joint optimization, and joint decoding methods for transformer-RNN-transducer systems. Our proposed methods have the main advantage in that the model can maintain information on the large text corpus eliminating the necessity of an external language model (LM). We prove their effectiveness by performing experiments utilizing the well-known ESPNET toolkit for the widely used Librispeech datasets. We also show that the proposed methods can reduce word error rate (WER) by 16.6 % and 13.3 % for test-clean and tes… |
| https://openalex.org/W4377711218 | Can Artificial Intelligence Pass the American Board of Orthopaedic Surgery Examination? Orthopaedic Residents Versus ChatGPT | 2023 | Clinical Orthopaedics and Related Research | article | 139 | yes | Zachary C. Lum | Medicine, Artificial intelligence, Orthopedic surgery, Percentile, Machine learning, Surgery, +3 more | https://escholarship.org/uc/item/2w3583s5 | Abstract Background Advances in neural networks, deep learning, and artificial intelligence (AI) have progressed recently. Previous deep learning AI has been structured around domain-specific areas that are trained on dataset-specific areas of interest that yield high accuracy and precision. A new AI model using large language models (LLM) and nonspecific domain areas, ChatGPT (OpenAI), has gained attention. Although AI has demonstrated proficiency in managing vast amounts of data, implementation of that knowledge remains a challenge. Questions/purposes (1) What percentage of Orthopaedic In-Training Examination questions can a generative, pretrained transformer chatbot (ChatGPT) answer correctly? (2) How does that percentage compare with results achieved by orthopaedic residents of different levels, and if scoring lower than the 10th percentile relative to 5th-year residents is likely t… |
| https://openalex.org/W4394998126 | De novo drug design as GPT language modeling: large chemistry models with supervised and reinforcement learning | 2024 | Journal of Computer-Aided Molecular Design | article | 22 | yes | Gavin Ye | Computer science, Reinforcement learning, Machine learning, Artificial intelligence, Drug discovery, Language model, +2 more | https://doi.org/10.1007/s10822-024-00559-z |  |
| https://openalex.org/W2967269971 | QuGAN: Quasi Generative Adversarial Network for Tibetan Question Answering Corpus Generation | 2019 | IEEE Access | article | 14 | yes | Yuan Sun, Chaofan Chen, Tianci Xia, Xiaobing Zhao | Adversarial system, Computer science, Question answering, Generative grammar, Generative adversarial network, Artificial intelligence, +2 more | https://doi.org/10.1109/access.2019.2934581 | In recent years, the large-scale open Chinese and English question answering (QA) corpora have provided important support for the application of deep learning in the Chinese and English QA systems. However, for low-resource languages, such as Tibetan, it is difficult to construct satisfactory QA systems, owing to the lack of large-scale Tibetan QA corpora. To solve this problem, this paper proposes a QA corpus generation model, called QuGAN. This model combines Quasi-Recurrent Neural Networks and Reinforcement Learning. The Quasi-Recurrent Neural Networks model is used as a generator for Generative Adversarial Network, which speeds up the generation of text. At the same time, the reward strategy and Monte Carlo search strategy are optimized to effectively update the generator network. Finally, we use the Bidirectional Encoder Representations from Transformers model to correct the genera… |
| https://openalex.org/W4399594884 | When geoscience meets generative <scp>AI</scp> and large language models: Foundations, trends, and future challenges | 2024 | Expert Systems | article | 34 | yes | Abdenour Hadid, Tanujit Chakraborty, D. Busby | Computer science, Generative grammar, Data science, Field (mathematics), Generative model, Artificial intelligence, +4 more | https://doi.org/10.1111/exsy.13654 | Abstract Generative Artificial Intelligence (GAI) represents an emerging field that promises the creation of synthetic data and outputs in different modalities. GAI has recently shown impressive results across a large spectrum of applications ranging from biology, medicine, education, legislation, computer science, and finance. As one strives for enhanced safety, efficiency, and sustainability, generative AI indeed emerges as a key differentiator and promises a paradigm shift in the field. This article explores the potential applications of generative AI and large language models in geoscience. The recent developments in the field of machine learning and deep learning have enabled the generative model's utility for tackling diverse prediction problems, simulation, and multi‐criteria decision‐making challenges related to geoscience and Earth system dynamics. This survey discusses several… |
| https://openalex.org/W4313467238 | An Energy-Efficient Transformer Processor Exploiting Dynamic Weak Relevances in Global Attention | 2022 | IEEE Journal of Solid-State Circuits | article | 43 | no | Yang Wang, Yubin Qin, Dazheng Deng, Jingchuan Wei, Yang Zhou, Yuanqi Fan, Tianbao Chen, Hao Sun, +3 more | Computer science, Bottleneck, Computation, Transformer, Efficient energy use, Artificial neural network, +14 more | https://doi.org/10.1109/jssc.2022.3213521 | Transformer-based models achieve tremendous success in many artificial intelligence (AI) tasks, outperforming conventional convolution neural networks (CNNs) from natural language processing (NLP) to computer vision (CV). Their success relies on the self-attention mechanism that provides a global rather than local receptive field as CNNs. Despite its superiority, the global–level self-attention consumes <inline-formula xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"> <tex-math notation="LaTeX">$\sim 100\times $ </tex-math></inline-formula> more operations than CNNs and cannot be effectively handled by the existing CNN processor due to the distinct operations. It inspires an urgent requirement to design a dedicated Transformer processor. However, global self-attention involves massive naturally existent weakly related tokens (WR-Tokens) due to th… |
| https://openalex.org/W4288056589 | Cross lingual transfer learning for sentiment analysis of Italian TripAdvisor reviews | 2022 | Expert Systems with Applications | article | 29 | no | Rosario Catelli, Luca Bevilacqua, N. Mariniello, Vladimiro Scotto di Carlo, Massimo Magaldi, Hamido Fujita, Giuseppe De Pietro, Massimo Esposito | Sentiment analysis, Computer science, Transfer of learning, Natural language processing, Artificial intelligence | https://doi.org/10.1016/j.eswa.2022.118246 |  |
| https://openalex.org/W3036422752 | Neural machine translation of low-resource languages using SMT phrase pair injection | 2020 | Natural Language Engineering | article | 30 | no | Sukanta Sen, Mohammed Hasanuzzaman, Asif Ekbal, Pushpak Bhattacharyya, Andy Way | Computer science, Machine translation, Transformer, Artificial intelligence, Natural language processing, Phrase, +14 more | https://doi.org/10.1017/s1351324920000303 | Abstract Neural machine translation (NMT) has recently shown promising results on publicly available benchmark datasets and is being rapidly adopted in various production systems. However, it requires high-quality large-scale parallel corpus, and it is not always possible to have sufficiently large corpus as it requires time, money, and professionals. Hence, many existing large-scale parallel corpus are limited to the specific languages and domains. In this paper, we propose an effective approach to improve an NMT system in low-resource scenario without using any additional data. Our approach aims at augmenting the original training data by means of parallel phrases extracted from the original training data itself using a statistical machine translation (SMT) system. Our proposed approach is based on the gated recurrent unit (GRU) and transformer networks. We choose the Hindi–English, H… |
| https://openalex.org/W2994914025 | Playing the lottery with rewards and multiple languages: lottery tickets in RL and NLP | 2020 | arXiv (Cornell University) | article | 27 | yes | Haonan Yu, Sergey Edunov, Yuandong Tian, Ari S. Morcos | Artificial intelligence, Computer science, Lottery, Initialization, Reinforcement learning, Machine learning, +11 more | https://arxiv.org/pdf/1906.02768.pdf | The lottery ticket hypothesis proposes that over-parameterization of deep neural networks (DNNs) aids training by increasing the probability of a “lucky” sub-network initialization being present rather than by helping the optimization process (Frankle& Carbin, 2019). Intriguingly, this phenomenon suggests that initialization strategies for DNNs can be improved substantially, but the lottery ticket hypothesis has only previously been tested in the context of supervised learning for natural image tasks. Here, we evaluate whether “winning ticket” initializations exist in two different domains: natural language processing (NLP) and reinforcement learning (RL).For NLP, we examined both recurrent LSTM models and large-scale Transformer models (Vaswani et al., 2017). For RL, we analyzed a number of discrete-action space tasks, including both classic control and pixel control. Consistent with w… |
| https://openalex.org/W2988916019 | Deep Learning for Generic Object Detection: A Survey | 2019 | International Journal of Computer Vision | article | 2672 | yes | Li Liu, Wanli Ouyang, Xiaogang Wang, Paul Fieguth, Jie Chen, Xinwang Liu, Matti Pietikäinen | Computer science, Object detection, Artificial intelligence, Deep learning, Field (mathematics), Representation (politics), +17 more | https://doi.org/10.1007/s11263-019-01247-4 | Abstract Object detection, one of the most fundamental and challenging problems in computer vision, seeks to locate object instances from a large number of predefined categories in natural images. Deep learning techniques have emerged as a powerful strategy for learning feature representations directly from data and have led to remarkable breakthroughs in the field of generic object detection. Given this period of rapid evolution, the goal of this paper is to provide a comprehensive survey of the recent achievements in this field brought about by deep learning techniques. More than 300 research contributions are included in this survey, covering many aspects of generic object detection: detection frameworks, object feature representation, object proposal generation, context modeling, training strategies, and evaluation metrics. We finish the survey by identifying promising directions fo… |
| https://openalex.org/W4312719444 | Language Transformers for Remote Sensing Visual Question Answering | 2022 | IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium | article | 16 | yes | Christel Chappuis, Vincent Mendez, Eliot Walt, Sylvain Lobry, Bertrand Le Saux, Devis Tuia | Interfacing, Computer science, Question answering, Transformer, Natural language, Architecture, +13 more | https://infoscience.epfl.ch/handle/20.500.14299/194538 | Remote sensing visual question answering (RSVQA) opens new avenues to promote the use of satellites data, by interfacing satellite image analysis with natural language processing. Capitalizing on the remarkable advances in natural language processing and computer vision, RSVQA aims at finding an answer to a question formulated by a human user about a remote sensing image. This is achieved by extracting representations from images and questions, and then fusing them in a joint representation. Focusing on the language part of the architecture, this study compares and evaluates the adequacy to the RSVQA task of two language models, a traditional recurrent neural network (Skip-thoughts) and a recent attentionbased Transformer (BERT). We study whether large transformer models are beneficial to the task and whether fine-tuning is needed for these models to perform at their best. Our findings… |
| https://openalex.org/W3104178968 | Leap-Of-Thought: Teaching Pre-Trained Models to Systematically Reason Over Implicit Knowledge | 2020 | arXiv (Cornell University) | article | 27 | yes | Alon Talmor, Oyvind Tafjord, Peter E. Clark, Yoav Goldberg, Jonathan Berant | Computer science, Chaining, Inference, Artificial intelligence, Backward chaining, Forward chaining, +14 more | https://arxiv.org/pdf/2006.06609.pdf | To what extent can a neural network systematically reason over symbolic facts? Evidence suggests that large pre-trained language models (LMs) acquire some reasoning capacity, but this ability is difficult to control. Recently, it has been shown that Transformer-based models succeed in consistent reasoning over explicit symbolic facts, under a "closed-world" assumption. However, in an open-domain setup, it is desirable to tap into the vast reservoir of implicit knowledge already encoded in the parameters of pre-trained LMs. In this work, we provide a first demonstration that LMs can be trained to reliably perform systematic reasoning combining both implicit, pre-trained knowledge and explicit natural language statements. To do this, we describe a procedure for automatically generating datasets that teach a model new reasoning skills, and demonstrate that models learn to effectively perfo… |
| https://openalex.org/W4388183180 | Battle of the Backbones: A Large-Scale Comparison of Pretrained Models across Computer Vision Tasks | 2023 | arXiv (Cornell University) | preprint | 26 | yes | Micah Goldblum, Hossein Souri, Renkun Ni, Manli Shu, Viraj Prabhu, Gowthami Somepalli, Prithvijit Chattopadhyay, Mark Ibrahim, +5 more | Computer science, Artificial intelligence, Convolutional neural network, Machine learning, Benchmarking, Deep learning, +8 more | http://arxiv.org/abs/2310.19909 | Neural network based computer vision systems are typically built on a backbone, a pretrained or randomly initialized feature extractor. Several years ago, the default option was an ImageNet-trained convolutional neural network. However, the recent past has seen the emergence of countless backbones pretrained using various algorithms and datasets. While this abundance of choice has led to performance increases for a range of systems, it is difficult for practitioners to make informed decisions about which backbone to choose. Battle of the Backbones (BoB) makes this choice easier by benchmarking a diverse suite of pretrained models, including vision-language models, those trained via self-supervised learning, and the Stable Diffusion backbone, across a diverse set of computer vision tasks ranging from classification to object detection to OOD generalization and more. Furthermore, BoB shed… |
| https://openalex.org/W4372260119 | RCDPT: Radar-Camera Fusion Dense Prediction Transformer | 2023 | Lirias (KU Leuven) | article | 16 | yes | Chen-Chou Lo, Patrick Vandewalle | Computer science, Artificial intelligence, Transformer, Convolutional neural network, Fuse (electrical), Radar, +15 more | https://lirias.kuleuven.be/handle/20.500.12942/715570 | Recently, transformer networks have outperformed traditional deep neural networks in natural language processing and show a large potential in many computer vision tasks compared to convolutional backbones. In the original transformer, readout tokens are used as designated vectors for aggregating information from other tokens. However, the performance of using readout tokens in a vision transformer is limited. Therefore, we propose a novel fusion strategy to integrate radar data into a dense prediction transformer network by reassembling camera representations with radar representations. Instead of using readout tokens, radar representations contribute additional depth information to a monocular depth estimation model and improve performance. We further investigate different fusion approaches that are commonly used for integrating additional modality in a dense prediction transformer ne… |
| https://openalex.org/W4226365456 | Natural Language Processing Applied to Forensics Information Extraction With Transformers and Graph Visualization | 2022 | IEEE Transactions on Computational Social Systems | article | 23 | no | Fillipe Barros Rodrigues, William Ferreira Giozza, Robson de Oliveira Albuquerque, Luis Javier García Villalba | Computer science, Information extraction, Natural language processing, Named-entity recognition, Relationship extraction, Categorization, +17 more | https://doi.org/10.1109/tcss.2022.3159677 | Digital forensics analysis is a slow process mainly due to the large amount and variety of data. Some forensic tools help categorize files by type and allow automatization of tasks, like named entity recognition (NER). NER is a key component in many natural language processing (NLP) applications, such as relation extraction (RE) and information retrieval. The introduction of neural networks and transformer architectures in the last few years made it possible to develop more accurate models in different languages. This work proposes a reproducible setup to build a forensic pipeline for information extraction using NLP of texts. Our results show that it is possible to develop both NER and RE models in any language and tune its hyper-parameters to achieve state-of-art performance and build comprehensive knowledge graphs, decreasing the amount of time required for human supervision and revi… |
| https://openalex.org/W3048823912 | Compression of Deep Learning Models for Text: A Survey | 2022 | ACM Transactions on Knowledge Discovery from Data | preprint | 12 | yes | Manish Gupta, Puneet Agrawal | Computer science, Deep learning, Recurrent neural network, Transformer, Artificial intelligence, Encoder, +9 more | http://arxiv.org/abs/2008.05221 | In recent years, the fields of natural language processing (NLP) and information retrieval (IR) have made tremendous progress thanks to deep learning models like Recurrent Neural Networks (RNNs), Gated Recurrent Units (GRUs) and Long Short-Term Memory (LSTMs) networks, and Transformer [ 121 ] based models like Bidirectional Encoder Representations from Transformers (BERT) [ 24 ], Generative Pre-training Transformer (GPT-2) [ 95 ], Multi-task Deep Neural Network (MT-DNN) [ 74 ], Extra-Long Network (XLNet) [ 135 ], Text-to-text transfer transformer (T5) [ 96 ], T-NLG [ 99 ], and GShard [ 64 ]. But these models are humongous in size. On the other hand, real-world applications demand small model size, low response times, and low computational power wattage. In this survey, we discuss six different types of methods (Pruning, Quantization, Knowledge Distillation (KD), Parameter Sharing, Tenso… |
| https://openalex.org/W4406213428 | Fake News Detection and Classification: A Comparative Study of Convolutional Neural Networks, Large Language Models, and Natural Language Processing Models | 2025 | Future Internet | article | 20 | yes | Konstantinos I. Roumeliotis, Nikolaos D. Tselikas, Dimitrios Κ. Nasiopoulos | Computer science, Convolutional neural network, Artificial intelligence, Natural language processing, Natural language, Language model, +1 more | https://doi.org/10.3390/fi17010028 | In an era where fake news detection has become a pressing issue due to its profound impacts on public opinion, democracy, and social trust, accurately identifying and classifying false information is a critical challenge. In this study, the effectiveness is investigated of advanced machine learning models—convolutional neural networks (CNNs), bidirectional encoder representations from transformers (BERT), and generative pre-trained transformers (GPTs)—for robust fake news classification. Each model brings unique strengths to the task, from CNNs’ pattern recognition capabilities to BERT and GPTs’ contextual understanding in the embedding space. Our results demonstrate that the fine-tuned GPT-4 Omni models achieve 98.6% accuracy, significantly outperforming traditional models like CNNs, which achieved only 58.6%. Notably, the smaller GPT-4o mini model performed comparably to its larger co… |
| https://openalex.org/W4386017236 | Fine-Tuning Vision Encoder–Decoder Transformers for Handwriting Text Recognition on Historical Documents | 2023 | Lecture notes in computer science | book-chapter | 20 | no | Daniel Parres, Roberto Paredes | Computer science, Language model, Transformer, Initialization, Encoder, Artificial intelligence, +13 more | https://doi.org/10.1007/978-3-031-41685-9_16 |  |
| https://openalex.org/W4287072584 | Sequence-to-Sequence Piano Transcription with Transformers | 2021 | arXiv (Cornell University) | preprint | 20 | yes | Curtis Hawthorne, Ian Simon, Rigel Swavely, Ethan Manilow, Jesse Engel | Computer science, Transcription (linguistics), Transformer, Encoder, Language model, Decoding methods, +19 more | http://arxiv.org/abs/2107.09142 | Automatic Music Transcription has seen significant progress in recent years by training custom deep neural networks on large datasets. However, these models have required extensive domain-specific design of network architectures, input/output representations, and complex decoding schemes. In this work, we show that equivalent performance can be achieved using a generic encoder-decoder Transformer with standard decoding methods. We demonstrate that the model can learn to translate spectrogram inputs directly to MIDI-like output events for several transcription tasks. This sequence-to-sequence approach simplifies transcription by jointly modeling audio features and language-like output dependencies, thus removing the need for task-specific architectures. These results point toward possibilities for creating new Music Information Retrieval models by focusing on dataset creation and labelin… |
| https://openalex.org/W3118630514 | Multi-label Classification of Commit Messages using Transfer Learning | 2020 |  | article | 23 | no | Muhammad Usman Sarwar, Sarim Zafar, Mohamed Wiem Mkaouer, Gursimran Walia, Muhammad Zubair Malik | Computer science, Commit, Artificial intelligence, Natural language processing, Task (project management), Language model, +10 more | https://doi.org/10.1109/issrew51248.2020.00034 | Commit messages are used in the industry by developers to annotate changes made to the code. Accurate classification of these messages can help monitor the software evolution process and enable better tracking for various industrial stakeholders. In this paper, we present a state of the art method for commit message classification into categories as per Swanson's maintenance activities i.e. "Corrective", "Perfective", and "Adaptive". This is a challenging task because not all commit messages are well written and informative. Existing approaches rely on keyword-based techniques to solve this problem. However, these approaches are oblivious to the full language model and do not recognize the contextual relationship between words. State of the art methodology in Natural Language Processing (NLP), is to train a context-aware neural network (Transformer) on a very large data set that encompa… |
| https://openalex.org/W3158542464 | Bidirectional Language Modeling: A Systematic Literature Review | 2021 | Scientific Programming | article | 16 | yes | Muhammad Shah Jahan, Habib Ullah Khan, Shahzad Akbar, Muhammad Umar Farooq, Sarah Gul, Anam Amjad | Computer science, Transformer, Sentence, Language model, Encoder, Transfer of learning, +12 more | https://doi.org/10.1155/2021/6641832 | In transfer learning, two major activities, i.e., pretraining and fine-tuning, are carried out to perform downstream tasks. The advent of transformer architecture and bidirectional language models, e.g., bidirectional encoder representation from transformer (BERT), enables the functionality of transfer learning. Besides, BERT bridges the limitations of unidirectional language models by removing the dependency on the recurrent neural network (RNN). BERT also supports the attention mechanism to read input from any side and understand sentence context better. It is analyzed that the performance of downstream tasks in transfer learning depends upon the various factors such as dataset size, step size, and the number of selected parameters. In state-of-the-art, various research studies produced efficient results by contributing to the pretraining phase. However, a comprehensive investigation… |
| https://openalex.org/W4205500136 | Investigating the Effect of Preprocessing Arabic Text on Offensive Language and Hate Speech Detection | 2022 | ACM Transactions on Asian and Low-Resource Language Information Processing | article | 36 | no | Fatemah Husain, Özlem Uzuner | Preprocessor, Computer science, Artificial intelligence, Normalization (sociology), Natural language processing, Offensive, +11 more | https://doi.org/10.1145/3501398 | Preprocessing of input text can play a key role in text classification by reducing dimensionality and removing unnecessary content. This study aims to investigate the impact of preprocessing on Arabic offensive language classification. We explore six preprocessing techniques: conversion of emojis to Arabic textual labels, normalization of different forms of Arabic letters, normalization of selected nouns from dialectal Arabic to Modern Standard Arabic, conversion of selected hyponyms to hypernyms, hashtag segmentation, and basic cleaning such as removing numbers, kashidas, diacritics, and HTML tags. We also experiment with raw text and a combination of all six preprocessing techniques. We apply different types of classifiers in our experiments including traditional machine learning, ensemble machine learning, Artificial Neural Networks, and Bidirectional Encoder Representations from Tra… |
| https://openalex.org/W4388140384 | Deep temporal networks for EEG-based motor imagery recognition | 2023 | Scientific Reports | article | 20 | yes | Neha Sharma, Avinash Upadhyay, Manoj Kumar Sharma, Amit Singhal | Computer science, Artificial intelligence, Transformer, Motor imagery, Deep learning, Electroencephalography, +12 more | https://doi.org/10.1038/s41598-023-41653-w |  |
| https://openalex.org/W4399769862 | A Protein Structure Prediction Approach Leveraging Transformer and CNN Integration | 2024 |  | article | 16 | no | Yanlin Zhou, Kai Tan, Xinyu Shen, Zheng He, Haotian Zheng | Computer science, Transformer, Artificial intelligence, Engineering, Electrical engineering, Voltage | https://doi.org/10.1109/icaace61206.2024.10548253 | Proteins are essential for life, and their structure determines their function. The protein secondary structure is formed by the folding of the protein primary structure, and the protein tertiary structure is formed by the bending and folding of the secondary structure. Therefore, the study of protein secondary structure is very helpful to the overall understanding of protein structure. Although the accuracy of protein secondary structure prediction has continuously improved with the development of machine learning and deep learning, progress in the field of protein structure prediction, unfortunately, remains insufficient to meet the large demand for protein information. Therefore, based on the advantages of deep learning-based methods in feature extraction and learning ability, this paper adopts a two-dimensional fusion deep neural network model, DstruCCN, which uses Convolutional Neu… |
| https://openalex.org/W4401410647 | Generative Artificial Intelligence and Large Language Models in Primary Care Medical Education | 2024 | Family Medicine | article | 26 | yes | Daniel J. Parente | Generative grammar, Primary care, Artificial intelligence, Computer science, Psychology, Medical education, +2 more | https://doi.org/10.22454/fammed.2024.775525 | Generative artificial intelligence and large language models are the continuation of a technological revolution in information processing that began with the invention of the transistor in 1947. These technologies, driven by transformer architectures for artificial neural networks, are poised to broadly influence society. It is already apparent that these technologies will be adapted to drive innovation in education. Medical education is a high-risk activity: Information that is incorrectly taught to a student may go unrecognized for years until a relevant clinical situation appears in which that error can lead to patient harm. In this article, I discuss the principal limitations to the use of generative artificial intelligence in medical education—hallucination, bias, cost, and security—and suggest some approaches to confronting these problems. Additionally, I identify the potential ap… |
| https://openalex.org/W2953356739 | ERNIE: Enhanced Language Representation with Informative Entities | 2019 |  | preprint | 1373 | yes | Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, Qun Liu | Computer science, Natural language processing, Representation (politics), Artificial intelligence, Language model, Code (set theory), +10 more | https://doi.org/10.18653/v1/p19-1139 | Neural language representation models such as BERT pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various NLP tasks. However, the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding. We argue that informative entities in KGs can enhance language representation with external knowledge. In this paper, we utilize both large-scale textual corpora and KGs to train an enhanced language representation model (ERNIE), which can take full advantage of lexical, syntactic, and knowledge information simultaneously. The experimental results have demonstrated that ERNIE achieves significant improvements on various knowledge-driven tasks, and meanwhile is comparable with the st… |
| https://openalex.org/W3175042876 | On Orthogonality Constraints for Transformers | 2021 |  | article | 9 | yes | Aston Zhang, Alvin Chan, Yi Tay, Jie Fu, Shuohang Wang, Shuai Zhang, Huajie Shao, Shuochao Yao, +1 more | Zhàng, Orthogonality, Computer science, Joint (building), Library science, Artificial intelligence, +7 more | https://doi.org/10.18653/v1/2021.acl-short.48 | Orthogonality constraints encourage matrices to be orthogonal for numerical stability. These plug-and-play constraints, which can be conveniently incorporated into model training, have been studied for popular architectures in natural language processing, such as convolutional neural networks and recurrent neural networks. However, a dedicated study on such constraints for transformers has been absent. To fill this gap, this paper studies orthogonality constraints for transformers, showing the effectiveness with empirical evidence from ten machine translation tasks and two dialogue generation tasks. For example, on the large-scale WMT'16 En -&gt; De benchmark, simply plugging-and-playing orthogonality constraints on the original transformer model (Vaswani et al., 2017) increases the BLEU from 28.4 to 29.6, coming close to the 29.7 BLEU achieved by the very competitive dynamic convolutio… |
| https://openalex.org/W4220714594 | A Vision Transformer network <scp>SeedViT</scp> for classification of maize seeds | 2022 | Journal of Food Process Engineering | article | 20 | no | Jiqing Chen, Tian Luo, Jiahua Wu, Zhikui Wang, Hongdu Zhang | Convolutional neural network, Artificial intelligence, Computer science, Pattern recognition (psychology), Support vector machine, Classifier (UML), +9 more | https://doi.org/10.1111/jfpe.13998 | Abstract Maize is a crop that is widely cultivated all over the world. Thus, the classification of maize seeds quality is important, while the traditional methods based on the texture, shape, and color which require repeated work is not efficient. Recently, deep learning reached the goal in the field of image processing, and a deep convolutional neural network (DCNN) is often used to do the image classification task. Here, we explored another neural network called Vision Transformer (ViT), which originally was applied to the natural language processing. Based on the self‐attention mechanism, ViT discards the convolutional structure. But when trained from scratch on medium‐sized datasets, ViT performed poorly compared to CNN. Due to the lack of local structure within the input image, tokenization cannot be used to generate a valid training set in the original ViT model. As a result, we p… |
| https://openalex.org/W3169483174 | mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer | 2021 |  | article | 1477 | yes | Linting Xue, Noah Constant, Adam P. Roberts, Mihir Kale, Rami Al‐Rfou, Aditya Siddhant, Aditya Barua, Colin Raffel | Transformer, Computer science, Massively parallel, Computational linguistics, Association (psychology), Artificial intelligence, +7 more | https://doi.org/10.18653/v1/2021.naacl-main.41 | Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, Colin Raffel. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021. |
| https://openalex.org/W3036737707 | New Vietnamese Corpus for Machine Reading Comprehension of Health News Articles | 2022 | ACM Transactions on Asian and Low-Resource Language Information Processing | article | 24 | no | Kiet Van Nguyen, Tin Van Huynh, Duc-Vu Nguyen, Anh Gia-Tuan Nguyen, Ngan Luu-Thuy Nguyen | Vietnamese, Computer science, Artificial intelligence, Natural language processing, Comprehension, Reading comprehension, +13 more | https://doi.org/10.1145/3527631 | Machine reading comprehension is a natural language understanding task where the computing system is required to read a text and then find the answer to a specific question posed by a human. Large-scale and high-quality corpora are necessary for evaluating machine reading comprehension models. Furthermore, machine reading comprehension (MRC) for the health sector has potential for practical applications; nevertheless, MRC research in this domain is currently scarce. This article presents UIT-ViNewsQA, a new corpus for the Vietnamese language to evaluate MRC models for the healthcare textual domain. The corpus consists of 22,057 human-generated question-answer pairs. Crowd-workers create the questions and answers on a collection of 4,416 online Vietnamese healthcare news articles, where the answers are textual spans extracted from the corresponding articles. We introduce a process for cr… |
| https://openalex.org/W4221155853 | Neural Grapheme-To-Phoneme Conversion with Pre-Trained Grapheme Models | 2022 | ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signa… | article | 12 | no | Lu Dong, Zhiqiang Guo, Chao-Hong Tan, Jun Hu, Yuan Jiang, Zhen-Hua Ling | Grapheme, Computer science, Transformer, Pronunciation, Artificial intelligence, Natural language processing, +9 more | https://doi.org/10.1109/icassp43922.2022.9746447 | Neural network models have achieved state-of-the-art performance on grapheme-to-phoneme (G2P) conversion. However, their performance relies on large-scale pronunciation dictionaries, which may not be available for a lot of languages. Inspired by the success of the pre-trained language model BERT, this paper proposes a pre-trained grapheme model called grapheme BERT (GBERT), which is built by self-supervised training on a large, language-specific word list with only grapheme information. Furthermore, two approaches are developed to incorporate GBERT into the state-of-the-art Transformer-based G2P model, i.e., fine-tuning GBERT or fusing GBERT into the Transformer model by attention. Experimental results on the Dutch, Serbo-Croatian, Bulgarian and Korean datasets of the SIGMORPHON 2021 G2P task confirm the effectiveness of our GBERT-based G2P models under both medium-resource and low-reso… |
| https://openalex.org/W4365420820 | An Experimental Analysis of Deep Neural Network Based Classifiers for Sentiment Analysis Task | 2023 | IEEE Access | article | 11 | yes | Mrigank Shukla, Akhil Kumar | Computer science, Artificial intelligence, Word2vec, Sentiment analysis, Convolutional neural network, Word embedding, +15 more | https://doi.org/10.1109/access.2023.3266640 | The application of natural language processing (NLP) in sentiment analysis task by using textual data has wide scale application across various domains in plethora of industries. We have methodically studied pre-existing models and proposed new models for examining sentiment analysis task. The models proposed were analysed with three widely popular word embeddings separately and in combined approach using all embeddings as unique channels. We combined deep neural network models such as Bidirectional Long Short-Term Memory (BiLSTM) and Convolutional Neural Network (CNN) so that integrated models complement each other with their unique architectures. The word embeddings used had profound impact in accuracy of models owing to performative changes. The best word embedding was Word2Vec giving highest accuracy in almost all implemented models, followed by GloVe. FastText embedding performed c… |
| https://openalex.org/W4391680077 | TransEFVP: A Two-Stage Approach for the Prediction of Human Pathogenic Variants Based on Protein Sequence Embedding Fusion | 2024 | Journal of Chemical Information and Modeling | article | 21 | no | Zihao Yan, Fang Ge, Yan Liu, Yumeng Zhang, Fuyi Li, Jiangning Song, Dong‐Jun Yu | Matthews correlation coefficient, Autoencoder, Encoder, Computer science, Transformer, Artificial intelligence, +14 more | https://doi.org/10.1021/acs.jcim.3c02019 | Studying the effect of single amino acid variations (SAVs) on protein structure and function is integral to advancing our understanding of molecular processes, evolutionary biology, and disease mechanisms. Screening for deleterious variants is one of the crucial issues in precision medicine. Here, we propose a novel computational approach, TransEFVP, based on large-scale protein language model embeddings and a transformer-based neural network to predict disease-associated SAVs. The model adopts a two-stage architecture: the first stage is designed to fuse different feature embeddings through a transformer encoder. In the second stage, a support vector machine model is employed to quantify the pathogenicity of SAVs after dimensionality reduction. The prediction performance of TransEFVP on blind test data achieves a Matthews correlation coefficient of 0.751, an F<sub>1</sub>-score of 0.84… |
| https://openalex.org/W4392163668 | Neural machine translation of clinical text: an empirical investigation into multilingual pre-trained language models and transfer-learning | 2024 | Frontiers in Digital Health | article | 18 | yes | Lifeng Han, Serge Gladkoff, Gleb Erofeev, Irina Sorokina, Betty Galiano, Goran Nenadić | Computer science, Machine translation, Artificial intelligence, Natural language processing, Terminology, Transfer of learning, +8 more | https://doi.org/10.3389/fdgth.2024.1211564 | Clinical text and documents contain very rich information and knowledge in healthcare, and their processing using state-of-the-art language technology becomes very important for building intelligent systems for supporting healthcare and social good. This processing includes creating language understanding models and translating resources into other natural languages to share domain-specific cross-lingual knowledge. In this work, we conduct investigations on clinical text machine translation by examining multilingual neural network models using deep learning such as Transformer based structures. Furthermore, to address the language resource imbalance issue, we also carry out experiments using a transfer learning methodology based on massive multilingual pre-trained language models (MMPLMs). The experimental results on three sub-tasks including (1) clinical case (CC), (2) clinical termino… |
| https://openalex.org/W3216974065 | Identifying wildlife observations on twitter | 2021 | Ecological Informatics | article | 31 | no | Thomas C. Edwards, Christopher B. Jones, Padraig Corcoran | Computer science, Wildlife, Artificial intelligence, Machine learning, Classifier (UML), Social media, +6 more | https://doi.org/10.1016/j.ecoinf.2021.101500 |  |
| https://openalex.org/W2973182667 | Password Guessing via Neural Language Modeling | 2019 | Lecture notes in computer science | book-chapter | 12 | no | Hang Li, Mengqi Chen, Shengbo Yan, Chunfu Jia, Zhaohui Li | Computer science, Password, Artificial intelligence, Natural language processing, Programming language, Computer security | https://doi.org/10.1007/978-3-030-30619-9_7 |  |
| https://openalex.org/W4324258884 | Deep Semantic-Visual Alignment for zero-shot remote sensing image scene classification | 2023 | ISPRS Journal of Photogrammetry and Remote Sensing | article | 26 | yes | Wenjia Xu, Jiuniu Wang, Zhiwei Wei, Mugen Peng, Yirong Wu | Computer science, Artificial intelligence, Convolutional neural network, Focus (optics), Context (archaeology), Transfer of learning, +13 more | http://arxiv.org/abs/2402.02094 |  |
| https://openalex.org/W4386280750 | Offensive Language Detection in Spanish Social Media: Testing From Bag-of-Words to Transformers Models | 2023 | IEEE Access | article | 10 | yes | José María Molero, Jorge Pérez‐Martín, Álvaro Rodrigo, Anselmo Peñas | Offensive, Computer science, Transformer, Artificial intelligence, Machine learning, Natural language processing, +9 more | https://doi.org/10.1109/access.2023.3310244 | Social networks allow us to communicate with people around the world. However, some users usually take advantage of anonymity for writing offensive comments to others, which might affect those who receive offensive messages or discourage the use of these networks. However, it is impossible to manually check every message. This has promoted several proposals for automatic detection systems. Current state-of-the-art systems are based on the transformers&#x2019; architecture and most of the work has been focused on the English language. However, these systems do not pay too much attention to the unbalanced nature of data, since there are fewer offensive comments than non-offensive in a real environment. Besides, these previous works have not studied the impact on the final results of pre-processing or the corpora used for pre-training the models. In this work, we propose and evaluate a ser… |
| https://openalex.org/W4390824394 | Generative Retrieval-Augmented Ontologic Graph and Multiagent Strategies for Interpretive Large Language Model-Based Materials Design | 2024 | ACS Engineering Au | article | 50 | yes | Markus J. Buehler | Computer science, Generative grammar, Artificial intelligence, Domain knowledge, Data science, Generative model, +1 more | https://doi.org/10.1021/acsengineeringau.3c00058 | Transformer neural networks show promising capabilities, in particular for uses in materials analysis, design, and manufacturing, including their capacity to work effectively with human language, symbols, code, and numerical data. Here, we explore the use of large language models (LLMs) as a tool that can support engineering analysis of materials, applied to retrieving key information about subject areas, developing research hypotheses, discovery of mechanistic relationships across disparate areas of knowledge, and writing and executing simulation codes for active knowledge generation based on physical ground truths. Moreover, when used as sets of AI agents with specific features, capabilities, and instructions, LLMs can provide powerful problem-solution strategies for applications in analysis and design problems. Our experiments focus on using a fine-tuned model, MechGPT, developed bas… |
| https://openalex.org/W2982399380 | BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension | 2020 |  | preprint | 1222 | yes | Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer | Machine translation, Computer science, Artificial intelligence, Natural language processing, Translation (biology), Sequence (biology), +10 more | https://doi.org/10.18653/v1/2020.acl-main.703 | We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources… |
| https://openalex.org/W4280586606 | Work-in-Progress: Computing Sentence Similarity for Short Texts using Transformer models | 2022 | 2022 IEEE Global Engineering Education Conference (EDUCON) | article | 11 | no | Vidasha Ramnarain-Seetohul, Vandana Bassoo, Yasmine Rosunally | Transformer, Sentence, Computer science, Artificial intelligence, Natural language processing, Data mining, +5 more | https://doi.org/10.1109/educon52537.2022.9766649 | The field of natural language processing is being revolutionized with transformers. The latter is based on a novel type of neural network framework that is already pre-trained. Hence, large datasets to train models are no longer required. This framework is suitable for automated assessment systems (AAS), where a large number of labeled data is needed. The larger the dataset, the higher the accuracy of the AAS. In this work-in-progress paper, a prototype for an AAS has been built where two transformer models, namely the Sentence-Transformers from hugging face and the OpenAI GPT-3 models have been used. The transformer models generate the similarity index between students' answers and reference answers from the Texas dataset. Then the similarity index is used to compute marks for students. The performance of the prototype is evaluated using the quadratic weighted kappa metric. |
| https://openalex.org/W2948947170 | What Does BERT Learn about the Structure of Language? | 2019 |  | preprint | 1175 | yes | Ganesh Jawahar, Benoît Sagot, Djamé Seddah | Computer science, Hierarchy, Natural language processing, Artificial intelligence, Representation (politics), Dependency (UML), +16 more | https://doi.org/10.18653/v1/p19-1356 | BERT is a recent language representation model that has surprisingly performed well in diverse language understanding benchmarks. This result indicates the possibility that BERT networks capture structural information about language. In this work, we provide novel support for this claim by performing a series of experiments to unpack the elements of English language structure learned by BERT. Our findings are fourfold. BERT’s phrasal representation captures the phrase-level information in the lower layers. The intermediate layers of BERT compose a rich hierarchy of linguistic information, starting with surface features at the bottom, syntactic features in the middle followed by semantic features at the top. BERT requires deeper layers while tracking subject-verb agreement to handle long-term dependency problem. Finally, the compositional scheme underlying BERT mimics classical, tree-lik… |
| https://openalex.org/W4306160361 | How to Detect Online Hate towards Migrants and Refugees? Developing and Evaluating a Classifier of Racist and Xenophobic Hate Speech Using Shallow and Deep Learning | 2022 | Sustainability | article | 25 | yes | Carlos Arcila Calderón, Javier J. Amores, Patricia Sánchez‐Holgado, Lazaros Vrysis, Nikolaos Vryzas, Martín Oller Alonso | Xenophobia, Computer science, Racism, Deep learning, Artificial intelligence, Classifier (UML), +5 more | https://doi.org/10.3390/su142013094 | Hate speech spreading online is a matter of growing concern since social media allows for its rapid, uncontrolled, and massive dissemination. For this reason, several researchers are already working on the development of prototypes that allow for the detection of cyberhate automatically and on a large scale. However, most of them are developed to detect hate only in English, and very few focus specifically on racism and xenophobia, the category of discrimination in which the most hate crimes are recorded each year. In addition, ad hoc datasets manually generated by several trained coders are rarely used in the development of these prototypes since almost all researchers use already available datasets. The objective of this research is to overcome the limitations of those previous works by developing and evaluating classification models capable of detecting racist and/or xenophobic hate… |
| https://openalex.org/W4399372338 | Hallucinations in Large Language Models (LLMs) | 2024 |  | article | 26 | no | G. Pradeep Reddy, Y. V. Pavan Kumar, Purna Prakash Kasaraneni | Computer science, Context (archaeology), Cognitive psychology, Data science, Artificial intelligence, Psychology, +2 more | https://doi.org/10.1109/estream61684.2024.10542617 | The recent advancements in neural network architectures, particularly transformers, have played a crucial role in the rapid progress of Large Language Models (LLMs). LLMs are trained on many parameters. By training these parameters on vast amounts of text data, LLMs can learn to generate reactions to a wide variety of prompts. These models have enabled machines to generate new data (human-like), driving significant developments in Natural Language Processing (NLP). They have demonstrated remarkable capabilities in producing new content. Besides their impressive performance, LLMs occasionally generate hallucinatory responses that produce nonsensical or inaccurate information. In simple terms, hallucinations in LLMs happen when the model generates information that may sound believable but is actually wrong. It can make up details or go beyond what it has learned from the training data, re… |
| https://openalex.org/W4377865076 | RWKV: Reinventing RNNs for the Transformer Era | 2023 | arXiv (Cornell University) | preprint | 15 | yes | Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, +26 more | Computer science, Scalability, Transformer, Recurrent neural network, Inference, Computation, +16 more | http://arxiv.org/abs/2305.13048 | Transformers have revolutionized almost all natural language processing (NLP) tasks but suffer from memory and computational complexity that scales quadratically with sequence length. In contrast, recurrent neural networks (RNNs) exhibit linear scaling in memory and computational requirements but struggle to match the same performance as Transformers due to limitations in parallelization and scalability. We propose a novel model architecture, Receptance Weighted Key Value (RWKV), that combines the efficient parallelizable training of transformers with the efficient inference of RNNs. Our approach leverages a linear attention mechanism and allows us to formulate the model as either a Transformer or an RNN, thus parallelizing computations during training and maintains constant computational and memory complexity during inference. We scale our models as large as 14 billion parameters, by f… |
| https://openalex.org/W4321393046 | Tensor Networks Meet Neural Networks: A Survey and Future Perspectives | 2023 | arXiv (Cornell University) | preprint | 13 | yes | Maolin Wang, Pan Yu, Xiangli Yang, Guangxi Li, Zenglin Xu, Mandic, Danilo, Cichocki, Andrzej | Curse of dimensionality, Artificial neural network, Computer science, Artificial intelligence, Tensor (intrinsic definition), Quantum, +6 more | http://arxiv.org/abs/2302.09019 | Tensor networks (TNs) and neural networks (NNs) are two fundamental data modeling approaches. TNs were introduced to solve the curse of dimensionality in large-scale tensors by converting an exponential number of dimensions to polynomial complexity. As a result, they have attracted significant attention in the fields of quantum physics and machine learning. Meanwhile, NNs have displayed exceptional performance in various applications, e.g., computer vision, natural language processing, and robotics research. Interestingly, although these two types of networks originate from different observations, they are inherently linked through the typical multilinearity structure underlying both TNs and NNs, thereby motivating a significant number of developments regarding combinations of TNs and NNs. In this paper, we refer to these combinations as tensorial neural networks~(TNNs) and present an i… |
| https://openalex.org/W4389437528 | Education in the Era of Generative Artificial Intelligence (AI): Understanding the Potential Benefits of ChatGPT in Promoting Teaching and Learning | 2023 | Journal of AI | article | 1362 | yes | David Baidoo-Anu, Leticia Owusu Ansah | Surprise, Formative assessment, Generative grammar, Promotion (chess), Computer science, Praxis, +14 more | https://doi.org/10.61969/jai.1337500 | Since its maiden release into the public domain on November 30, 2022, ChatGPT garnered more than one million subscribers within a week. The generative AI tool ⎼ChatGPT took the world by surprise with it sophisticated capacity to carry out remarkably complex tasks. The extraordinary abilities of ChatGPT to perform complex tasks within the field of education has caused mixed feelings among educators, as this advancement in AI seems to revolutionize existing educational praxis. This is an exploratory study that synthesizes recent extant literature to offer some potential benefits and drawbacks of ChatGPT in promoting teaching and learning. Benefits of ChatGPT include but are not limited to promotion of personalized and interactive learning, generating prompts for formative assessment activities that provide ongoing feedback to inform teaching and learning etc. The paper also highlights som… |
| https://openalex.org/W3139663946 | Low-Resource Language Modelling of South African Languages | 2021 | arXiv (Cornell University) | preprint | 10 | yes | Stuart Mesham, Luc Hayward, Jared Shapiro, Jan Buys | Computer science, Recurrent neural network, Natural language processing, Transformer, Artificial intelligence, Vocabulary, +8 more | http://arxiv.org/abs/2104.00772 | Language models are the foundation of current neural network-based models for natural language understanding and generation. However, research on the intrinsic performance of language models on African languages has been extremely limited, which is made more challenging by the lack of large or standardised training and evaluation sets that exist for English and other high-resource languages. In this paper, we evaluate the performance of open-vocabulary language models on low-resource South African languages, using byte-pair encoding to handle the rich morphology of these languages. We evaluate different variants of n-gram models, feedforward neural networks, recurrent neural networks (RNNs), and Transformers on small-scale datasets. Overall, well-regularized RNNs give the best performance across two isiZulu and one Sepedi datasets. Multilingual training further improves performance on t… |
| https://openalex.org/W4383108222 | Sim-to-Real Transfer for Quadrupedal Locomotion via Terrain Transformer | 2023 |  | article | 17 | no | Hang Lai, Weinan Zhang, Xialin He, Yu Chen, Tian Zheng, Yong Yu, Jun Wang | Terrain, Computer science, Transformer, Reinforcement learning, Scalability, Artificial intelligence, +7 more | https://doi.org/10.1109/icra48891.2023.10160497 | Deep reinforcement learning has recently emerged as an appealing alternative for legged locomotion over multiple terrains by training a policy in physical simulation and then transferring it to the real world (i.e., sim-to-real transfer). Despite considerable progress, the capacity and scalability of traditional neural networks are still limited, which may hinder their applications in more complex environments. In contrast, the Transformer architecture has shown its superiority in a wide range of large-scale sequence modeling tasks, including natural language processing and decision-making problems. In this paper, we propose Terrain Transformer (TERT), a high-capacity Transformer model for quadrupedal locomotion control on various terrains. Furthermore, to better leverage Transformer in sim-to-real scenarios, we present a novel two-stage training framework consisting of an offline pretr… |
| https://openalex.org/W4390674929 | Named entity recognition in aerospace based on multi-feature fusion transformer | 2024 | Scientific Reports | article | 15 | yes | Jing Chu, Yumeng Liu, Qi Yue, Zixuan Zheng, Xiaokai Han | Aerospace, Computer science, Named-entity recognition, Artificial intelligence, Transformer, Domain (mathematical analysis), +16 more | https://doi.org/10.1038/s41598-023-50705-0 | Abstract In recent years, along with the rapid development in the domain of artificial intelligence and aerospace, aerospace combined with artificial intelligence is the future trend. As an important basic tool for Natural Language Processing, Named Entity Recognition technology can help obtain key relevant knowledge from a large number of aerospace data. In this paper, we produced an aerospace domain entity recognition dataset containing 30 k sentences in Chinese and developed a named entity recognition model that is Multi-Feature Fusion Transformer (MFT), which combines features such as words and radicals to enhance the semantic information of the sentences. In our model, the double Feed-forward Neural Network is exploited as well to ensure MFT better performance. We use our aerospace dataset to train MFT. The experimental results show that MFT has great entity recognition performance… |
| https://openalex.org/W4210827551 | A survey on sentiment analysis methods, applications, and challenges | 2022 | Artificial Intelligence Review | article | 1285 | yes | Mayur Wankhade, Annavarapu Chandra Sekhara Rao, Chaitanya Kulkarni | Computer science, Sentiment analysis, Data science, Artificial intelligence | https://doi.org/10.1007/s10462-022-10144-1 |  |
| https://openalex.org/W4389273487 | Deep Learning Models for Analyzing Social Construction of Knowledge Online | 2023 | Online Learning | article | 10 | yes | Charlotte Nirmalani Gunawardena, Yan Chen, Nick V. Flor, Damien M. Sánchez | Computer science, Qualitative analysis, Artificial intelligence, Artificial neural network, Key (lock), Social constructivism, +13 more | https://doi.org/10.24059/olj.v27i4.4055 | Gunawardena et al.’s (1997) Interaction Analysis Model (IAM) is one of the most frequently employed frameworks to guide the qualitative analysis of social construction of knowledge online. However, qualitative analysis is time consuming, and precludes immediate feedback to revise online courses while being delivered. To expedite analysis with a large dataset, this study explores how two neural network architectures—a feed-forward network (Doc2Vec) and a large language model transformer (BERT)—could automatically predict phases of knowledge construction using IAM. The methods interrogated the extent to which the artificial neural networks’ predictions of IAM Phases approximated a human coder’s qualitative analysis. Key results indicate an accuracy of 21.55% for Doc2Vec phases I-V, 43% for fine-tuning a pre-trained large language model (LLM), and 52.79% for prompt-engineering an LLM. Futu… |
| https://openalex.org/W4384263459 | ReLoRA: High-Rank Training Through Low-Rank Updates | 2023 | arXiv (Cornell University) | preprint | 11 | yes | Vladislav Lialin, Namrata Shivagunde, Sherin Muckatira, Anna Rumshisky | Training (meteorology), Computer science, Rank (graph theory), Scaling, Artificial neural network, Transformer, +16 more | http://arxiv.org/abs/2307.05695 | Despite the dominance and effectiveness of scaling, resulting in large networks with hundreds of billions of parameters, the necessity to train overparameterized models remains poorly understood, while training costs grow exponentially. In this paper, we explore parameter-efficient training techniques as an approach to training large neural networks. We introduce a novel method called ReLoRA, which utilizes low-rank updates to train high-rank networks. We apply ReLoRA to training transformer language models with up to 1.3B parameters and demonstrate comparable performance to regular neural network training. ReLoRA saves up to 5.5Gb of RAM per GPU and improves training speed by 9-40% depending on the model size and hardware setup. Our findings show the potential of parameter-efficient techniques for large-scale pre-training. |
| https://openalex.org/W4383218913 | Accurate medium-range global weather forecasting with 3D neural networks | 2023 | Nature | article | 1174 | yes | Kaifeng Bi, Lingxi Xie, Hengheng Zhang, Xin Chen, Xiaotao Gu, Qi Tian | Numerical weather prediction, Global Forecast System, Weather forecasting, Meteorology, Tropical cyclone forecast model, Range (aeronautics), +15 more | https://doi.org/10.1038/s41586-023-06185-3 |  |
| https://openalex.org/W4405844927 | RenAIssance: A Survey Into AI Text-to-Image Generation in the Era of Large Model | 2024 | IEEE Transactions on Pattern Analysis and Machine Intelligence | article | 25 | no | Fengxiang Bie, Yibo Yang, Zhongzhu Zhou, Adam Ghanem, Minjia Zhang, Zhewei Yao, Xiaoxia Wu, Connor Holmes, +5 more | The Renaissance, Computer science, Artificial intelligence, Image (mathematics), Computer vision, Natural language processing, +5 more | https://doi.org/10.1109/tpami.2024.3522305 | Text-to-image generation (TTI) refers to the usage of models that could process text input and generate high fidelity images based on text descriptions. Text-to-image generation using neural networks could be traced back to the emergence of Generative Adversial Network (GAN), followed by the autoregressive Transformer. Diffusion models are one prominent type of generative model used for the generation of images through the systematic introduction of noises with repeating steps. As an effect of the impressive results of diffusion models on image synthesis, it has been cemented as the major image decoder used by text-to-image models and brought text-to-image generation to the forefront of machine-learning (ML) research. In the era of large models, scaling up model size and the integration with large language models have further improved the performance of TTI models, resulting the generat… |
| https://openalex.org/W3128590981 | Scaling Laws for Transfer | 2021 | arXiv (Cornell University) | preprint | 26 | yes | Danny Hernandez, Jared Kaplan, Tom Henighan, Sam McCandlish | Generality, Scaling, Computer science, Scratch, Transfer of learning, Power law, +11 more | http://arxiv.org/abs/2102.01293 | We study empirical scaling laws for transfer learning between distributions in an unsupervised, fine-tuning setting. When we train increasingly large neural networks from-scratch on a fixed-size dataset, they eventually become data-limited and stop improving in performance (cross-entropy loss). When we do the same for models pre-trained on a large language dataset, the slope in performance gains is merely reduced rather than going to zero. We calculate the effective data "transferred" from pre-training by determining how much data a transformer of the same size would have required to achieve the same loss when training from scratch. In other words, we focus on units of data while holding everything else fixed. We find that the effective data transferred is described well in the low data regime by a power-law of parameter count and fine-tuning dataset size. We believe the exponents in th… |
| https://openalex.org/W4387010034 | Extrapolation of affective norms using transformer-based neural networks and its application to experimental stimuli selection | 2023 | Behavior Research Methods | article | 13 | yes | Hubert Plisiecki, Adam Sobieszek | Extrapolation, Computer science, Valence (chemistry), Artificial intelligence, Transformer, German, +14 more | https://doi.org/10.3758/s13428-023-02212-3 |  |
| https://openalex.org/W4386250621 | Generative pretrained autoregressive transformer graph neural network applied to the analysis and discovery of novel proteins | 2023 | Journal of Applied Physics | article | 25 | yes | Markus J. Buehler | Computer science, Transformer, Artificial intelligence, Machine learning, Generative grammar, Graph, +8 more | https://doi.org/10.1063/5.0157367 | We report a flexible language-model-based deep learning strategy, applied here to solve complex forward and inverse problems in protein modeling, based on an attention neural network that integrates transformer and graph convolutional architectures in a causal multi-headed graph mechanism, to realize a generative pretrained model. The model is applied to predict the secondary structure content (per-residue level and overall content), protein solubility, and sequencing tasks. Further trained on inverse tasks, the model is rendered capable of designing proteins with these properties as target features. The model is formulated as a general framework, completely prompt-based, and can be adapted for a variety of downstream tasks. We find that adding additional tasks yields emergent synergies that the model exploits in improving overall performance, beyond what would be possible by training a… |
| https://openalex.org/W4377224459 | Classifying European Court of Human Rights Cases Using Transformer-Based Techniques | 2023 | IEEE Access | article | 10 | yes | Ali Shariq Imran, Henrik Hodnefjeld, Zenun Kastrati, Noureen Fatima, Sher Muhammad Daudpota, Mudasir Ahmad Wani | Computer science, Artificial intelligence, Encoder, Machine learning, Transformer, Support vector machine, +11 more | https://doi.org/10.1109/access.2023.3279034 | In the field of text classification, researchers have repeatedly shown the value of transformer-based models such as Bidirectional Encoder Representation from Transformers (BERT) and its variants. Nonetheless, these models are expensive in terms of memory and computational power but have not been utilized to classify long documents of several domains. In addition, transformer models are also often pre-trained on generalized languages, making them less effective in language-specific domains, such as legal documents. In the natural language processing (NLP) domain, there is a growing interest in creating newer models that can handle more complex input sequences and domain-specific languages. Keeping the power of NLP in mind, this study proposes a legal documentation classifier that classifies the legal document by using the sliding window approach to increase the maximum sequence length o… |
| https://openalex.org/W3166796161 | Going Beyond Linear Transformers with Recurrent Fast Weight Programmers | 2021 | arXiv (Cornell University) | preprint | 13 | yes | Kazuki Irie, Imanol Schlag, Róbert Csordás, Jürgen Schmidhuber | Transformer, Computer science, Artificial neural network, Reinforcement learning, Feed forward, Scalability, +12 more | http://arxiv.org/abs/2106.06295 | Transformers with linearised attention (''linear Transformers'') have demonstrated the practical scalability and effectiveness of outer product-based Fast Weight Programmers (FWPs) from the '90s. However, the original FWP formulation is more general than the one of linear Transformers: a slow neural network (NN) continually reprograms the weights of a fast NN with arbitrary architecture. In existing linear Transformers, both NNs are feedforward and consist of a single layer. Here we explore new variations by adding recurrence to the slow and fast nets. We evaluate our novel recurrent FWPs (RFWPs) on two synthetic algorithmic tasks (code execution and sequential ListOps), Wikitext-103 language models, and on the Atari 2600 2D game environment. Our models exhibit properties of Transformers and RNNs. In the reinforcement learning setting, we report large improvements over LSTM in several A… |
| https://openalex.org/W4365143687 | Foundation models for generalist medical artificial intelligence | 2023 | Nature | review | 1324 | yes | Michael Moor, Oishi Banerjee, Zahra Shakeri Hossein Abad, Harlan M. Krumholz, Jure Leskovec, Eric J. Topol, Pranav Rajpurkar | Computer science, Set (abstract data type), Artificial intelligence, Modalities, Task (project management), Data science, +6 more | https://doi.org/10.1038/s41586-023-05881-4 | The exceptionally rapid development of highly flexible, reusable artificial intelligence (AI) models is likely to usher in newfound capabilities in medicine. We propose a new paradigm for medical AI, which we refer to as generalist medical AI (GMAI). GMAI models will be capable of carrying out a diverse set of tasks using very little or no task-specific labelled data. Built through self-supervision on large, diverse datasets, GMAI will flexibly interpret different combinations of medical modalities, including data from imaging, electronic health records, laboratory results, genomics, graphs or medical text. Models will in turn produce expressive outputs such as free-text explanations, spoken recommendations or image annotations that demonstrate advanced medical reasoning abilities. Here we identify a set of high-impact potential applications for GMAI and lay out specific technical capab… |
| https://openalex.org/W4385453147 | A Deep Diacritics-Based Recognition Model for Arabic Speech: Quranic Verses as Case Study | 2023 | IEEE Access | article | 10 | yes | Sarah S. Alrumiah, Amal A. Al-Shargabi | Computer science, Speech recognition, Pronunciation, Artificial intelligence, Word error rate, Recurrent neural network, +8 more | https://doi.org/10.1109/access.2023.3300972 | Arabic is the language of more than 422 million of the world&#x2019;s population. Although classic Arabic is the Quran language that 1.9 billion Muslims are required to recite, limited Arabic speech recognition exists. In classic Arabic, diacritics affect the pronunciation of a word, a change in a diacritic can change the meaning of a word. However, most of the Arabic-based speech recognition models discarded the diacritics. This work aims to recognize the classic Arabic speech while considering diacritics by converting audio signals to diacritized text using Deep Neural Network (DNN)-based models. The DNN-based model recognizes speech using DNN which outperformed the traditional speech recognition systems&#x2019; phonetics dependency. Three models were developed to recognize Arabic speech: (i) Time Delay Neural Network-Connectionist Temporal Classification (CTC), (ii) Recurrent Neural… |
| https://openalex.org/W4224137820 | Large-Scale Streaming End-to-End Speech Translation with Neural Transducers | 2022 | Interspeech 2022 | article | 18 | no | Jian Xue, Peidong Wang, Jinyu Li, Matt Post, Yashesh Gaur | Computer science, End-to-end principle, Speech recognition, Scale (ratio), Artificial intelligence, Physics, +1 more | https://doi.org/10.21437/interspeech.2022-10953 | Neural transducers have been widely used in automatic speech recognition (ASR). In this paper, we introduce it to streaming end-to-end speech translation (ST), which aims to convert audio signals to texts in other languages directly. Compared with cascaded ST that performs ASR followed by text-based machine translation (MT), the proposed Transformer transducer (TT)-based ST model drastically reduces inference latency, exploits speech information, and avoids error propagation from ASR to MT. To improve the modeling capacity, we propose attention pooling for the joint network in TT. In addition, we extend TT-based ST to multilingual ST, which generates texts of multiple languages at the same time. Experimental results on a large-scale 50 thousand (K) hours pseudo-labeled training set show that TT-based ST not only significantly reduces inference time but also outperforms non-streaming cas… |
| https://openalex.org/W4402713424 | Learning long sequences in spiking neural networks | 2024 | Scientific Reports | article | 12 | yes | Matei-Ioan Stan, Oliver Rhodes | Computer science, Spiking neural network, Artificial intelligence, Benchmark (surveying), Recurrent neural network, Neuromorphic engineering, +9 more | https://doi.org/10.1038/s41598-024-71678-8 | Abstract Spiking neural networks (SNNs) take inspiration from the brain to enable energy-efficient computations. Since the advent of Transformers, SNNs have struggled to compete with artificial networks on modern sequential tasks, as they inherit limitations from recurrent neural networks (RNNs), with the added challenge of training with non-differentiable binary spiking activations. However, a recent renewed interest in efficient alternatives to Transformers has given rise to state-of-the-art recurrent architectures named state space models (SSMs). This work systematically investigates, for the first time, the intersection of state-of-the-art SSMs with SNNs for long-range sequence modelling. Results suggest that SSM-based SNNs can outperform the Transformer on all tasks of a well-established long-range sequence modelling benchmark. It is also shown that SSM-based SNNs can outperform cu… |
| https://openalex.org/W3198685994 | CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation | 2021 | Proceedings of the 2021 Conference on Empirical Methods in Natural Language Pro… | article | 1117 | yes | Yue Wang, Weishi Wang, Shafiq Joty, Steven C. H. Hoi | Computer science, Identifier, Encoder, Source code, Code generation, Code (set theory), +10 more | https://doi.org/10.18653/v1/2021.emnlp-main.685 | Pre-trained models for Natural Languages (NL) like BERT and GPT have been recently shown to transfer well to Programming Languages (PL) and largely benefit a broad set of code-related tasks. Despite their success, most current methods either rely on an encoder-only (or decoder-only) pre-training that is suboptimal for generation (resp. understanding) tasks or process the code snippet in the same way as NL, neglecting the special characteristics of PL such as token types. We present CodeT5, a unified pre-trained encoder-decoder Transformer model that better leverages the code semantics conveyed from the developer-assigned identifiers. Our model employs a unified framework to seamlessly support both code understanding and generation tasks and allows for multi-task learning. Besides, we propose a novel identifier-aware pre-training task that enables the model to distinguish which code toke… |
| https://openalex.org/W4407031014 | LLM-Augmented Linear Transformer–CNN for Enhanced Stock Price Prediction | 2025 | Mathematics | article | 17 | yes | Lei Zhou, Yuqi Zhang, Jian Yu, Guiling Wang, Zhizhong Liu, Sira Yongchareon, Nancy Wang | Computer science, Transformer, Convolutional neural network, Stock (firearms), Deep learning, Technical analysis, +10 more | https://doi.org/10.3390/math13030487 | Accurately predicting stock prices remains a challenging task due to the volatile and complex nature of financial markets. In this study, we propose a novel hybrid deep learning framework that integrates a large language model (LLM), a Linear Transformer (LT), and a Convolutional Neural Network (CNN) to enhance stock price prediction using solely historical market data. The framework leverages the LLM as a professional financial analyst to perform daily technical analysis. The technical indicators, including moving averages (MAs), relative strength index (RSI), and Bollinger Bands (BBs), are calculated directly from historical stock data. These indicators are then analyzed by the LLM, generating descriptive textual summaries. The textual summaries are further transformed into vector representations using FinBERT, a pre-trained financial language model, to enhance the dataset with contex… |
| https://openalex.org/W3163368926 | Mixed Precision Quantization of Transformer Language Models for Speech Recognition | 2021 |  | article | 14 | no | Junhao Xu, Shoukang Hu, Jianwei Yu, Xunying Liu, Helen Meng | Quantization (signal processing), Computer science, Transformer, Artificial neural network, Language model, Speech recognition, +5 more | https://doi.org/10.1109/icassp39728.2021.9414076 | State-of-the-art neural language models represented by Transformers are becoming increasingly complex and expensive for practical applications. Low-bit deep neural network quantization techniques provides a powerful solution to dramatically reduce their model size. Current low-bit quantization methods are based on uniform precision and fail to account for the varying performance sensitivity at different parts of the system to quantization errors. To this end, novel mixed precision DNN quantization methods are proposed in this paper. The optimal local precision settings are automatically learned using two techniques. The first is based on a quantization sensitivity metric in the form of Hessian trace weighted quantization perturbation. The second is based on mixed precision Transformer architecture search. Alternating direction methods of multipliers (ADMM) are used to efficiently train… |
| https://openalex.org/W4390872954 | Enhancing NeRF akin to Enhancing LLMs: Generalizable NeRF Transformer with Mixture-of-View-Experts | 2023 |  | article | 19 | no | Wenyan Cong, Hanxue Liang, Peihao Wang, Zhiwen Fan, Tianlong Chen, Mukund Varma, Yi Wang, Zhangyang Wang | Computer science, Artificial intelligence, Transformer, Inference, Rendering (computer graphics), View synthesis, +5 more | https://doi.org/10.1109/iccv51070.2023.00296 | Cross-scene generalizable NeRF models, which can directly synthesize novel views of unseen scenes, have become a new spotlight of the NeRF field. Several existing attempts rely on increasingly end-to-end "neuralized" architectures, i.e., replacing scene representation and/or rendering modules with performant neural networks such as transformers, and turning novel view synthesis into a feed-forward inference pipeline. While those feedforward "neuralized" architectures still do not fit diverse scenes well out of the box, we propose to bridge them with the powerful Mixture-of-Experts (MoE) idea from large language models (LLMs), which has demonstrated superior generalization ability by balancing between larger overall model capacity and flexible per-instance specialization. Starting from a recent generalizable NeRF architecture called GNT [52], we first demonstrate that MoE can be neatly p… |
| https://openalex.org/W2972054167 | Maestro: A Memory-on-Logic Architecture for Coordinated Parallel Use of Many Systolic Arrays | 2019 |  | article | 26 | no | H. T. Kung, Bradley McDanel, Sai Qian Zhang, Xin Dong, Chih Chiang Chen | Computer science, Parallel computing, Latency (audio), Architecture, Inference, Systolic array, +12 more | https://doi.org/10.1109/asap.2019.00-31 | We present the Maestro memory-on-logic 3D-IC architecture for coordinated parallel use of a plurality of systolic arrays (SAs) in performing deep neural network (DNN) inference. Maestro reduces under-utilization common for a single large SA by allowing parallel use of many smaller SAs on DNN weight matrices of varying shapes and sizes. In order to buffer immediate results in memory blocks (MBs) and provide coordinated high-bandwidth communication between SAs and MBs in transferring weights and results Maestro employs three innovations. (1) An SA on the logic die can access its corresponding MB on the memory die in short distance using 3D-IC interconnects, (2) through an efficient switch based on H-trees, an SA can access any MB with low latency, and (3) the switch can combine partial results from SAs in an elementwise fashion before writing back to a destination MB. We describe the Maes… |
| https://openalex.org/W2946794439 | Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned | 2019 |  | article | 998 | yes | Elena Voita, David Talbot, Fédor Moiseev, Rico Sennrich, Ivan Titov | Computer science, Machine translation, Encoder, Transformer, Artificial intelligence, Pruning, +13 more | https://doi.org/10.18653/v1/p19-1580 | Multi-head self-attention is a key component of the Transformer, a state-of-the-art architecture for neural machine translation. In this work we evaluate the contribution made by individual attention heads to the overall performance of the model and analyze the roles played by them in the encoder. We find that the most important and confident heads play consistent and often linguistically-interpretable roles. When pruning heads using a method based on stochastic gates and a differentiable relaxation of the L0 penalty, we observe that specialized heads are last to be pruned. Our novel pruning method removes the vast majority of heads without seriously affecting performance. For example, on the English-Russian WMT dataset, pruning 38 out of 48 encoder heads results in a drop of only 0.15 BLEU. |
| https://openalex.org/W4402842255 | The Accuracy and Capability of Artificial Intelligence Solutions in Health Care Examinations and Certificates: Systematic Review and Meta-Analysis | 2024 | Journal of Medical Internet Research | review | 26 | yes | William J. Waldock, Joe Zhang, Ahmad Guni, Ahmad Nabeel, Ara Darzi, Hutan Ashrafian | Preprint, Meta-analysis, Health care, Psychology, Medical education, Computer science, +7 more | https://doi.org/10.2196/56532 | Background Large language models (LLMs) have dominated public interest due to their apparent capability to accurately replicate learned knowledge in narrative text. However, there is a lack of clarity about the accuracy and capability standards of LLMs in health care examinations. Objective We conducted a systematic review of LLM accuracy, as tested under health care examination conditions, as compared to known human performance standards. Methods We quantified the accuracy of LLMs in responding to health care examination questions and evaluated the consistency and quality of study reporting. The search included all papers up until September 10, 2023, with all LLMs published in English journals that report clear LLM accuracy standards. The exclusion criteria were as follows: the assessment was not a health care exam, there was no LLM, there was no evaluation of comparable success accura… |
| https://openalex.org/W2946417913 | BERT Rediscovers the Classical NLP Pipeline | 2019 |  | article | 1235 | yes | Ian Tenney, Dipanjan Das, Ellie Pavlick | Coreference, Pipeline (software), Computer science, Natural language processing, Parsing, Artificial intelligence, +7 more | https://doi.org/10.18653/v1/p19-1452 | Pre-trained text encoders have rapidly advanced the state of the art on many NLP tasks. We focus on one such model, BERT, and aim to quantify where linguistic information is captured within the network. We find that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lower-level decisions on the basis of disambiguating information from higher-level representations. |
| https://openalex.org/W2947037928 | Pre-training of Graph Augmented Transformers for Medication Recommendation | 2019 | arXiv (Cornell University) | preprint | 18 | yes | Junyuan Shang, Tengfei Ma, Cao Xiao, Jimeng Sun | Computer science, Encoder, Transformer, Artificial intelligence, Machine learning, Health records, +16 more | http://arxiv.org/abs/1906.00346 | Medication recommendation is an important healthcare application. It is commonly formulated as a temporal prediction task. Hence, most existing works only utilize longitudinal electronic health records (EHRs) from a small number of patients with multiple visits ignoring a large number of patients with a single visit (selection bias). Moreover, important hierarchical knowledge such as diagnosis hierarchy is not leveraged in the representation learning process. To address these challenges, we propose G-BERT, a new model to combine the power of Graph Neural Networks (GNNs) and BERT (Bidirectional Encoder Representations from Transformers) for medical code representation and medication recommendation. We use GNNs to represent the internal hierarchical structures of medical codes. Then we integrate the GNN representation into a transformer-based visit encoder and pre-train it on EHR data fro… |
| https://openalex.org/W4392295981 | Learning stochastic dynamics and predicting emergent behavior using transformers | 2024 | Nature Communications | article | 8 | yes | Corneel Casert, Isaac Tamblyn, Stephen Whitelam | Statistical physics, Computer science, Granularity, Transformer, Dynamical systems theory, Non-equilibrium thermodynamics, +11 more | https://doi.org/10.1038/s41467-024-45629-w | Abstract We show that a neural network originally designed for language processing can learn the dynamical rules of a stochastic system by observation of a single dynamical trajectory of the system, and can accurately predict its emergent behavior under conditions not observed during training. We consider a lattice model of active matter undergoing continuous-time Monte Carlo dynamics, simulated at a density at which its steady state comprises small, dispersed clusters. We train a neural network called a transformer on a single trajectory of the model. The transformer, which we show has the capacity to represent dynamical rules that are numerous and nonlocal, learns that the dynamics of this model consists of a small number of processes. Forward-propagated trajectories of the trained transformer, at densities not encountered during training, exhibit motility-induced phase separation and… |
| https://openalex.org/W4403276261 | The Evolution of Artificial Intelligence in Medical Imaging: From Computer Science to Machine and Deep Learning | 2024 | Preprints.org | preprint | 13 | yes | Michele Avanzo, Joseph Stancanello, G. Pirrone, Annalisa Drigo, Alessandra Retico | Artificial intelligence, Computer science, Deep learning, Cognitive science, Data science, Psychology | https://doi.org/10.20944/preprints202410.0025.v1 | Artificial intelligence (AI), the wide spectrum of technologies aiming to give machines or computers the ability to perform human-like cognitive functions, began in the 40s with the first abstract models of intelligent machines. Soon later in the 50s and 60s machine learning algorithms such as neural networks and decision trees ignited large enthusiasm. More recent advancements include the refinement of learning algorithms, the development of convolutional neural networks to efficiently analyze images, and methods to synthesize new images. The renewed enthusiasm was also due to the increase in computational power with graphical processing units and the availability of large digital databases to be mined by neural networks. AI soon began to be applied in medicine, first through expert systems designed to support the clinician’s decision, and later with neural networks for the detection o… |
| https://openalex.org/W4389895915 | Attention network for predicting T-cell receptor–peptide binding can associate attention with interpretable protein structural properties | 2023 | Frontiers in Bioinformatics | article | 13 | yes | Kyohei Koyama, Kosuke Hashimoto, Chioko Nagao, Kenji Mizuguchi | T-cell receptor, Computational biology, Major histocompatibility complex, Complementarity (molecular biology), Computer science, Peptide, +14 more | https://doi.org/10.3389/fbinf.2023.1274599 | Understanding how a T-cell receptor (TCR) recognizes its specific ligand peptide is crucial for gaining an insight into biological functions and disease mechanisms. Despite its importance, experimentally determining TCR–peptide–major histocompatibility complex (TCR–pMHC) interactions is expensive and time-consuming. To address this challenge, computational methods have been proposed, but they are typically evaluated by internal retrospective validation only, and few researchers have incorporated and tested an attention layer from language models into structural information. Therefore, in this study, we developed a machine learning model based on a modified version of Transformer, a source–target attention neural network, to predict the TCR–pMHC interaction solely from the amino acid sequences of the TCR complementarity-determining region (CDR) 3 and the peptide. This model achieved comp… |
| https://openalex.org/W3184008713 | Training Deep Code Comment Generation Models via Data Augmentation | 2020 |  | article | 15 | no | Xiaoqing Zhang, Yu Zhou, Tingting Han, Taolue Chen | Computer science, Robustness (evolution), Transformer, Artificial intelligence, Source code, Deep neural networks, +17 more | https://doi.org/10.1145/3457913.3457937 | With the development of deep neural networks (DNNs) and the publicly available source code repositories, deep code comment generation models have demonstrated reasonable performance on test datasets. However, it has been confirmed in computer vision (CV) and natural language processing (NLP) that DNNs are vulnerable to adversarial examples. In this paper, we investigate how to maintain the performance of the models against these perturbed samples. We propose a simple, but effective, method to improve the robustness by training the model via data augmentation. We conduct experiments to evaluate our approach on two mainstream sequence-sequence (seq2seq) architectures which are based on the LSTM and the Transformer with a large-scale publicly available dataset. The experimental results demonstrate that our method can efficiently improve the capability of different models to defend the pert… |
| https://openalex.org/W4391272424 | A Survey of Deep Learning and Foundation Models for Time Series Forecasting | 2024 | arXiv (Cornell University) | preprint | 21 | yes | John A. Miller, Mohammed S. Al‐Dosari, Farah Saeed, Nasid Habib Barna, Subas Rana, I. Budak Arpinar, Ninghao Liu | Deep learning, Artificial intelligence, Interpretability, Computer science, Machine learning, Domain knowledge, +1 more | http://arxiv.org/abs/2401.13912 | Deep Learning has been successfully applied to many application domains, yet its advantages have been slow to emerge for time series forecasting. For example, in the well-known Makridakis (M) Competitions, hybrids of traditional statistical or machine learning techniques have only recently become the top performers. With the recent architectural advances in deep learning being applied to time series forecasting (e.g., encoder-decoders with attention, transformers, and graph neural networks), deep learning has begun to show significant advantages. Still, in the area of pandemic prediction, there remain challenges for deep learning models: the time series is not long enough for effective training, unawareness of accumulated scientific knowledge, and interpretability of the model. To this end, the development of foundation models (large deep learning models with extensive pre-training) all… |
| https://openalex.org/W3048352504 | Fortschritte bei der neuronalen Sprachmodellierung in der automatischen Spracherkennung | 2020 | RWTH Publications (RWTH Aachen) | article | 7 | yes | Kazuki Irie | Computer science, Natural language processing, Speech recognition, Artificial intelligence, Language model, Artificial neural network, +2 more | https://doi.org/10.18154/rwth-2020-04984 | Statistical language modeling is one of the fundamental problems in natural language processing. In the recent years, language modeling has seen great advances by active research and engineering efforts in applying artificial neural networks, especially those which are recurrent. The application of neural language models to speech recognition has now become well established and ubiquitous. Despite this impression of some degree of maturity, we claim that the full potential of the neural network based language modeling is yet to be explored. In this thesis, we further advance neural language modeling in automatic speech recognition, by investigating a number of new perspectives. From the architectural view point, we investigate the newly proposed Transformer neural net- works for language modeling application. The original model architecture proposed for machine translation is studied an… |
| https://openalex.org/W4360942793 | Arabic Speech Recognition Based on Encoder-Decoder Architecture of Transformer | 2023 | Journal of Techniques | article | 8 | yes | Mohanad Sameer, Ahmed Karim Talib, Alla Hussein | Computer science, Speech recognition, Transformer, Encoder, Recurrent neural network, Word error rate, +18 more | http://dx.doi.org/10.51173/jt.v5i1.749 | Recognizing and transcribing human speech has become an increasingly important task. Recently, researchers have been more interested in automatic speech recognition (ASR) using End to End models. Previous choices for the Arabic ASR architecture have been time-delay neural networks, recurrent neural networks (RNN), and long short-term memory (LSTM). Preview end-to-end approaches have suffered from slow training and inference speed because of the limitations of training parallelization, and they require a large amount of data to achieve acceptable results in recognizing Arabic speech This research presents an Arabic speech recognition based on a transformer encoder-decoder architecture with self-attention to transcribe Arabic audio speech segments into text, which can be trained faster with more efficiency. The proposed model exceeds the performance of previous end-to-end approaches when… |
| https://openalex.org/W4403735204 | Efficient Deep Learning Infrastructures for Embedded Computing Systems: A Comprehensive Survey and Future Envision | 2024 | ACM Transactions on Embedded Computing Systems | article | 13 | yes | Xiangzhong Luo, Di Liu, Hao Kong, Shuo Huai, Hui Chen, Guochu Xiong, Weichen Liu | Computer science, Deep learning, Data science, Knowledge management, Artificial intelligence | https://doi.org/10.1145/3701728 | Deep neural networks (DNNs) have recently achieved impressive success across a wide range of real-world vision and language processing tasks, spanning from image classification to many other downstream vision tasks, such as object detection, tracking, and segmentation. However, previous well-established DNNs, despite being able to maintain superior accuracy, have also been evolving to be deeper and wider and thus inevitably necessitate prohibitive computational resources for both training and inference. This trend further enlarges the computational gap between computation-intensive DNNs and resource-constrained embedded computing systems, making it challenging to deploy powerful DNNs in real-world embedded computing systems towards ubiquitous embedded intelligence. To alleviate this computational gap and enable ubiquitous embedded intelligence, we focus in this survey on discussing rece… |
| https://openalex.org/W3016960123 | lamBERT: Language and Action Learning Using Multimodal BERT | 2020 | arXiv (Cornell University) | preprint | 11 | yes | Kazuki Miyazawa, Tatsuya Aoki, Takato Horii, Takayuki Nagai | Computer science, Transformer, Language model, Artificial intelligence, Reinforcement learning, Representation (politics), +9 more | http://arxiv.org/abs/2004.07093 | Recently, the bidirectional encoder representations from transformers (BERT) model has attracted much attention in the field of natural language processing, owing to its high performance in language understanding-related tasks. The BERT model learns language representation that can be adapted to various tasks via pre-training using a large corpus in an unsupervised manner. This study proposes the language and action learning using multimodal BERT (lamBERT) model that enables the learning of language and actions by 1) extending the BERT model to multimodal representation and 2) integrating it with reinforcement learning. To verify the proposed model, an experiment is conducted in a grid environment that requires language understanding for the agent to act properly. As a result, the lamBERT model obtained higher rewards in multitask settings and transfer settings when compared to other mo… |
| https://openalex.org/W4241840347 | Sentence Representation | 2020 |  | book-chapter | 6 | yes | Zhiyuan Liu, Yankai Lin, Maosong Sun | Computer science, Sentence, Natural language processing, Artificial intelligence, Automatic summarization, Machine translation, +7 more | https://doi.org/10.1007/978-981-15-5573-2_4 | Abstract Sentence is an important linguistic unit of natural language. Sentence Representation has remained as a core task in natural language processing, because many important applications in related fields lie on understanding sentences, for example, summarization, machine translation, sentiment analysis, and dialogue system. Sentence representation aims to encode the semantic information into a real-valued representation vector, which will be utilized in further sentence classification or matching tasks. With large-scale text data available on the Internet and recent advances on deep neural networks, researchers tend to employ neural networks (e.g., convolutional neural networks and recurrent neural networks) to learn low-dimensional sentence representations and achieve great progress on relevant tasks. In this chapter, we first introduce the one-hot representation for sentences and… |
| https://openalex.org/W4381827750 | Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing | 2023 | arXiv (Cornell University) | preprint | 10 | yes | Yelysei Bondarenko, Markus Nagel, Tijmen Blankevoort | Outlier, Computer science, Softmax function, Transformer, Quantization (signal processing), Artificial intelligence, +8 more | http://arxiv.org/abs/2306.12929 | Transformer models have been widely adopted in various domains over the last years, and especially large language models have advanced the field of AI significantly. Due to their size, the capability of these networks has increased tremendously, but this has come at the cost of a significant increase in necessary compute. Quantization is one of the most effective ways to reduce the computational time and memory consumption of neural networks. Many studies have shown, however, that modern transformer models tend to learn strong outliers in their activations, making them difficult to quantize. To retain acceptable performance, the existence of these outliers requires activations to be in higher bitwidth or the use of different numeric formats, extra fine-tuning, or other workarounds. We show that strong outliers are related to very specific behavior of attention heads that try to learn a… |
| https://openalex.org/W3139537596 | Finetuning Pretrained Transformers into RNNs | 2021 | Proceedings of the 2021 Conference on Empirical Methods in Natural Language Pro… | preprint | 9 | yes | Jungo Kasai, Hao Peng, Yizhe Zhang, Dani Yogatama, Gabriel Ilharco, Nikolaos Pappas, Yi Mao, Weizhu Chen, +1 more | Softmax function, Computer science, Transformer, Recurrent neural network, Inference, Artificial intelligence, +7 more | https://doi.org/10.18653/v1/2021.emnlp-main.830 | Transformers have outperformed recurrent neural networks (RNNs) in natural language generation. But this comes with a significant computational cost, as the attention mechanism's complexity scales quadratically with sequence length. Efficient transformer variants have received increasing interest in recent works. Among them, a linear-complexity recurrent variant has proven well suited for autoregressive generation. It approximates the softmax attention with randomized or heuristic feature maps, but can be difficult to train and may yield suboptimal accuracy. This work aims to convert a pretrained transformer into its efficient recurrent counterpart, improving efficiency while maintaining accuracy. Specifically, we propose a swap-then-finetune procedure: in an off-the-shelf pretrained transformer, we replace the softmax attention with its linear-complexity recurrent alternative and then… |
| https://openalex.org/W4386713326 | Image-Based Lunar Hazard Detection in Low Illumination Simulated Conditions via Vision Transformers | 2023 | Sensors | article | 9 | yes | Luca Ghilardi, Roberto Furfaro | Terrain, Computer science, Artificial intelligence, Computer vision, Convolutional neural network, Deep learning, +9 more | https://doi.org/10.3390/s23187844 | Hazard detection is fundamental for a safe lunar landing. State-of-the-art autonomous lunar hazard detection relies on 2D image-based and 3D Lidar systems. The lunar south pole is challenging for vision-based methods. The low sun inclination and the terrain rich in topographic features create large areas in shadow, hiding the terrain features. The proposed method utilizes a vision transformer (ViT) model, which is a deep learning architecture based on the transformer blocks used in natural language processing, to solve this problem. Our goal is to train the ViT model to extract terrain features information from low-light RGB images. The results show good performances, especially at high altitudes, beating the UNet, one of the most popular convolutional neural networks, in every scenario. |
| https://openalex.org/W3114632476 | A Survey on Contrastive Self-Supervised Learning | 2020 | MDPI (MDPI AG) | article | 1447 | yes | Ashish Jaiswal | Computer science, Artificial intelligence, Pretext, Machine learning, Embedding, Popularity, +11 more | https://dx.doi.org/10.3390/technologies9010002 | Self-supervised learning has gained popularity because of its ability to avoid the cost of annotating large-scale datasets. It is capable of adopting self-defined pseudolabels as supervision and use the learned representations for several downstream tasks. Specifically, contrastive learning has recently become a dominant component in self-supervised learning for computer vision, natural language processing (NLP), and other domains. It aims at embedding augmented versions of the same sample close to each other while trying to push away embeddings from different samples. This paper provides an extensive review of self-supervised methods that follow the contrastive approach. The work explains commonly used pretext tasks in a contrastive learning setup, followed by different architectures that have been proposed so far. Next, we present a performance comparison of different methods for mult… |
| https://openalex.org/W4389475249 | GPT-Assisted Learning of Structure–Property Relationships by Graph Neural Networks: Application to Rare-Earth-Doped Phosphors | 2023 | The Journal of Physical Chemistry Letters | article | 14 | no | Xiang Zhang, Zichun Zhou, Ming Chen, Yi‐Yang Sun | Phosphor, Computer science, Artificial neural network, Transformer, Graph, Convolutional neural network, +11 more | https://doi.org/10.1021/acs.jpclett.3c02848 | Two challenges facing machine learning tasks in materials science are data set construction and descriptor design. Graph neural networks circumvent the need for empirical descriptors by encoding geometric information in graphs. Large language models have shown promise for database construction via text extraction. Here, we apply OpenAI's Generative Pre-trained Transformer 4 (GPT-4) and the Crystal Graph Convolutional Neural Network (CGCNN) to the problem of discovering rare-earth-doped phosphors for solid-state lighting. We used GPT-4 to datamine the chemical formulas and emission wavelengths of 264 Eu<sup>2+</sup>-doped phosphors from 274 articles. A CGCNN model was trained on the acquired data set, achieving a test <i>R</i><sup>2</sup> of 0.77. Using this model, we predicted the emission wavelengths of over 40 000 inorganic materials. We also used transfer learning to fine-tune a band… |
| https://openalex.org/W4393234129 | Learning from Nature to Achieve Material Sustainability: Generative AI for Rigorous Bio-inspired Materials Design | 2024 |  | article | 17 | yes | Rachel K. Luu, Sofia Arevalo, Wei Lu, Bo Ni, Zhenze Yang, Sabrina C. Shen, Jaime Berkovich, Yu-Chuan Hsu, +2 more | Sustainability, Generative grammar, Artificial intelligence, Computer science, Architectural engineering, Engineering ethics, +4 more | https://doi.org/10.21428/e4baedd9.33bd7449 | Nature has severely outpaced humans in developing multifunctional, hierarchical materials that access impressive material properties, all the while being fully degradable and part of native ecosystems. But how can we effectively model the intricate time and length scales in biological systems to translate design principles to meet engineering demands? We postulate that generative artificial intelligence (AI) can play a crucial role in solving this interdisciplinary challenge, translating insights across disparate knowledge domains and forming the basis for a new sustainable materials economy. Techniques like generative adversarial networks, transformer neural networks, and diffusion denoising modeling have been used to solve complex bio-inspired design challenges. Emerging tools such as multimodal large language models provide robust foundations with reasoning abilities that can be mult… |
| https://openalex.org/W4380884690 | ChatGPT and Other Natural Language Processing Artificial Intelligence Models in Adult Reconstruction | 2023 | The Journal of Arthroplasty | editorial | 12 | no | Matthew L. Magruder, Ronald E. Delanois, James Nace, Michael A. Mont | Transformer, Artificial intelligence, Artificial neural network, Natural language processing, Natural language understanding, Natural language, +11 more | https://doi.org/10.1016/j.arth.2023.06.030 |  |
| https://openalex.org/W4287688861 | Is Supervised Syntactic Parsing Beneficial for Language Understanding?\n An Empirical Investigation | 2020 | arXiv (Cornell University) | preprint | 10 | yes | Goran Glavaš, Ivan Vulić | Computer science, Parsing, Natural language processing, Transformer, Artificial intelligence, Syntax, +5 more | http://arxiv.org/abs/2008.06788 | Traditional NLP has long held (supervised) syntactic parsing necessary for\nsuccessful higher-level semantic language understanding (LU). The recent advent\nof end-to-end neural models, self-supervised via language modeling (LM), and\ntheir success on a wide range of LU tasks, however, questions this belief. In\nthis work, we empirically investigate the usefulness of supervised parsing for\nsemantic LU in the context of LM-pretrained transformer networks. Relying on\nthe established fine-tuning paradigm, we first couple a pretrained transformer\nwith a biaffine parsing head, aiming to infuse explicit syntactic knowledge\nfrom Universal Dependencies treebanks into the transformer. We then fine-tune\nthe model for LU tasks and measure the effect of the intermediate parsing\ntraining (IPT) on downstream LU task performance. Results from both monolingual\nEnglish and zero-shot language tran… |
| https://openalex.org/W4283464866 | GNoM: Graph Neural Network Enhanced Language Models for Disaster Related Multilingual Text Classification | 2022 |  | article | 16 | no | Samujjwal Ghosh, Subhadeep Maji, Maunendra Sankar Desarkar | Computer science, Generalizability theory, Natural language processing, Categorization, Artificial intelligence, Transformer, +11 more | https://doi.org/10.1145/3501247.3531561 | Online social media works as a source of various valuable and actionable information during disasters. These information might be available in multiple languages due to the nature of user generated content. An effective system to automatically identify and categorize these actionable information should be capable to handle multiple languages and under limited supervision. However, existing works mostly focus on English language only with the assumption that sufficient labeled data is available. To overcome these challenges, we propose a multilingual disaster related text classification system which is capable to work undervmonolingual, cross-lingual and multilingual lingual scenarios and under limited supervision. Our end-to-end trainable framework combines the versatility of graph neural networks, by applying over the corpus, with the power of transformer based large language models, o… |
| https://openalex.org/W4399053202 | Vision-Enabled Large Language and Deep Learning Models for Image-Based Emotion Recognition | 2024 | Cognitive Computation | article | 31 | no | Mohammad Nadeem, Shahab Saquib Sohail, Laeeba Javed, Faisal Anwer, Abdul Khader Jilani Saudagar, Khan Muhammad | Computer science, Artificial intelligence, Convolutional neural network, Transformer, Generative grammar, Generative model, +7 more | https://doi.org/10.1007/s12559-024-10281-5 |  |
| https://openalex.org/W3095319910 | GPT-3: Its Nature, Scope, Limits, and Consequences | 2020 | Minds and Machines | article | 2013 | yes | Luciano Floridi, Massimo Chiriatti | Turing test, Philosophy of science, Scope (computer science), Turing, Epistemology, Philosophy of mind, +11 more | https://doi.org/10.1007/s11023-020-09548-1 | Abstract In this commentary, we discuss the nature of reversible and irreversible questions, that is, questions that may enable one to identify the nature of the source of their answers. We then introduce GPT-3, a third-generation, autoregressive language model that uses deep learning to produce human-like texts, and use the previous distinction to analyse it. We expand the analysis to present three tests based on mathematical, semantic (that is, the Turing Test), and ethical questions and show that GPT-3 is not designed to pass any of them. This is a reminder that GPT-3 does not do what it is not supposed to do, and that any interpretation of GPT-3 as the beginning of the emergence of a general form of artificial intelligence is merely uninformed science fiction. We conclude by outlining some of the significant consequences of the industrialisation of automatic and cheap production of… |
| https://openalex.org/W3173777717 | Making Pre-trained Language Models Better Few-shot Learners | 2021 |  | article | 1209 | yes | Tianyu Gao, Adam Fisch, Danqi Chen | Chen, Computer science, Joint (building), Computational linguistics, Natural language processing, Artificial intelligence, +16 more | https://doi.org/10.18653/v1/2021.acl-long.295 | Tianyu Gao, Adam Fisch, Danqi Chen. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021. |
| https://openalex.org/W4317910584 | ChatGPT: Bullshit spewer or the end of traditional assessments in higher education? | 2023 | Journal of Applied Learning & Teaching | article | 1557 | yes | Jürgen Rudolph, Samson Tan, Shannon Tan | Chatbot, Conversation, Relevance (law), Context (archaeology), Higher education, Computer science, +11 more | https://doi.org/10.37074/jalt.2023.6.1.9 | ChatGPT is the world’s most advanced chatbot thus far. Unlike other chatbots, it can create impressive prose within seconds, and it has created much hype and doomsday predictions when it comes to student assessment in higher education and a host of other matters. ChatGPT is a state-of-the-art language model (a variant of OpenAI’s Generative Pretrained Transformer (GPT) language model) designed to generate text that can be indistinguishable from text written by humans. It can engage in conversation with users in a seemingly natural and intuitive way. In this article, we briefly tell the story of OpenAI, the organisation behind ChatGPT. We highlight the fundamental change from a not-for-profit organisation to a commercial business model. In terms of our methods, we conducted an extensive literature review and experimented with this artificial intelligence (AI) software. Our literature rev… |
| https://openalex.org/W2981710752 | An Empirical Study of Efficient ASR Rescoring with Transformers | 2019 | arXiv (Cornell University) | preprint | 8 | yes | Hongzhao Huang, Fuchun Peng | Computer science, Transformer, Language model, Softmax function, Deep neural networks, Artificial neural network, +6 more | http://arxiv.org/abs/1910.11450 | Neural language models (LMs) have been proved to significantly outperform classical n-gram LMs for language modeling due to their superior abilities to model long-range dependencies in text and handle data sparsity problems. And recently, well configured deep Transformers have exhibited superior performance over shallow stack of recurrent neural network layers for language modeling. However, these state-of-the-art deep Transformer models were mostly engineered to be deep with high model capacity, which makes it computationally inefficient and challenging to be deployed into large-scale real-world applications. Therefore, it is important to develop Transformer LMs that have relatively small model sizes, while still retaining good performance of those much larger models. In this paper, we aim to conduct empirical study on training Transformers with small parameter sizes in the context of… |
| https://openalex.org/W4385562468 | Test Accuracy vs. Generalization Gap: Model Selection in NLP without Accessing Training or Testing Data | 2023 |  | article | 11 | yes | Yaoqing Yang, Ryan Theisen, Liam Hodgkinson, Joseph E. Gonzalez, Kannan Ramchandran, Charles H. Martin, Michael W. Mahoney | Hyperparameter, Computer science, Artificial intelligence, Machine learning, Generalization, Model selection, +14 more | https://doi.org/10.1145/3580305.3599518 | Selecting suitable architecture parameters and training hyperparameters is essential for enhancing machine learning (ML) model performance. Several recent empirical studies conduct large-scale correlational analysis on neural networks (NNs) to search for effective generalization metrics that can guide this type of model selection. Effective metrics are typically expected to correlate strongly with test performance. In this paper, we expand on prior analyses by examining generalization-metric-based model selection with the following objectives: (i) focusing on natural language processing (NLP) tasks, as prior work primarily concentrates on computer vision (CV) tasks; (ii) considering metrics that directly predict test error instead of the generalization gap; (iii) exploring metrics that do not need access to data to compute. From these objectives, we are able to provide the first model s… |
| https://openalex.org/W4401415327 | Fine-Tuned Understanding: Enhancing Social Bot Detection With Transformer-Based Classification | 2024 | IEEE Access | article | 11 | yes | Amine Sallah, El Arbi Abdellaoui Alaoui, Saïd Agoujil, Mudasir Ahmad Wani, Mohamed Hammad, Yassine Maleh, Ahmed A. Abd El‐Latif | Computer science, Language model, Transformer, Preprocessor, Artificial intelligence, Artificial neural network, +10 more | https://doi.org/10.1109/access.2024.3440657 | In recent years, the proliferation of online communication platforms and social media has given rise to a new wave of challenges, including the rapid spread of malicious bots. These bots, often programmed to impersonate human users, can infiltrate online communities, disseminate misinformation, and engage in various activities detrimental to the integrity of digital discourse. It is becoming more and more difficult to discern a text produced by deep neural networks from that created by humans. Transformer-based Pre-trained Language Models (PLMs) have recently shown excellent results in challenges involving natural language understanding (NLU). The suggested method is to employ an approach to detect bots at the tweet level by utilizing content and fine-tuning PLMs, to reduce the current threat. Building on the recent developments of the BERT (Bidirectional Encoder Representations from Tr… |
| https://openalex.org/W2994891905 | Incorporating Label Co-Occurrence Into Neural Network-Based Models for Multi-Label Text Classification | 2019 | IEEE Access | article | 9 | yes | Jiaqi Yao, Keren Wang, Jikun Yan | Computer science, Initialization, Embedding, Artificial neural network, Artificial intelligence, Class (philosophy), +11 more | https://doi.org/10.1109/access.2019.2960626 | Multi-label text classification (MLTC) addresses a fundamental problem in natural language processing, which assigns multiple relevant labels to each document. In recent years, Neural Network-based models (NN models) for MLTC have attracted much attention. In addition, NN models achieve favorable performances because they can exploit label correlations in the penultimate layer. To further capture and explore label correlations, we propose a novel initialization to incorporate label co-occurrence into NN models. First, we represent each class as a column vector of the weight matrix in the penultimate layer, which we name the class embedding matrix. Second, we deduce an equation for correlating the class embedding matrix with the label co-occurrence matrix, ensuring that relevant classes are denoted by vectors with large correlations. Finally, we provide a theoretical analysis of the equa… |
| https://openalex.org/W4312231462 | A Novel Model Combining Transformer and Bi-LSTM for News Categorization | 2022 | IEEE Transactions on Computational Social Systems | article | 9 | no | Yuanzhi Liu, Min He, Mengjia Shi, Seunggil Jeon | Categorization, Computer science, Transformer, Artificial intelligence, Natural language processing, Machine learning, +3 more | https://doi.org/10.1109/tcss.2022.3223621 | News categorization (NC), the aim of which is to identify distinct categories of news through analyzing the contents, has acquired substantial progress since deep learning was introduced into the natural language processing (NLP) field. As a state-of-art model, transformer's classification performance is not satisfied compared with recurrent neural network (RNN) and convolutional neural network (CNN) if it does not get pretrained. Based on the transformer model, this article proposes a novel framework that combines bidirectional long short-term memory (Bi-LSTM) network and transformer to solve this problem. In the suggested framework, the self-attention mechanism is substituted with Bi-LSTM to capture the semantic information from sentences. Meanwhile, an attention mechanism model is applied to focus on those important words and adjust their weights to solve the problem of long-distance… |
| https://openalex.org/W4285124216 | Transformer Model and Convolutional Neural Networks (CNNs) for Arabic to English Machine Translation | 2022 | Lecture notes in networks and systems | book-chapter | 9 | no | Nouhaila Bensalah, Habib Ayad, Abdellah Adib, Abdelhamid Ibn El Farouk | Transformer, Convolutional neural network, Computer science, Machine translation, Arabic, Artificial intelligence, +12 more | https://doi.org/10.1007/978-3-031-07969-6_30 |  |
| https://openalex.org/W4397029886 | A survey on wind power forecasting with machine learning approaches | 2024 | Neural Computing and Applications | article | 26 | yes | Yang Yang, Hao Lou, Jinran Wu, Shaotong Zhang, Shangce Gao | Wind power, Wind power forecasting, Meteorology, Environmental science, Power (physics), Computer science, +8 more | https://doi.org/10.1007/s00521-024-09923-4 | Abstract Wind power forecasting techniques have been well developed over the last half-century. There has been a large number of research literature as well as review analyses. Over the past 5 decades, considerable advancements have been achieved in wind power forecasting. A large body of research literature has been produced, including review articles that have addressed various aspects of the subject. However, these reviews have predominantly utilized horizontal comparisons and have not conducted a comprehensive analysis of the research that has been undertaken. This survey aims to provide a systematic and analytical review of the technical progress made in wind power forecasting. To accomplish this goal, we conducted a knowledge map analysis of the wind power forecasting literature published in the Web of Science database over the last 2 decades. We examined the collaboration network… |
| https://openalex.org/W4311557996 | What do Vision Transformers Learn? A Visual Exploration | 2022 | arXiv (Cornell University) | preprint | 20 | yes | Amin Ghiasi, Hamid Kazemi, Eitan Borgnia, Steven Reich, Manli Shu, Micah Goldblum, Andrew Gordon Wilson, Tom Goldstein | Computer science, Pooling, Convolutional neural network, Artificial intelligence, Architecture, Transformer, +12 more | http://arxiv.org/abs/2212.06727 | Vision transformers (ViTs) are quickly becoming the de-facto architecture for computer vision, yet we understand very little about why they work and what they learn. While existing studies visually analyze the mechanisms of convolutional neural networks, an analogous exploration of ViTs remains challenging. In this paper, we first address the obstacles to performing visualizations on ViTs. Assisted by these solutions, we observe that neurons in ViTs trained with language model supervision (e.g., CLIP) are activated by semantic concepts rather than visual features. We also explore the underlying differences between ViTs and CNNs, and we find that transformers detect image background features, just like their convolutional counterparts, but their predictions depend far less on high-frequency information. On the other hand, both architecture types behave similarly in the way features progr… |
| https://openalex.org/W3101118213 | Findings of the Association for Computational Linguistics: EMNLP 2021 | 2021 |  | paratext | 896 | yes | Firoj Alam, Shaden Shaar, Fahim Dalvi, Hassan Sajjad, Alex Nikolov, Hamdy Mubarak, Giovanni Da, San Martino, +92 more | Computational linguistics, Computer science, Association (psychology), Linguistics, Natural language processing, Artificial intelligence, +2 more | https://doi.org/10.18653/v1/2021.findings-emnlp |  |
| https://openalex.org/W3203734430 | Pseudo-labeling with transformers for improving Question Answering systems | 2021 | Procedia Computer Science | article | 7 | yes | Karolina Kuligowska, Bartłomiej Kowalczuk | Computer science, Question answering, Transformer, Artificial intelligence, Artificial neural network, Labeled data, +8 more | https://doi.org/10.1016/j.procs.2021.08.119 | Advances in neural networks contributed to the fast development of Natural Language Processing systems. As a result, Question Answering systems have evolved and can classify and answer questions in an intuitive yet communicative way. However, the lack of large volumes of labeled data prevents large-scale training and development of Question Answering systems, confirming the need for further research. This paper aims to handle this real-world problem of lack of labeled datasets by applying a pseudo-labeling technique relying on a neural network transformer model DistilBERT. In order to evaluate our contribution, we examined the performance of a text classification transformer model that was fine-tuned on the data subject to prior pseudo-labeling. Research has shown the usefulness of the applied pseudo-labeling technique on a neural network text classification transformer model DistilBERT… |
| https://openalex.org/W4211244284 | Fine-Tuning Transformers For Genomic Tasks | 2022 |  | preprint | 7 | yes | Vlastimil Martinek, David Čechák, Katarína Grešová, Panagiotis Alexiou, Petr Šimeček | Computer science, Transformer, Genomics, Artificial intelligence, DNA sequencing, Artificial neural network, +11 more | https://doi.org/10.1101/2022.02.07.479412 | Abstract Transformers are a type of neural network architecture that has been successfully used to achieve state-of-the-art performance in numerous natural language processing tasks. However, what about DNA, the language life written in the four-letter alphabet? In this paper, we review the current state of Transformers usage in genomics and molecular biology in general, introduce a collection of benchmark datasets for the classification of genomic sequences, and compare the performance of several model architectures on those benchmarks, including a BERT-like model for DNA sequences DNABERT as implemented in HuggingFace (armheb/DNA_bert_6 model). In particular, we explore the effect of pre-training on a large DNA corpus vs training from scratch (with randomized weights). The results presented here can be used for identification of functional elements in human and other genomes. |
| https://openalex.org/W4393113494 | Neural Data Augmentation for Legal Overruling Task: Small Deep Learning Models vs. Large Language Models | 2024 | Neural Processing Letters | article | 10 | yes | Reshma Sheik, K. P. Siva Sundara, S. Jaya Nirmala | Task (project management), Computational intelligence, Computer science, Artificial intelligence, Deep learning, Artificial neural network, +8 more | https://doi.org/10.1007/s11063-024-11574-4 | Abstract Deep learning models produce impressive results in any natural language processing applications when given a better learning strategy and trained with large labeled datasets. However, the annotation of massive training data is far too expensive, especially in the legal domain, due to the need for trained legal professionals. Data augmentation solves the problem of learning without labeled big data. In this paper, we employ pre-trained language models and prompt engineering to generate large-scale pseudo-labeled data for the legal overruling task using 100 data samples. We train small recurrent and convolutional deep-learning models using this data and fine-tune a few other transformer models. We then evaluate the effectiveness of the models, both with and without data augmentation, using the benchmark dataset and analyze the results. We also test the performance of these models… |
| https://openalex.org/W3199888283 | Unsupervised cross-lingual model transfer for named entity recognition with contextualized word representations | 2021 | PLoS ONE | article | 9 | yes | Huijiong Yan, Qian Tao, Liang Xie, Shanguang Chen | Computer science, Natural language processing, Artificial intelligence, Named-entity recognition, Adapter (computing), Transformer, +15 more | https://doi.org/10.1371/journal.pone.0257230 | Named entity recognition (NER) is one fundamental task in the natural language processing (NLP) community. Supervised neural network models based on contextualized word representations can achieve highly-competitive performance, which requires a large-scale manually-annotated corpus for training. While for the resource-scarce languages, the construction of such as corpus is always expensive and time-consuming. Thus, unsupervised cross-lingual transfer is one good solution to address the problem. In this work, we investigate the unsupervised cross-lingual NER with model transfer based on contextualized word representations, which greatly advances the cross-lingual NER performance. We study several model transfer settings of the unsupervised cross-lingual NER, including (1) different types of the pretrained transformer-based language models as input, (2) the exploration strategies of the… |
| https://openalex.org/W3169906730 | From Transformers to Reformers | 2021 |  | article | 8 | no | Nauman Riaz, Seemab Latif, Rabia Latif | Transformer, Computer science, Computation, Artificial intelligence, Artificial neural network, Residual, +5 more | https://doi.org/10.1109/icodt252288.2021.9441516 | This paper investigates different deep learning models for various tasks of Natural Language Processing. Recent ongoing research is about the Transformer models and their variations (like the Reformer model). The Recurrent Neural Networks models were efficient up to an only a fixed size of the window. They were unable to capture long-term dependencies for large sequences. To overcome this limitation, the attention mechanism was introduced which is incorporated in the Transformer model. The dot product attention in transformers has a complexity of O(n <sup xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">2</sup>) where n is the sequence length. This computation becomes infeasible for large sequences. Also, the residual layers consume a lot of memory because activations need to be stored for back-propagation. To overcome this limitation of memory e… |
| https://openalex.org/W4386827564 | Transferable adversarial distribution learning: Query-efficient adversarial attack against large language models | 2023 | Computers & Security | article | 16 | no | Huoyuan Dong, Jialiang Dong, Shaohua Wan, Shuai Yuan, Zhitao Guan | Computer science, Overfitting, Machine learning, Language model, Artificial intelligence, Leverage (statistics), +4 more | https://doi.org/10.1016/j.cose.2023.103482 |  |
| https://openalex.org/W3049366647 | Is Supervised Syntactic Parsing Beneficial for Language Understanding? An Empirical Investigation | 2020 | arXiv (Cornell University) | preprint | 8 | yes | Goran Glavaš, Ivan Vulić | Computer science, Parsing, Natural language processing, Transformer, Artificial intelligence, S-attributed grammar, +5 more | http://arxiv.org/abs/2008.06788 | Traditional NLP has long held (supervised) syntactic parsing necessary for successful higher-level semantic language understanding (LU). The recent advent of end-to-end neural models, self-supervised via language modeling (LM), and their success on a wide range of LU tasks, however, questions this belief. In this work, we empirically investigate the usefulness of supervised parsing for semantic LU in the context of LM-pretrained transformer networks. Relying on the established fine-tuning paradigm, we first couple a pretrained transformer with a biaffine parsing head, aiming to infuse explicit syntactic knowledge from Universal Dependencies treebanks into the transformer. We then fine-tune the model for LU tasks and measure the effect of the intermediate parsing training (IPT) on downstream LU task performance. Results from both monolingual English and zero-shot language transfer experi… |
| https://openalex.org/W4391440418 | Advances in machine learning with chemical language models in molecular property and reaction outcome predictions | 2024 | Journal of Computational Chemistry | article | 16 | yes | Manajit Das, Ankit Ghosh, Raghavan B. Sunoj | Chemical space, Computer science, Artificial intelligence, Outcome (game theory), Machine learning, Property (philosophy), +7 more | https://doi.org/10.1002/jcc.27315 | Abstract Molecular properties and reactions form the foundation of chemical space. Over the years, innumerable molecules have been synthesized, a smaller fraction of them found immediate applications, while a larger proportion served as a testimony to creative and empirical nature of the domain of chemical science. With increasing emphasis on sustainable practices, it is desirable that a target set of molecules are synthesized preferably through a fewer empirical attempts instead of a larger library, to realize an active candidate. In this front, predictive endeavors using machine learning (ML) models built on available data acquire high timely significance. Prediction of molecular property and reaction outcome remain one of the burgeoning applications of ML in chemical science. Among several methods of encoding molecular samples for ML models, the ones that employ language like represe… |
| https://openalex.org/W3157746834 | GraphFormers: GNN-nested Language Models for Linked Text Representation. | 2021 | arXiv (Cornell University) | preprint | 9 | yes | Junhan Yang, Zheng Liu, Shitao Xiao, Chaozhuo Li, Guangzhong Sun, Xing Xie | Computer science, Leverage (statistics), Language model, Neighbourhood (mathematics), Graph, Artificial intelligence, +10 more | https://arxiv.org/abs/2105.02605 | Linked text representation is critical for many intelligent web applications, such as online advertisement and recommender systems. Recent breakthroughs on pretrained language models and graph neural networks facilitate the development of corresponding techniques. However, the existing works mainly rely on cascaded model structures: the texts are independently encoded by language models at first, and the textual embeddings are further aggregated by graph neural networks. We argue that the neighbourhood information is insufficiently utilized within the above process, which restricts the representation quality. In this work, we propose GraphFormers, where graph neural networks are nested alongside each transformer layer of the language models. On top of the above architecture, the linked texts will iteratively extract neighbourhood information for the enhancement of their own semantics. S… |
| https://openalex.org/W4395049422 | Using <scp>GPT</scp>‐4 for <scp>LI‐RADS</scp> feature extraction and categorization with multilingual free‐text reports | 2024 | Liver International | article | 36 | yes | Kyowon Gu, Jeong Hyun Lee, Jaeseung Shin, Jeong Ah Hwang, Ji Hye Min, Woo Kyoung Jeong, Min Woo Lee, Kyoung Doo Song, +1 more | Categorization, Computer science, Extraction (chemistry), Feature (linguistics), Text messaging, Information retrieval, +9 more | https://doi.org/10.1111/liv.15891 | Abstract Background and Aims The Liver Imaging Reporting and Data System (LI‐RADS) offers a standardized approach for imaging hepatocellular carcinoma. However, the diverse styles and structures of radiology reports complicate automatic data extraction. Large language models hold the potential for structured data extraction from free‐text reports. Our objective was to evaluate the performance of Generative Pre‐trained Transformer (GPT)‐4 in extracting LI‐RADS features and categories from free‐text liver magnetic resonance imaging (MRI) reports. Methods Three radiologists generated 160 fictitious free‐text liver MRI reports written in Korean and English, simulating real‐world practice. Of these, 20 were used for prompt engineering, and 140 formed the internal test cohort. Seventy‐two genuine reports, authored by 17 radiologists were collected and de‐identified for the external test cohor… |
| https://openalex.org/W3023176787 | Investigating the Effectiveness of Representations Based on Pretrained Transformer-based Language Models in Active Learning for Labelling Text Datasets | 2020 | arXiv (Cornell University) | preprint | 13 | yes | Jinghui Lu, Brian Mac Namee | Labelling, Transformer, Computer science, Natural language processing, Artificial intelligence, Psychology, +4 more | http://arxiv.org/abs/2004.13138 | Active learning has been shown to be an effective way to alleviate some of the effort required in utilising large collections of unlabelled data for machine learning tasks without needing to fully label them. The representation mechanism used to represent text documents when performing active learning, however, has a significant influence on how effective the process will be. While simple vector representations such as bag-of-words and embedding-based representations based on techniques such as word2vec have been shown to be an effective way to represent documents during active learning, the emergence of representation mechanisms based on the pre-trained transformer-based neural network models popular in natural language processing research (e.g. BERT) offer a promising, and as yet not fully explored, alternative. This paper describes a comprehensive evaluation of the effectiveness of r… |
| https://openalex.org/W4309163757 | Multiscale Modeling at the Interface of Molecular Mechanics and Natural Language through Attention Neural Networks | 2022 | Accounts of Chemical Research | review | 26 | no | Markus J. Buehler | Computer science, Process (computing), Interface (matter), Class (philosophy), Property (philosophy), Artificial neural network, +14 more | https://doi.org/10.1021/acs.accounts.2c00330 | Humans are continually bombarded with massive amounts of data. To deal with this influx of information, we use the concept of attention in order to perceive the most relevant input from vision, hearing, touch, and others. Thereby, the complex ensemble of signals is used to generate output by querying the processed data in appropriate ways. Attention is also the hallmark of the development of scientific theories, where we elucidate which parts of a problem are critical, often expressed through differential equations. In this Account we review the emergence of attention-based neural networks as a class of approaches that offer many opportunities to describe materials across scales and modalities, including how universal building blocks interact to yield a set of material properties. In fact, the self-assembly of hierarchical, structurally complex, and multifunctional biomaterials remains… |
| https://openalex.org/W4390957671 | Implementing Generative AI and Large Language Models in Education | 2023 |  | article | 15 | no | Neil Anderson, Aidan McGowan, Leo Galway, Philip Hanna, Matthew Collins, David Cutting | CLARITY, Computer science, Generative grammar, Artificial intelligence, Process (computing), Generative model, +3 more | https://doi.org/10.1109/isas60782.2023.10391517 | The recent advancements in Generative AI have been highlighted by the emergence of Large Language Models (LLMs) like ChatGPT. We track this evolution from the initial recurrent neural networks to the development of architectures like Transformers and Generative Pre-trained Transformers (GPT). ChatGPT, with its impressive ability to comprehend, process, and produce natural language, has piqued the interest of educators, students, and institutions within the education sector through its creation of high-quality textual responses.This marks the beginning of a new era in educational possibilities: we emphasize the beneficial effects of ChatGPT in learning environments, noting its utility in programming assistance, its clarity in concept explanation, and its role in enhancing automated learning processes. We also recognize potential drawbacks, including the risks of over-reliance, plagiarism… |
| https://openalex.org/W4295951577 | Multimodal biomedical AI | 2022 | Nature Medicine | review | 952 | yes | Julián Acosta, Guido J. Falcone, Pranav Rajpurkar, Eric J. Topol | Computer science, Digital health, Biobank, Data science, Wearable computer, Wearable technology, +13 more | https://doi.org/10.1038/s41591-022-01981-2 | The increasing availability of biomedical data from large biobanks, electronic health records, medical imaging, wearable and ambient biosensors, and the lower cost of genome and microbiome sequencing have set the stage for the development of multimodal artificial intelligence solutions that capture the complexity of human health and disease. In this Review, we outline the key applications enabled, along with the technical and analytical challenges. We explore opportunities in personalized medicine, digital clinical trials, remote monitoring and care, pandemic surveillance, digital twin technology and virtual health assistants. Further, we survey the data, modeling and privacy challenges that must be overcome to realize the full potential of multimodal artificial intelligence in health. |
| https://openalex.org/W4399898969 | Advanced deep learning and large language models for suicide ideation detection on social media | 2024 | Progress in Artificial Intelligence | article | 15 | no | Mohammed Qorich, Rajae El Ouazzani | Deep learning, Autoencoder, Word2vec, Computer science, Artificial intelligence, Word embedding, +14 more | https://doi.org/10.1007/s13748-024-00326-z |  |
| https://openalex.org/W4392271377 | Natural Language Processing Methods for Symbolic Music Generation and Information Retrieval: A Survey | 2025 | ACM Computing Surveys | review | 17 | yes | Dinh-Viet-Toan Le, Louis Bigo, Mikaela Keller, Dorien Herremans | Computer science, Music information retrieval, Artificial intelligence, Natural language processing, Analogy, Variety (cybernetics), +9 more | https://doi.org/10.1145/3714457 | Music is frequently associated with the notion of language, as both domains share several similarities, including the ability for their content to be represented as sequences of symbols. In computer science, the fields of Natural Language Processing (NLP) and Music Information Retrieval (MIR) reflect this analogy through a variety of similar tasks, such as author detection or content generation. This similarity has long encouraged the adaptation of NLP methods to process musical data, particularly symbolic music data, and the rise of Transformer neural networks has considerably strengthened this practice. This survey reviews NLP methods applied to symbolic music generation and information retrieval following two axes. We first propose an overview of representations of symbolic music inspired by text sequential representations. We then review a large set of computational models, particul… |
| https://openalex.org/W3212516020 | Palette: Image-to-Image Diffusion Models | 2022 |  | preprint | 1377 | yes | Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David J. Fleet, Mohammad Norouzi | Computer science, Inpainting, Artificial intelligence, Image (mathematics), Task (project management), Image translation, +21 more | https://doi.org/10.1145/3528233.3530757 | This paper develops a unified framework for image-to-image translation based on conditional diffusion models and evaluates this framework on four challenging image-to-image translation tasks, namely colorization, inpainting, uncropping, and JPEG restoration. Our simple implementation of image-to-image diffusion models outperforms strong GAN and regression baselines on all tasks, without task-specific hyper-parameter tuning, architecture customization, or any auxiliary loss or sophisticated new techniques needed. We uncover the impact of an L2 vs. L1 loss in the denoising diffusion objective on sample diversity, and demonstrate the importance of self-attention in the neural architecture through empirical studies. Importantly, we advocate a unified evaluation protocol based on ImageNet, with human evaluation and sample quality scores (FID, Inception Score, Classification Accuracy of a pre… |
| https://openalex.org/W4385346076 | Visual attention network | 2023 | Computational Visual Media | article | 867 | yes | Meng-Hao Guo, Cheng-Ze Lu, Zheng-Ning Liu, Ming‐Ming Cheng, Shi‐Min Hu | Computer science, Artificial intelligence, Segmentation, Convolutional neural network, Object detection, Benchmark (surveying), +5 more | https://doi.org/10.1007/s41095-023-0364-2 | Abstract While originally designed for natural language processing tasks, the self-attention mechanism has recently taken various computer vision areas by storm. However, the 2D nature of images brings three challenges for applying self-attention in computer vision: (1) treating images as 1D sequences neglects their 2D structures; (2) the quadratic complexity is too expensive for high-resolution images; (3) it only captures spatial adaptability but ignores channel adaptability. In this paper, we propose a novel linear attention named large kernel attention (LKA) to enable self-adaptive and long-range correlations in self-attention while avoiding its shortcomings. Furthermore, we present a neural network based on LKA, namely Visual Attention Network (VAN). While extremely simple, VAN achieves comparable results with similar size convolutional neural networks (CNNs) and vision transformer… |
| https://openalex.org/W3023958515 | TextAT: Adversarial Training for Natural Language Understanding with Token-Level Perturbation | 2020 | arXiv (Cornell University) | article | 12 | yes | Linyang Li, Xipeng Qiu | Adversarial system, Security token, Computer science, Fist, Artificial intelligence, Transformer, +14 more | https://arxiv.org/abs/2004.14543 | Adversarial training is effective in improving the robustness of neural networks. In NLP, languages are discrete in nature, separate tokens possess discrete semantics. Therefore, to incorporate adversarial training in sequence-level tasks, we introduce a novel training strategy: Text Adversarial Training with token-level perturbation. We fist craft perturbations that are initialized using a fine-grained token-level accumulated perturbations. Then we constrain these perturbations considering that inputs are separate tokens, rather than constraining them under a naive normalization ball. We validate the effectiveness of such normalization method using large-scale Transformer-based language models. Experiments on GLUE benchmark and NER task show that our adversarial training strategy improves the performances on various tasks including text classification and sequence labeling. |
| https://openalex.org/W4317569480 | Combining protein sequences and structures with transformers and equivariant graph neural networks to predict protein function | 2023 |  | preprint | 10 | yes | Frimpong Boadu, Hongyuan Cao, Jianlin Cheng | Equivariant map, Transformer, Artificial neural network, Computer science, Graph, Artificial intelligence, +8 more | https://doi.org/10.1101/2023.01.17.524477 | Abstract Motivation Millions of protein sequences have been generated by numerous genome and transcriptome sequencing projects. However, experimentally determining the function of the proteins is still a time consuming, low-throughput, and expensive process, leading to a large protein sequence-function gap. Therefore, it is important to develop computational methods to accurately predict protein function to fill the gap. Even though many methods have been developed to use protein sequences as input to predict function, much fewer methods leverage protein structures in protein function prediction because there was lack of accurate protein structures for most proteins until recently. Results We developed TransFun - a method using a transformer-based protein language model and 3D-equivariant graph neural networks to distill information from both protein sequences and structures to predict… |
| https://openalex.org/W4234577219 | Identifying Patients With Delirium Based on Unstructured Clinical Notes: Observational Study | 2022 | JMIR Formative Research | article | 19 | yes | Wendong Ge, Haitham Alabsi, Aayushee Jain, Elissa Ye, Haoqi Sun, Marta Fernandes, Colin Magdamo, Ryan A. Tesh, +10 more | Delirium, Receiver operating characteristic, Medicine, Artificial intelligence, Machine learning, Confusion, +16 more | https://doi.org/10.2196/33834 | Background Delirium in hospitalized patients is a syndrome of acute brain dysfunction. Diagnostic (International Classification of Diseases [ICD]) codes are often used in studies using electronic health records (EHRs), but they are inaccurate. Objective We sought to develop a more accurate method using natural language processing (NLP) to detect delirium episodes on the basis of unstructured clinical notes. Methods We collected 1.5 million notes from &gt;10,000 patients from among 9 hospitals. Seven experts iteratively labeled 200,471 sentences. Using these, we trained three NLP classifiers: Support Vector Machine, Recurrent Neural Networks, and Transformer. Testing was performed using an external data set. We also evaluated associations with delirium billing (ICD) codes, medications, orders for restraints and sitters, direct assessments (Confusion Assessment Method [CAM] scores), and i… |
| https://openalex.org/W4390822968 | Transformers and LLMs as the New Benchmark in Early Cancer Detection | 2024 | ITM Web of Conferences | article | 7 | yes | Yulia Kumar, Kuan Huang, Zachary Gordon, Lais Castro, Egan Okumu, Patricia Morreale, J. Jenny Li | Transformer, Convolutional neural network, Computer science, Segmentation, Health care, Transformative learning, +14 more | https://doi.org/10.1051/itmconf/20246000004 | The study explores the transformative capabilities of Transformers and Large Language Models (LLMs) in the early detection of Acute Lymphoblastic Leukaemia (ALL). The researchers benchmark Vision Transformers with Deformable Attention (DAT) and Hierarchical Vision Transformers (Swin) against established Convolutional Neural Networks (CNNs) like ResNet-50 and VGG-16. The findings reveal that transformer models exhibit remarkable accuracy in identifying ALL from original images, demonstrating efficiency in image analysis without necessitating labour-intensive segmentation. A thorough bias analysis is conducted to ensure the robustness and fairness of the models. The promising performance of the transformer models indicates a trajectory towards surpassing CNNs in cancer detection, setting new standards for accuracy. In addition, the study explores the capabilities of LLMs in revolutionisin… |
| https://openalex.org/W4367627286 | ACTNet: A Dual-Attention Adapter with a CNN-Transformer Network for the Semantic Segmentation of Remote Sensing Imagery | 2023 | Remote Sensing | article | 11 | yes | Zheng Zhang, Fan-Chen Liu, Changan Liu, Qing Tian, Hongquan Qu | Computer science, Segmentation, Artificial intelligence, Convolutional neural network, Transformer, Adapter (computing), +8 more | https://doi.org/10.3390/rs15092363 | In recent years, the application of semantic segmentation methods based on the remote sensing of images has become increasingly prevalent across a diverse range of domains, including but not limited to forest detection, water body detection, urban rail transportation planning, and building extraction. With the incorporation of the Transformer model into computer vision, the efficacy and accuracy of these algorithms have been significantly enhanced. Nevertheless, the Transformer model’s high computational complexity and dependence on a pre-training weight of large datasets leads to a slow convergence during the training for remote sensing segmentation tasks. Motivated by the success of the adapter module in the field of natural language processing, this paper presents a novel adapter module (ResAttn) for improving the model training speed for remote sensing segmentation. The ResAttn adop… |
| https://openalex.org/W3047855151 | Earthquake transformer—an attentive deep-learning model for simultaneous earthquake detection and phase picking | 2020 | Nature Communications | article | 969 | yes | S. Mostafa Mousavi, William L. Ellsworth, Weiqiang Zhu, Lindsay Chuang, Gregory C. Beroza | Computer science, Seismology, Earthquake simulation, Waveform, Earthquake prediction, Deep learning, +11 more | https://doi.org/10.1038/s41467-020-17591-w |  |
| https://openalex.org/W4382536189 | HBert | 2023 | International Journal on Semantic Web and Information Systems | article | 8 | yes | Xueqiang Lv, Zhaonan Liu, Ying Zhao, Ge Xu, Xindong You | Computer science, Sentence, Transformer, Encoder, Artificial intelligence, Natural language processing, +10 more | https://doi.org/10.4018/ijswis.322769 | With the emergence of a large-scale pre-training model based on the transformer model, the effect of all-natural language processing tasks has been pushed to a new level. However, due to the high complexity of the transformer's self-attention mechanism, these models have poor processing ability for long text. Aiming at solving this problem, a long text processing method named HBert based on Bert and hierarchical attention neural network is proposed. Firstly, the long text is divided into multiple sentences whose vectors are obtained through the word encoder composed of Bert and the word attention layer. And the article vector is obtained through the sentence encoder that is composed of transformer and sentence attention. Then the article vector is used to complete the subsequent tasks. The experimental results show that the proposed HBert method achieves good results in text classificat… |
| https://openalex.org/W3087432510 | ALBERT-based fine-tuning model for cyberbullying analysis | 2020 | Multimedia Systems | article | 27 | no | Jatin Karthik Tripathy, S. Sibi Chakkaravarthy, Suresh Chandra Satapathy, Madhulika Sahoo, V. Vaidehi | Computer science, Recurrent neural network, Language model, Artificial intelligence, Architecture, Flexibility (engineering), +16 more | https://doi.org/10.1007/s00530-020-00690-5 |  |
| https://openalex.org/W4386931911 | Application of Entity-BERT model based on neuroscience and brain-like cognition in electronic medical record entity recognition | 2023 | Frontiers in Neuroscience | article | 15 | yes | Weijia Lu, Jiehui Jiang, Yaxiang Shi, Xiaowei Zhong, Jun Gu, Lixia Huangfu, Ming Gong | Computer science, Artificial intelligence, Named-entity recognition, Information extraction, Leverage (statistics), Language model, +7 more | https://doi.org/10.3389/fnins.2023.1259652 | Introduction In the medical field, electronic medical records contain a large amount of textual information, and the unstructured nature of this information makes data extraction and analysis challenging. Therefore, automatic extraction of entity information from electronic medical records has become a significant issue in the healthcare domain. Methods To address this problem, this paper proposes a deep learning-based entity information extraction model called Entity-BERT. The model aims to leverage the powerful feature extraction capabilities of deep learning and the pre-training language representation learning of BERT(Bidirectional Encoder Representations from Transformers), enabling it to automatically learn and recognize various entity types in medical electronic records, including medical terminologies, disease names, drug information, and more, providing more effective support f… |
| https://openalex.org/W4385876602 | Two sequence- and two structure-based ML models have learned different aspects of protein biochemistry | 2023 | Scientific Reports | article | 12 | yes | Anastasiya V. Kulikova, Daniel J. Diaz, Tianlong Chen, T. Jeffrey Cole, Andrew D. Ellington, Claus O. Wilke | Protein structure prediction, Leverage (statistics), Convolutional neural network, Computer science, Artificial intelligence, Machine learning, +6 more | https://doi.org/10.1038/s41598-023-40247-w |  |
| https://openalex.org/W4293863334 | Comparison of Transformer-Based Models Trained in Turkish and Different Languages on Turkish Natural Language Processing Problems | 2022 | 2022 30th Signal Processing and Communications Applications Conference (SIU) | article | 9 | no | Burak Aytan, C. Okan Sakar | Turkish, Computer science, Transformer, Natural language processing, Artificial intelligence, Language model, +8 more | https://doi.org/10.1109/siu55565.2022.9864818 | Transformer-based pre-trained language models have yielded successful results in natural language processing (NLP) problems in recent years. In these approaches, the models are trained in an unsupervised manner using a large corpus, with the use of mechanisms such as masking and next sentence prediction. In sub-NLP problems, these models are fully or partially updated with a fine-tuning approach, or the vectors obtained from these models are directly mapped to the output with neural network layers added. In this study, transformer-based BERT, RoBERTa, ConvBERT and Electra models, which have given successful results for different languages and problems in the literature, have been applied to Turkish sentiment analysis, text classification and named entity recognition problems, and the results are presented comparatively. One of the contributions of the study is to train the RoBERTa model… |
| https://openalex.org/W4311252752 | Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints | 2022 | arXiv (Cornell University) | preprint | 12 | yes | Aran Komatsuzaki, Joan Puigcerver, James Lee-Thorp, Carlos Riquelme Ruiz, Basil Mustafa, Joshua Ainslie, Yi Tay, Mostafa Dehghani, +1 more | Computer science, Initialization, Computation, Scratch, Reuse, Artificial intelligence, +18 more | http://arxiv.org/abs/2212.05055 | Training large, deep neural networks to convergence can be prohibitively expensive. As a result, often only a small selection of popular, dense models are reused across different contexts and tasks. Increasingly, sparsely activated models, which seek to decouple model size from computation costs, are becoming an attractive alternative to dense models. Although more efficient in terms of quality and computation cost, sparse models remain data-hungry and costly to train from scratch in the large scale regime. In this work, we propose sparse upcycling -- a simple way to reuse sunk training costs by initializing a sparsely activated Mixture-of-Experts model from a dense checkpoint. We show that sparsely upcycled T5 Base, Large, and XL language models and Vision Transformer Base and Large models, respectively, significantly outperform their dense counterparts on SuperGLUE and ImageNet, using… |
| https://openalex.org/W4403635980 | Prompt Tuning of Deep Neural Networks for Speaker-Adaptive Visual Speech Recognition | 2024 | IEEE Transactions on Pattern Analysis and Machine Intelligence | article | 16 | no | Minsu Kim, Hyung-Il Kim, Yong Man Ro | Speech recognition, Computer science, Speaker recognition, Artificial neural network, Deep neural networks, Artificial intelligence, +2 more | https://doi.org/10.1109/tpami.2024.3484658 | Visual Speech Recognition (VSR) aims to infer speech into text depending on lip movements alone. As it focuses on visual information to model the speech, its performance is inherently sensitive to personal lip appearances and movements, and this makes the VSR models show degraded performance when they are applied to unseen speakers. In this paper, to remedy the performance degradation of the VSR model on unseen speakers, we propose prompt tuning methods of Deep Neural Networks (DNNs) for speaker-adaptive VSR. Specifically, motivated by recent advances in Natural Language Processing (NLP), we finetune prompts on adaptation data of target speakers instead of modifying the pre-trained model parameters. Different from the previous prompt tuning methods mainly limited to Transformer variant architecture, we explore different types of prompts, the addition, the padding, and the concatenation… |
| https://openalex.org/W2997591391 | Unified Vision-Language Pre-Training for Image Captioning and VQA | 2020 | Proceedings of the AAAI Conference on Artificial Intelligence | article | 818 | yes | Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason J. Corso, Jianfeng Gao | Closed captioning, Computer science, Transformer, Question answering, Language model, Decoding methods, +17 more | https://doi.org/10.1609/aaai.v34i07.7005 | This paper presents a unified Vision-Language Pre-training (VLP) model. The model is unified in that (1) it can be fine-tuned for either vision-language generation (e.g., image captioning) or understanding (e.g., visual question answering) tasks, and (2) it uses a shared multi-layer transformer network for both encoding and decoding, which differs from many existing methods where the encoder and decoder are implemented using separate models. The unified VLP model is pre-trained on a large amount of image-text pairs using the unsupervised learning objectives of two tasks: bidirectional and sequence-to-sequence (seq2seq) masked vision-language prediction. The two tasks differ solely in what context the prediction conditions on. This is controlled by utilizing specific self-attention masks for the shared transformer network. To the best of our knowledge, VLP is the first reported model tha… |
| https://openalex.org/W4404116679 | Artificial intelligence and machine learning at various stages and scales of process systems engineering | 2024 | The Canadian Journal of Chemical Engineering | article | 19 | no | Karthik K. Srinivasan, Anjana Puliyanda, Devavrat Thosar, Abhijit Bhakte, Kuldeep Singh, Prince Addo, Rajagopalan Srinivasan, Vinay Prasad | Artificial intelligence, Generative grammar, Computer science, Machine learning, Process (computing), Relevance (law), +5 more | https://doi.org/10.1002/cjce.25525 | Abstract We review the utility and application of artificial intelligence (AI) and machine learning (ML) at various process scales in this work, from molecules and reactions to materials to processes, plants, and supply chains; furthermore, we highlight whether the application is at the design or operational stage of the process. In particular, we focus on the distinct representational frameworks employed at the various scales and the physics (equivariance, additivity, injectivity, connectivity, hierarchy, and heterogeneity) they capture. We also review AI techniques and frameworks important in process systems, including hybrid AI modelling, human‐AI collaborations, and generative AI techniques. In hybrid AI models, we emphasize the importance of hyperparameter tuning, especially in the case of physics‐informed regularization. We highlight the importance of studying human‐AI interaction… |
| https://openalex.org/W3111934536 | Deep Learning Algorithm for Judicial Judgment Prediction Based on BERT | 2020 | 2020 5th International Conference on Computing, Communication and Security (ICC… | article | 13 | no | Yongjun Wang, Jing Gao, Junjie Chen | Deep learning, Computer science, Artificial intelligence, Convolutional neural network, Machine learning, Field (mathematics), +8 more | https://doi.org/10.1109/icccs49678.2020.9277068 | With the continuous development of artificial intelligence AI, machine learning and deep learning algorithms have been applied to more fields and scenarios, changing people's work and lifestyle, which also brought new development and opportunities to the judicial field. Judicial case judgment is a very important work in the legal field, and it is an indispensable process for judges to determine the nature and punishment of criminals. Judgment prediction can be seen as a multi-label text classification problem. Based on a large number of text files of judicial cases, we use the recently very popular pre-training language model Bidirectional Encoder Representations from Transformers (BERT) to train word embedding of the case data, and combines deep learning model algorithms such as Convolutional Neural Networks (CNN), Long Short-Term Memory (LSTM), Deep Pyramid CNN (DPCNN), Recurrent Conv… |
| https://openalex.org/W4205773061 | ProteinBERT: a universal deep-learning model of protein sequence and function | 2022 | Bioinformatics | article | 871 | yes | Nadav Brandes, Dan Ofer, Yam Peleg, Nadav Rappoport, Michal Linial | Computer science, Artificial intelligence, Deep learning, Annotation, Task (project management), Function (biology), +12 more | https://doi.org/10.1093/bioinformatics/btac020 | Abstract Summary Self-supervised deep language modeling has shown unprecedented success across natural language tasks, and has recently been repurposed to biological sequences. However, existing models and pretraining methods are designed and optimized for text analysis. We introduce ProteinBERT, a deep language model specifically designed for proteins. Our pretraining scheme combines language modeling with a novel task of Gene Ontology (GO) annotation prediction. We introduce novel architectural elements that make the model highly efficient and flexible to long sequences. The architecture of ProteinBERT consists of both local and global representations, allowing end-to-end processing of these types of inputs and outputs. ProteinBERT obtains near state-of-the-art performance, and sometimes exceeds it, on multiple benchmarks covering diverse protein properties (including protein structur… |
| https://openalex.org/W4313056976 | Comparison of Deep Learning Approaches for Lithuanian Sentiment Analysis | 2022 | Baltic Journal of Modern Computing | article | 5 | yes | Jurgita Kapočiūtė-Dzikienė, Askars Salimbajevs | Lithuanian, Sentiment analysis, Artificial intelligence, Natural language processing, Computer science, Deep learning, +2 more | https://doi.org/10.22364/bjmc.2022.10.3.02 | Sentiment analysis is one of the oldest Natural Language Processing problems, still relevant and challenging today.It is usually formulated and solved as a supervised machine learning problem.In this research, we are solving the three-class sentiment analysis problem for the non-normative Lithuanian language.The contribution of our research is related to applying the innovative BERT-based multilingual sentence transformer models to the Lithuanian sentiment analysis problem.For comparison purposes, we have also investigated traditional Deep Learning approaches, such as fastText or BERT word embeddings with the Convolutional Neural Network as the classifier.The best accuracy ∼0.788 was achieved with the purely monolingual model, i.e., fastText (trained on the very large and diverse Lithuanian corpus) and the Convolutional Neural Network (refined in various text classification tasks).The b… |
| https://openalex.org/W3135302781 | ReportAGE: Automatically extracting the exact age of Twitter users based on self-reports in tweets | 2022 | PLoS ONE | article | 20 | yes | Ari Z Klein, Arjun Magge, Graciela Gonzalez‐Hernandez | Computer science, Classifier (UML), Artificial intelligence, Social media, Recall, F1 score, +10 more | https://doi.org/10.1371/journal.pone.0262087 | Advancing the utility of social media data for research applications requires methods for automatically detecting demographic information about social media study populations, including users’ age. The objective of this study was to develop and evaluate a method that automatically identifies the exact age of users based on self-reports in their tweets. Our end-to-end automatic natural language processing (NLP) pipeline, ReportAGE, includes query patterns to retrieve tweets that potentially mention an age, a classifier to distinguish retrieved tweets that self-report the user’s exact age (“age” tweets) and those that do not (“no age” tweets), and rule-based extraction to identify the age. To develop and evaluate ReportAGE, we manually annotated 11,000 tweets that matched the query patterns. Based on 1000 tweets that were annotated by all five annotators, inter-annotator agreement (Fleiss… |
| https://openalex.org/W4312220150 | A large language model for electronic health records | 2022 | npj Digital Medicine | article | 746 | yes | Xi Yang, Aokun Chen, Nima PourNejatian, Hoo Chang Shin, Kaleb E Smith, Christopher Parisien, Colin B. Compas, Cheryl Martin, +11 more | Computer science, Artificial intelligence, Natural language processing, Relationship extraction, Domain (mathematical analysis), Language model, +7 more | https://doi.org/10.1038/s41746-022-00742-2 |  |
| https://openalex.org/W4312718069 | Training Object Detectors from Scratch: An Empirical Study in the Era of Vision Transformer | 2022 | 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) | article | 13 | no | Weixiang Hong, Jiangwei Lao, Ren Wang, Jian Wang, Jingdong Chen, Wei-Ta Chu | Transformer, Computer science, Detector, Artificial intelligence, Scratch, Object detection, +8 more | https://doi.org/10.1109/cvpr52688.2022.00462 | Modeling in computer vision has long been dominated by convolutional neural networks (CNNs). Recently, in light of the excellent performances of self-attention mech-anism in the language field, transformers tailored for visual data have drawn numerous attention and triumphed CNNs in various vision tasks. These vision transformers heavily rely on large-scale pre-training to achieve competitive accuracy, which not only hinders the freedom of architectural design in downstream tasks like object detection, but also causes learning bias and domain mismatch in the fine-tuning stages. To this end, we aim to get rid of the "pre-train & fine-tune" paradigm of vision transformer and train transformer based object detector from scratch. Some earlier work in the CNNs era have successfully trained CNNs based detectors without pre-training, unfortunately, their findings do not generalize well when th… |
| https://openalex.org/W2950339735 | COMET: Commonsense Transformers for Automatic Knowledge Graph Construction | 2019 |  | article | 827 | yes | Antoine Bosselut, Hannah Rashkin, Maarten Sap, Chaitanya Malaviya, Aslı Çelikyılmaz, Yejin Choi | Commonsense knowledge, Computer science, Commonsense reasoning, Transformer, Knowledge graph, Knowledge base, +8 more | https://doi.org/10.18653/v1/p19-1470 | We present the first comprehensive study on automatic knowledge base construction for two prevalent commonsense knowledge graphs: ATOMIC (Sap et al., 2019) and ConceptNet (Speer et al., 2017). Contrary to many conventional KBs that store knowledge with canonical templates, commonsense KBs only store loosely structured open-text descriptions of knowledge. We posit that an important step toward automatic commonsense completion is the development of generative models of commonsense knowledge, and propose COMmonsEnse Transformers (COMET) that learn to generate rich and diverse commonsense descriptions in natural language. Despite the challenges of commonsense modeling, our investigation reveals promising results when implicit knowledge from deep pre-trained language models is transferred to generate explicit knowledge in commonsense knowledge graphs. Empirical results demonstrate that COMET… |
| https://openalex.org/W3035091181 | Teaching Pre-Trained Models to Systematically Reason Over Implicit Knowledge. | 2020 | arXiv (Cornell University) | preprint | 13 | yes | Alon Talmor, Oyvind Tafjord, Peter E. Clark, Yoav Goldberg, Jonathan Berant | Computer science, Chaining, Inference, Backward chaining, Forward chaining, Artificial intelligence, +14 more | https://arxiv.org/abs/2006.06609v1 | To what extent can a neural network systematically reason over symbolic facts? Evidence suggests that large pre-trained language models (LMs) acquire some reasoning capacity, but this ability is difficult to control. Recently, it has been shown that Transformer-based models succeed in consistent reasoning over explicit symbolic facts, under a closed-world assumption. However, in an open-domain setup, it is desirable to tap into the vast reservoir of implicit knowledge already encoded in the parameters of pre-trained LMs. In this work, we provide a first demonstration that LMs can be trained to reliably perform systematic reasoning combining both implicit, pre-trained knowledge and explicit natural language statements. To do this, we describe a procedure for automatically generating datasets that teach a model new reasoning skills, and demonstrate that models learn to effectively perform… |
| https://openalex.org/W4285493940 | Medical Text Prediction and Suggestion Using Generative Pretrained Transformer Models with Dental Medical Notes | 2022 | Methods of Information in Medicine | article | 20 | no | Joseph Sirrianni, Emre Sezgın, Daniel Claman, Simon Lin Linwood | Security token, Computer science, Transformer, Artificial intelligence, Punctuation, Natural language processing, +8 more | https://doi.org/10.1055/a-1900-7351 | Abstract Background Generative pretrained transformer (GPT) models are one of the latest large pretrained natural language processing models that enables model training with limited datasets and reduces dependency on large datasets, which are scarce and costly to establish and maintain. There is a rising interest to explore the use of GPT models in health care. Objective We investigate the performance of GPT-2 and GPT-Neo models for medical text prediction using 374,787 free-text dental notes. Methods We fine-tune pretrained GPT-2 and GPT-Neo models for next word prediction on a dataset of over 374,000 manually written sections of dental clinical notes. Each model was trained on 80% of the dataset, validated on 10%, and tested on the remaining 10%. We report model performance in terms of next word prediction accuracy and loss. Additionally, we analyze the performance of the models on di… |
| https://openalex.org/W4402548373 | Advanced computational methods for news classification: A study in neural networks and CNN integrated with GPT | 2024 | Journal of Economy and Technology | article | 11 | yes | Fahim Sufi | Computer science, Artificial neural network, Convolutional neural network, Artificial intelligence, Pattern recognition (psychology) | https://doi.org/10.1016/j.ject.2024.09.001 | In an era inundated with vast amounts of information, the imperative for efficient news classification is paramount. This research explores the sophisticated integration of neural networks and convolutional neural networks (CNN) with Generative Pre-trained Transformers (GPT) to enhance the precision and efficacy of news categorization. The rapid digital dissemination of news necessitates advanced computational methodologies capable of accurate classification and event prediction that include finance and economic events. Leveraging recent advancements in machine learning and natural language processing (NLP), this study utilizes large language models (LLMs) such as GPT and BERT, known for their exceptional comprehension and generation of human-like text. Over 232 days, our methodology classified 33,979 news articles into Education &amp; Learning, Health &amp; Medicine, and Science &amp;… |
| https://openalex.org/W4401587020 | Integrated Method of Deep learning and Large Language Model in Speech Recognition | 2024 | Preprints.org | preprint | 6 | yes | Bo Guan, Jin Cao, Bingjie Huang, Zhuoyue Wang, Xingqi Wang, Zixiang Wang | Computer science, Speech recognition, Language model, Word error rate, Cache language model, Artificial intelligence, +20 more | https://doi.org/10.20944/preprints202407.1520.v2 | This research aims to explore the integration method of deep learning and large language models in speech recognition to improve the system&amp;rsquo;s recognition accuracy and ability to handle complex contexts. Deep neural network (DNN), convolutional neural network (CNN), long short-term memory network (LSTM) and Transformer-based large language model are used to build an integrated acoustic and language model framework. Experiments on TIMIT, LibriSpeech and Common Voice datasets show that the ensemble model shows significant improvements in both word error rate (WER) and real-time factor (RTF) compared to traditional models. Especially in terms of adaptability to multiple languages and accent changes, the model shows superior performance. The conclusion shows that through technology integration, the performance of the speech recognition system in complex environments can be effectiv… |
| https://openalex.org/W4376226279 | Multimodal Learning With Transformers: A Survey | 2023 | IEEE Transactions on Pattern Analysis and Machine Intelligence | article | 735 | yes | Peng Xu, Xiatian Zhu, David A. Clifton | Transformer, Computer science, Multimodal learning, Multimodal therapy, Artificial intelligence, Machine learning, +5 more | https://doi.org/10.1109/tpami.2023.3275156 | Transformer is a promising neural network learner, and has achieved great success in various machine learning tasks. Thanks to the recent prevalence of multimodal applications and Big Data, Transformer-based multimodal learning has become a hot topic in AI research. This paper presents a comprehensive survey of Transformer techniques oriented at multimodal data. The main contents of this survey include: (1) a background of multimodal learning, Transformer ecosystem, and the multimodal Big Data era, (2) a systematic review of Vanilla Transformer, Vision Transformer, and multimodal Transformers, from a geometrically topological perspective, (3) a review of multimodal Transformer applications, via two important paradigms, i.e., for multimodal pretraining and for specific multimodal tasks, (4) a summary of the common challenges and designs shared by the multimodal Transformer models and app… |
| https://openalex.org/W4386142022 | Interpreting Black-Box Models: A Review on Explainable Artificial Intelligence | 2023 | Cognitive Computation | review | 1329 | yes | Vikas Hassija, Vinay Chamola, A. Mahapatra, Abhinandan Singal, Divyansh Goel, Kaizhu Huang, Simone Scardapane, Indro Spinelli, +2 more | Transparency (behavior), Computer science, Black box, Process (computing), Predictability, Artificial intelligence, +9 more | https://doi.org/10.1007/s12559-023-10179-8 |  |
| https://openalex.org/W3160137267 | Med-BERT: pretrained contextualized embeddings on large-scale structured electronic health records for disease prediction | 2021 | npj Digital Medicine | article | 749 | yes | Laila Rasmy, Yang Xiang, Ziqian Xie, Cui Tao, Degui Zhi | Computer science, Machine learning, Artificial intelligence, Encoder, Pace, Training set, +17 more | https://doi.org/10.1038/s41746-021-00455-y |  |
| https://openalex.org/W3196337540 | Graphine: A Dataset for Graph-aware Terminology Definition Generation | 2021 | Proceedings of the 2021 Conference on Empirical Methods in Natural Language Pro… | article | 7 | yes | Zequn Liu, Shukai Wang, Yiyang Gu, Ruiyi Zhang, Ming Zhang, Sheng Wang | Terminology, Computer science, Text generation, Artificial intelligence, Graph, Sentence, +8 more | https://doi.org/10.18653/v1/2021.emnlp-main.278 | Precisely defining the terminology is the first step in scientific communication. Developing neural text generation models for definition generation can circumvent the labor-intensity curation, further accelerating scientific discovery. Unfortunately, the lack of large-scale terminology definition dataset hinders the process toward definition generation. In this paper, we present a large-scale terminology definition dataset Graphine covering 2,010,648 terminology definition pairs, spanning 227 biomedical subdisciplines. Terminologies in each subdiscipline further form a directed acyclic graph, opening up new avenues for developing graph-aware text generation models. We then proposed a novel graph-aware definition generation model Graphex that integrates transformer with graph neural network. Our model outperforms existing text generation models by exploiting the graph structure of termi… |
| https://openalex.org/W3007385124 | Using a thousand optimization tasks to learn hyperparameter search strategies | 2020 | arXiv (Cornell University) | preprint | 14 | yes | Luke Metz, Niru Maheswaranathan, Ruoxi Sun, C. Daniel Freeman, Ben Poole, Jascha Sohl‐Dickstein | Hyperparameter, Computer science, Hyperparameter optimization, Machine learning, Artificial intelligence, Convolutional neural network, +12 more | http://arxiv.org/abs/2002.11887 | We present TaskSet, a dataset of tasks for use in training and evaluating optimizers. TaskSet is unique in its size and diversity, containing over a thousand tasks ranging from image classification with fully connected or convolutional neural networks, to variational autoencoders, to non-volume preserving flows on a variety of datasets. As an example application of such a dataset we explore meta-learning an ordered list of hyperparameters to try sequentially. By learning this hyperparameter list from data generated using TaskSet we achieve large speedups in sample efficiency over random search. Next we use the diversity of the TaskSet and our method for learning hyperparameter lists to empirically explore the generalization of these lists to new optimization tasks in a variety of settings including ImageNet classification with Resnet50 and LM1B language modeling with transformers. As pa… |
| https://openalex.org/W3016932024 | Transform and Tell: Entity-Aware News Image Captioning | 2020 | arXiv (Cornell University) | preprint | 8 | yes | Alasdair Tran, A. P. Mathews, Lexing Xie | Closed captioning, Computer science, Transformer, Artificial intelligence, Word (group theory), Language model, +18 more | http://arxiv.org/abs/2004.08070 | We propose an end-to-end model which generates captions for images embedded in news articles. News images present two key challenges: they rely on real-world knowledge, especially about named entities; and they typically have linguistically rich captions that include uncommon words. We address the first challenge by associating words in the caption with faces and objects in the image, via a multi-modal, multi-head attention mechanism. We tackle the second challenge with a state-of-the-art transformer language model that uses byte-pair-encoding to generate captions as a sequence of word parts. On the GoodNews dataset, our model outperforms the previous state of the art by a factor of four in CIDEr score (13 to 54). This performance gain comes from a unique combination of language models, word representation, image embeddings, face embeddings, object embeddings, and improvements in neural… |
| https://openalex.org/W4392452724 | Large-Scale Pretraining Improves Sample Efficiency of Active Learning-Based Virtual Screening | 2024 | Journal of Chemical Information and Modeling | article | 15 | no | Zhonglin Cao, Simone Sciabola, Ye Wang | Virtual screening, Computer science, Artificial intelligence, Machine learning, Drug discovery, Bayesian probability, +18 more | https://doi.org/10.1021/acs.jcim.3c01938 | Virtual screening of large compound libraries to identify potential hit candidates is one of the earliest steps in drug discovery. As the size of commercially available compound collections grows exponentially to the scale of billions, active learning and Bayesian optimization have recently been proven as effective methods of narrowing down the search space. An essential component of those methods is a surrogate machine learning model that predicts the desired properties of compounds. An accurate model can achieve high sample efficiency by finding hits with only a fraction of the entire library being virtually screened. In this study, we examined the performance of a pretrained transformer-based language model and graph neural network in a Bayesian optimization active learning framework. The best pretrained model identifies 58.97% of the top-50,000 compounds after screening only 0.6% of… |
| https://openalex.org/W3175316843 | Customized Impression Prediction From Radiology Reports Using BERT and LSTMs | 2021 | IEEE Transactions on Artificial Intelligence | article | 19 | no | Batuhan Gündoğdu, Utku Pamuksuz, Jonathan H. Chung, Jessica M. Telleria, Peng Liu, Farrukh Aslam Khan, Paul J. Chang | Workflow, Deep learning, Computer science, Artificial intelligence, Encoder, Radiology, +6 more | https://doi.org/10.1109/tai.2021.3086435 | Clinical language processing has become an attractive field with the improvements of deep learning applications and the abundance of large unstructured narratives in the healthcare records. The capability to extract unstructured information from raw text to provide actionable information for healthcare personnel plays a vital role in healthcare workflows. In this article, we introduce a deep learning approach to automate the generation of radiology impressions by analyzing radiology findings and patient background information of each examination. Since the impression section of a radiology report is an essential conclusion, any errors can prove to be detrimental. Thus, we developed a deep learning system to prevent important clinical findings from being overlooked by using almost 1 million de-identified radiology reports obtained from the University of Chicago Medicine over the last 12… |
| https://openalex.org/W4322766882 | Parameter-efficient fine-tuning of large-scale pre-trained language models | 2023 | Nature Machine Intelligence | article | 745 | yes | Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, +12 more | Computer science, Fine-tuning, Scale (ratio), Categorization, Computation, Adaptation (eye), +7 more | https://doi.org/10.1038/s42256-023-00626-4 | Abstract With the prevalence of pre-trained language models (PLMs) and the pre-training–fine-tuning paradigm, it has been continuously shown that larger models tend to yield better performance. However, as PLMs scale up, fine-tuning and storing all the parameters is prohibitively costly and eventually becomes practically infeasible. This necessitates a new branch of research focusing on the parameter-efficient adaptation of PLMs, which optimizes a small portion of the model parameters while keeping the rest fixed, drastically cutting down computation and storage costs. In general, it demonstrates that large-scale models could be effectively stimulated by the optimization of a few parameters. Despite the various designs, here we discuss and analyse the approaches under a more consistent and accessible term ‘delta-tuning’, where ‘delta’ a mathematical notation often used to denote changes… |
| https://openalex.org/W3209222989 | Development and multicenter validation of chest X-ray radiography interpretations based on natural language processing | 2021 | Communications Medicine | article | 15 | yes | Yaping Zhang, Mingqian Liu, Shundong Hu, Yao Shen, Jun Lan, Beibei Jiang, Geertruida H. de Bock, Rozemarijn Vliegenthart, +2 more | Convolutional neural network, Medicine, Radiography, Significant difference, Artificial intelligence, Radiology, +4 more | https://doi.org/10.1038/s43856-021-00043-x |  |
| https://openalex.org/W2971258845 | Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets | 2019 |  | preprint | 813 | yes | Yifan Peng, Shankai Yan, Zhiyong Lu | Benchmark (surveying), Benchmarking, Computer science, Artificial intelligence, Natural language processing, Biomedicine, +17 more | https://doi.org/10.18653/v1/w19-5006 | Inspired by the success of the General Language Understanding Evaluation benchmark, we introduce the Biomedical Language Understanding Evaluation (BLUE) benchmark to facilitate research in the development of pre-training language representations in the biomedicine domain. The benchmark consists of five tasks with ten datasets that cover both biomedical and clinical texts with different dataset sizes and difficulties. We also evaluate several baselines based on BERT and ELMo and find that the BERT model pre-trained on PubMed abstracts and MIMIC-III clinical notes achieves the best results. We make the datasets, pre-trained models, and codes publicly available at https://github.com/ ncbi-nlp/BLUE_Benchmark. |
| https://openalex.org/W4291908500 | Multi-Topic Categorization in a Low-Resource Ewe Language: A Modern Transformer Approach | 2022 | 2022 7th International Conference on Computer and Communication Systems (ICCCS) | article | 8 | no | Victor Kwaku Agbesi, Wenyu Chen, Noble Arden Kuadey, Gerald Tietaa Maale | Computer science, Categorization, Transformer, Artificial intelligence, Encoder, Natural language processing, +12 more | https://doi.org/10.1109/icccs55155.2022.9846372 | The evolution of natural language processing (NLP) recently, paved the way for text categorization. With this mechanism, allocating a large volume of textual data to a category is much easier. This task is more challenging in dealing with multi-topic categorizations in a low-resource language. Transformer-based mechanisms have shown much strength in NLP tasks. However, low-resourced, low-data settings and a lack of benchmark datasets make it difficult to perform any NLP-related task in these extremely low-resource languages with data-points and dataset constraints. In this work, the authors focus on creating a new benchmark dataset for a low-resourced language and performed a multi-topic categorization using this dataset. We further propose an EweBERT model, which is built on the pre-trained transformer model known as Bidirectional Encoder Representations from Transformers (BERT) for mu… |
| https://openalex.org/W2903739847 | Neural Speech Synthesis with Transformer Network | 2019 | Proceedings of the AAAI Conference on Artificial Intelligence | article | 712 | yes | Naihan Li, Shujie Liu, Yanqing Liu, Sheng Zhao, Ming Liu | Computer science, Transformer, Inference, Spectrogram, Artificial neural network, Encoder, +8 more | https://doi.org/10.1609/aaai.v33i01.33016706 | Although end-to-end neural text-to-speech (TTS) methods (such as Tacotron2) are proposed and achieve state-of-theart performance, they still suffer from two problems: 1) low efficiency during training and inference; 2) hard to model long dependency using current recurrent neural networks (RNNs). Inspired by the success of Transformer network in neural machine translation (NMT), in this paper, we introduce and adapt the multi-head attention mechanism to replace the RNN structures and also the original attention mechanism in Tacotron2. With the help of multi-head self-attention, the hidden states in the encoder and decoder are constructed in parallel, which improves training efficiency. Meanwhile, any two inputs at different times are connected directly by a self-attention mechanism, which solves the long range dependency problem effectively. Using phoneme sequences as input, our Transfor… |
| https://openalex.org/W2997200074 | ERNIE 2.0: A Continual Pre-Training Framework for Language Understanding | 2020 | Proceedings of the AAAI Conference on Artificial Intelligence | article | 717 | yes | Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Hao Tian, Hua Wu, Haifeng Wang | Computer science, Natural language processing, Artificial intelligence, Task (project management), GRASP, Construct (python library), +10 more | https://doi.org/10.1609/aaai.v34i05.6428 | Recently pre-trained models have achieved state-of-the-art results in various language understanding tasks. Current pre-training procedures usually focus on training the model with several simple tasks to grasp the co-occurrence of words or sentences. However, besides co-occurring information, there exists other valuable lexical, syntactic and semantic information in training corpora, such as named entities, semantic closeness and discourse relations. In order to extract the lexical, syntactic and semantic information from training corpora, we propose a continual pre-training framework named ERNIE 2.0 which incrementally builds pre-training tasks and then learn pre-trained models on these constructed tasks via continual multi-task learning. Based on this framework, we construct several tasks and train the ERNIE 2.0 model to capture lexical, syntactic and semantic aspects of information… |
| https://openalex.org/W2996856320 | end-to-end training of a large vocabulary end-to-end speech recognition system | 2019 | arXiv (Cornell University) | preprint | 9 | yes | Chanwoo Kim, Sung-Soo Kim, Kwangyoun Kim, Mehul Kumar, Jiyeon Kim, Kyungmin Lee, Changwoo Han, Abhinav Garg, +5 more | End-to-end principle, Computer science, Vocabulary, Test set, Speech recognition, Artificial neural network, +9 more | http://arxiv.org/abs/1912.11040 | In this paper, we present an end-to-end training framework for building state-of-the-art end-to-end speech recognition systems. Our training system utilizes a cluster of Central Processing Units(CPUs) and Graphics Processing Units (GPUs). The entire data reading, large scale data augmentation, neural network parameter updates are all performed "on-the-fly". We use vocal tract length perturbation [1] and an acoustic simulator [2] for data augmentation. The processed features and labels are sent to the GPU cluster. The Horovod allreduce approach is employed to train neural network parameters. We evaluated the effectiveness of our system on the standard Librispeech corpus [3] and the 10,000-hr anonymized Bixby English dataset. Our end-to-end speech recognition system built using this training infrastructure showed a 2.44 % WER on test-clean of the LibriSpeech test set after applying shallo… |
| https://openalex.org/W4404294063 | LegalReasoner: A Multi-Stage Framework for Legal Judgment Prediction via Large Language Models and Knowledge Integration | 2024 | IEEE Access | article | 20 | yes | Xuran Wang, Xinguang Zhang, Vanessa Hoo, Z. Shao, Xuguang Zhang | Computer science, Stage (stratigraphy), Artificial intelligence, Natural language processing, Machine learning, Paleontology, +1 more | https://doi.org/10.1109/access.2024.3496666 | Legal judgment prediction (LJP) presents a formidable challenge in artificial intelligence, demanding intricate comprehension of legal texts, nuanced interpretation of statutes, and complex reasoning over multifaceted case elements. While recent advancements in natural language processing have shown promise, existing approaches often struggle to capture the sophisticated interplay between facts, legal principles, and precedents that characterize legal decision-making. This paper introduces LegalReasoner, a novel multi-stage framework that leverages large language models (LLMs) and integrates domain-specific knowledge for enhanced legal judgment prediction. Our approach encompasses four key stages: 1) legal knowledge infusion, where we pre-train an LLM on a vast corpus of legal literature using contrastive learning techniques; 2) case-law retrieval, employing a graph neural network to id… |
| https://openalex.org/W2998356391 | Unicoder-VL: A Universal Encoder for Vision and Language by Cross-Modal Pre-Training | 2020 | Proceedings of the AAAI Conference on Artificial Intelligence | article | 734 | yes | Gen Li, Nan Duan, Yuejian Fang, Ming Gong, Daxin Jiang | Computer science, Modal, Transformer, Encoder, Natural language processing, Artificial intelligence, +15 more | https://doi.org/10.1609/aaai.v34i07.6795 | We propose Unicoder-VL, a universal encoder that aims to learn joint representations of vision and language in a pre-training manner. Borrow ideas from cross-lingual pre-trained models, such as XLM (Lample and Conneau 2019) and Unicoder (Huang et al. 2019), both visual and linguistic contents are fed into a multi-layer Transformer (Vaswani et al. 2017) for the cross-modal pre-training, where three pre-trained tasks are employed, including Masked Language Modeling(MLM), Masked Object Classification(MOC) and Visual-linguistic Matching(VLM). The first two tasks learn context-aware representations for input tokens based on linguistic and visual contents jointly. The last task tries to predict whether an image and a text describe each other. After pretraining on large-scale image-caption pairs, we transfer Unicoder-VL to caption-based image-text retrieval and visual commonsense reasoning, wi… |
| https://openalex.org/W4403304337 | Enhancement of Named Entity Recognition in Low-Resource Languages with Data Augmentation and BERT Models: A Case Study on Urdu | 2024 | Computers | article | 11 | yes | Fida Ullah, Alexander Gelbukh, Muhammad Tayyab Zamir, Edgardo M. Felipe‐Riverón, Grigori Sidorov | Urdu, Named-entity recognition, Computer science, Natural language processing, Artificial intelligence, Named entity, +7 more | https://doi.org/10.3390/computers13100258 | Identifying and categorizing proper nouns in text, known as named entity recognition (NER), is crucial for various natural language processing tasks. However, developing effective NER techniques for low-resource languages like Urdu poses challenges due to limited training data, particularly in the nastaliq script. To address this, our study introduces a novel data augmentation method, “contextual word embeddings augmentation” (CWEA), for Urdu, aiming to enrich existing datasets. The extended dataset, comprising 160,132 tokens and 114,912 labeled entities, significantly enhances the coverage of named entities compared to previous datasets. We evaluated several transformer models on this augmented dataset, including BERT-multilingual, RoBERTa-Urdu-small, BERT-base-cased, and BERT-large-cased. Notably, the BERT-multilingual model outperformed others, achieving the highest macro F1 score of… |
| https://openalex.org/W4388046764 | Chat GPT and its Capabilities | 2023 | International Journal for Research in Applied Science and Engineering Technology | article | 4 | yes | Sharada Vishvanath Sarode, Vrunda Kantilal Bhamare | Transformer, Computer science, Generative grammar, Artificial intelligence, Natural language processing, Natural language, +12 more | https://doi.org/10.22214/ijraset.2023.56355 | Abstract: ChatGPT is a large language model that uses deep learning techniques to generate human-like text.It is based on the GPT (Generative Pre-Trained Transformer)architecture, which uses a transformer neural network to process and generate text. The model is pre trained on a massive dataset of text such as books, articles and websites, so it can understand the pattern and structure of natural language when given a prompt or a starting point, the model uses this pre-trained knowledge to generate text that continues the given input in a coherent and natural way |
| https://openalex.org/W4387895541 | Using transformer in stock trend prediction | 2023 | Applied and Computational Engineering | article | 4 | yes | Zhichen Liu | Computer science, Transformer, Convolutional neural network, Scalability, Artificial intelligence, Recurrent neural network, +7 more | https://doi.org/10.54254/2755-2721/22/20231212 | Large transformer model had achieved good results in many tasks, such as computer vision (CV) and natural language processing (NLP). However, in financial domains, the application of large deep learning models is rarely observed. Stock Trend Prediction (STP) is a task that using Limit Order Books (LOBs) to predict the future stock price trend by the sequence of historical limit order information, the trend can be Current works are mostly based on the structure of Convolutional Neural Network (CNN) + Recurrent Neural Networks (RNN). This structure is hard to parallel and cannot make full use of GPU resources. It is also difficult to increase the dimension to fit more complex data and performs poor when time sequence is long. Recently, some works proposed that CNN + Transformer model can also work is solving this task. This paper verifies that Transformer can be directly used into STP tas… |
| https://openalex.org/W3208687975 | Recent advances and applications of deep learning methods in materials science | 2022 | npj Computational Materials | article | 953 | yes | Kamal Choudhary, Brian DeCost, Chi Chen, Anubhav Jain, Francesca Tavazza, Ryan Cohn, Cheol Woo Park, Alok Choudhary, +5 more | Deep learning, Computer science, Data science, Field (mathematics), Artificial intelligence, Identification (biology), +10 more | https://doi.org/10.1038/s41524-022-00734-6 |  |
| https://openalex.org/W4223604174 | Vision Transformer based System for Fruit Quality Evaluation | 2022 | Research Square (Research Square) | preprint | 6 | yes | Tanushri Kumar, Rallabandi Shivani | Transformer, Automation, Computer science, Convolutional neural network, Artificial intelligence, Agricultural engineering, +9 more | https://doi.org/10.21203/rs.3.rs-1526586/v1 | Abstract Purpose Fruit quality assessment is one of the most pressing issues in the farming industry. Agriculturists would benefit significantly from the capacity to recognize the freshness of fruits, as it will allow them to optimize the harvesting stage and avoid reaping either underdeveloped or overdeveloped natural products. Productivity has decreased due to a lack of low-cost technology and equitable access. Despite large-scale agricultural mechanization in some parts of the country, most agricultural operations are still carried out by hand with simple instruments. The goal of this research is to automate the task of evaluating fruit quality. Methods Transformers were first presented in the field of natural language processing, and they offer dramatic performance improvements over existing models in NLP like as LSTMs and GRU. The Vision Transformer, or ViT, is an image classificat… |
| https://openalex.org/W2996851481 | Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment | 2020 | Proceedings of the AAAI Conference on Artificial Intelligence | article | 819 | yes | Di Jin, Zhijing Jin, Joey Tianyi Zhou, Peter Szolovits | Adversarial system, Computer science, Grammaticality, Artificial intelligence, Robustness (evolution), Baseline (sea), +14 more | https://doi.org/10.1609/aaai.v34i05.6311 | Machine learning algorithms are often vulnerable to adversarial examples that have imperceptible alterations from the original counterparts but can fool the state-of-the-art models. It is helpful to evaluate or even improve the robustness of these models by exposing the maliciously crafted adversarial examples. In this paper, we present TextFooler, a simple but strong baseline to generate adversarial text. By applying it to two fundamental natural language tasks, text classification and textual entailment, we successfully attacked three target models, including the powerful pre-trained BERT, and the widely used convolutional and recurrent neural networks. We demonstrate three advantages of this framework: (1) effective—it outperforms previous attacks by success rate and perturbation rate, (2) utility-preserving—it preserves semantic content, grammaticality, and correct types classified… |
| https://openalex.org/W3127133114 | A Comparison of Transformer, Recurrent Neural Networks and SMT in Tamil to Sinhala MT | 2020 |  | article | 6 | no | Ashmari Pramodya, Randil Pushpananda, Ruvan Weerasinghe | Machine translation, Computer science, Tamil, Transformer, Artificial intelligence, Natural language processing, +11 more | https://doi.org/10.1109/icter51097.2020.9325431 | Neural Machine Translation (NMT) is currently the most promising approach for machine translation. The attention mechanism is a successful technique in modern Natural Language Processing (NLP), especially in tasks like machine translation. The recently proposed network architecture of the Transformer is based entirely on attention mechanisms and achieves a new state of the art results in neural machine translation, outperforming other sequence-to-sequence models. Although it is successful in a resource-rich setting, its applicability for low-resource language pairs is still debatable. Additionally when the language pair is morphologically rich and also when the corpora is multi-domain, the lack of a large parallel corpus becomes a significant barrier. In this study, we explore different NMT algorithms - Long Short Term Memory (LSTM) and Transformer based NMT, to translate the Tamil to S… |
| https://openalex.org/W4381982883 | ChatGPT for Education and Research: Opportunities, Threats, and Strategies | 2023 | Applied Sciences | article | 830 | yes | Md. Mostafizer Rahman, Yutaka Watanobe | Cheating, Computer science, Coding (social sciences), Perspective (graphical), Mathematics education, Artificial intelligence, +4 more | https://doi.org/10.3390/app13095783 | In recent years, the rise of advanced artificial intelligence technologies has had a profound impact on many fields, including education and research. One such technology is ChatGPT, a powerful large language model developed by OpenAI. This technology offers exciting opportunities for students and educators, including personalized feedback, increased accessibility, interactive conversations, lesson preparation, evaluation, and new ways to teach complex concepts. However, ChatGPT poses different threats to the traditional education and research system, including the possibility of cheating on online exams, human-like text generation, diminished critical thinking skills, and difficulties in evaluating information generated by ChatGPT. This study explores the potential opportunities and threats that ChatGPT poses to overall education from the perspective of students and educators. Furtherm… |
| https://openalex.org/W4287867803 | Stress Test Evaluation of Transformer-based Models in Natural Language\n Understanding Tasks | 2020 | arXiv (Cornell University) | preprint | 10 | yes | Carlos Aspillaga, Andrés Carvallo, Vladimir Araujo | Transformer, Computer science, Robustness (evolution), Inference, Artificial intelligence, Adversarial system, +14 more | http://arxiv.org/abs/2002.06261 | There has been significant progress in recent years in the field of Natural\nLanguage Processing thanks to the introduction of the Transformer architecture.\nCurrent state-of-the-art models, via a large number of parameters and\npre-training on massive text corpus, have shown impressive results on several\ndownstream tasks. Many researchers have studied previous (non-Transformer)\nmodels to understand their actual behavior under different scenarios, showing\nthat these models are taking advantage of clues or failures of datasets and\nthat slight perturbations on the input data can severely reduce their\nperformance. In contrast, recent models have not been systematically tested\nwith adversarial-examples in order to show their robustness under severe stress\nconditions. For that reason, this work evaluates three Transformer-based models\n(RoBERTa, XLNet, and BERT) in Natural Language In… |
| https://openalex.org/W4285294723 | GLM: General Language Model Pretraining with Autoregressive Blank Infilling | 2022 | Proceedings of the 60th Annual Meeting of the Association for Computational Lin… | article | 845 | yes | Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, Jie Tang | Autoregressive model, Blank, Natural language processing, Linguistics, Volume (thermodynamics), Association (psychology), +11 more | https://doi.org/10.18653/v1/2022.acl-long.26 | Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, Jie Tang. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022. |
| https://openalex.org/W3160818736 | Research and Implementation of Chinese Couplet Generation System With Attention-Based Transformer Mechanism | 2021 | IEEE Transactions on Computational Social Systems | article | 9 | no | Yufeng Wang, Jiang Zhang, Bo Zhang, Qun Jin | Couplet, Computer science, Transformer, Natural language processing, Artificial intelligence, Encoder, +8 more | https://doi.org/10.1109/tcss.2021.3072153 | Couplet is a unique art form in Chinese traditional culture. The development of deep neural network (DNN) technology makes it possible for computers to automatically generate couplets. Especially, Transformer is a DNN-based "Encoder–Decoder" framework, and widely used in natural language processing (NLP). However, the existed Transformer mechanism cannot fully exploit the essential linguistic knowledge in Chinese, including the special format and requirements of Chinese couplets. Therefore, this article adapts the Transformer mechanism to generate meaningful Chinese couplets. Specifically, the contributions of our work are threefold. First, considering the fact that the words in the corresponding positions of the antecedent clause and the subsequent clause in a Chinese couplet always have same part-of-speech (pos, i.e., word class), pos information is intentionally added into the Transf… |
| https://openalex.org/W2951286828 | Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference | 2019 |  | article | 899 | yes | Tom McCoy, Ellie Pavlick, Tal Linzen | Heuristics, Computer science, Heuristic, Artificial intelligence, Natural language processing, Subsequence, +14 more | https://doi.org/10.18653/v1/p19-1334 | A machine learning system can score well on a given test set by relying on heuristics that are effective for frequent example types but break down in more challenging cases. We study this issue within natural language inference (NLI), the task of determining whether one sentence entails another. We hypothesize that statistical NLI models may adopt three fallible syntactic heuristics: the lexical overlap heuristic, the subsequence heuristic, and the constituent heuristic. To determine whether models have adopted these heuristics, we introduce a controlled evaluation set called HANS (Heuristic Analysis for NLI Systems), which contains many examples where the heuristics fail. We find that models trained on MNLI, including BERT, a state-of-the-art model, perform very poorly on HANS, suggesting that they have indeed adopted these heuristics. We conclude that there is substantial room for imp… |
| https://openalex.org/W4391453501 | Dynamic Transformer Architecture for Continual Learning of Multimodal Tasks | 2024 | SSRN Electronic Journal | preprint | 6 | yes | Yuliang Cai, Mohammad Rostami | Architecture, Transformer, Computer science, Computer architecture, Artificial intelligence, Engineering, +4 more | https://doi.org/10.2139/ssrn.4713352 |  |
| https://openalex.org/W2965857891 | Heterogeneous Graph Neural Network | 2019 |  | article | 1418 | yes | Chuxu Zhang, Dongjin Song, Chao Huang, Ananthram Swami, Nitesh V. Chawla | Computer science, Theoretical computer science, Stochastic gradient descent, Graph embedding, Embedding, Graph, +11 more | https://doi.org/10.1145/3292500.3330961 | Representation learning in heterogeneous graphs aims to pursue a meaningful vector representation for each node so as to facilitate downstream applications such as link prediction, personalized recommendation, node classification, etc. This task, however, is challenging not only because of the demand to incorporate heterogeneous structural (graph) information consisting of multiple types of nodes and edges, but also due to the need for considering heterogeneous attributes or contents (e.g., text or image) associated with each node. Despite a substantial amount of effort has been made to homogeneous (or heterogeneous) graph embedding, attributed graph embedding as well as graph neural networks, few of them can jointly consider heterogeneous structural (graph) information as well as heterogeneous contents information of each node effectively. In this paper, we propose HetGNN, a heterogene… |
| https://openalex.org/W3037973456 | PowerNorm: Rethinking Batch Normalization in Transformers | 2020 | arXiv (Cornell University) | preprint | 16 | yes | Sheng Shen, Zhewei Yao, Amir Gholami, Michael W. Mahoney, Kurt Keutzer | Normalization (sociology), Transformer, Computer science, Backpropagation, Artificial intelligence, Artificial neural network, +8 more | http://arxiv.org/abs/2003.07845 | The standard normalization method for neural network (NN) models used in Natural Language Processing (NLP) is layer normalization (LN). This is different than batch normalization (BN), which is widely-adopted in Computer Vision. The preferred use of LN in NLP is principally due to the empirical observation that a (naive/vanilla) use of BN leads to significant performance degradation for NLP tasks; however, a thorough understanding of the underlying reasons for this is not always evident. In this paper, we perform a systematic study of NLP transformer models to understand why BN has a poor performance, as compared to LN. We find that the statistics of NLP data across the batch dimension exhibit large fluctuations throughout training. This results in instability, if BN is naively implemented. To address this, we propose Power Normalization (PN), a novel normalization scheme that resolves… |
| https://openalex.org/W3096655658 | Contrastive Representation Learning: A Framework and Review | 2020 | IEEE Access | article | 770 | yes | Phuc H. Le-Khac, Graham Healy, Alan F. Smeaton | Computer science, Artificial intelligence, Natural language processing, Active learning (machine learning), Contrastive analysis, Representation (politics), +8 more | https://doi.org/10.1109/access.2020.3031549 | Contrastive Learning has recently received interest due to its success in self-supervised representation learning in the computer vision domain. However, the origins of Contrastive Learning date as far back as the 1990s and its development has spanned across many fields and domains including Metric Learning and natural language processing. In this paper we provide a comprehensive literature review and we propose a general Contrastive Representation Learning framework that simplifies and unifies many different contrastive learning methods. We also provide a taxonomy for each of the components of contrastive learning in order to summarise it and distinguish it from other forms of machine learning. We then discuss the inductive biases which are present in any contrastive learning system and we analyse our framework under different views from various sub-fields of Machine Learning. Examples… |
| https://openalex.org/W3043034704 | FTRANS: Energy-Efficient Acceleration of Transformers using FPGA | 2020 | arXiv (Cornell University) | preprint | 7 | yes | Bingbing Li, Santosh Pandey, Haowen Fang, Yanjun Lyv, Ji Li, Jieyang Chen, Mimi Xie, Lipeng Wan, +2 more | Computer science, Field-programmable gate array, Transformer, Computation, Efficient energy use, Parallel computing, +11 more | http://arxiv.org/abs/2007.08563 | In natural language processing (NLP), the "Transformer" architecture was proposed as the first transduction model replying entirely on self-attention mechanisms without using sequence-aligned recurrent neural networks (RNNs) or convolution, and it achieved significant improvements for sequence to sequence tasks. The introduced intensive computation and storage of these pre-trained language representations has impeded their popularity into computation and memory-constrained devices. The field-programmable gate array (FPGA) is widely used to accelerate deep learning algorithms for its high parallelism and low latency. However, the trained models are still too large to accommodate to an FPGA fabric. In this paper, we propose an efficient acceleration framework, Ftrans, for transformer-based large scale language representations. Our framework includes enhanced block-circulant matrix (BCM)-b… |
| https://openalex.org/W4406072190 | Morphological parsing of Kazakh texts with deep learning approaches | 2025 | Journal of Mathematics Mechanics and Computer Science | article | 5 | yes | Мадина Мансурова, Diana Rakhimova | Kazakh, Parsing, Artificial intelligence, Natural language processing, Computer science, Linguistics, +1 more | https://doi.org/10.26577/jmmcs2024-v124-i4-a4 | Morphological analysis is a crucial task in Natural Language Processing (NLP) that greatly contributes to enhancing the performance of large language models (LLMs). Although NLP technologies have seen rapid advancements in recent years, the creation of efficient morphological analysis algorithms for morphologically complex languages, such as Kazakh, continues to be a significant challenge. This research focuses on designing a morphological analysis algorithm for the Kazakh language, specifically optimized for integration with LLMs. The study will address the following tasks: data corpus collection and processing, selection and adaptation of suitable algorithms, and model training and evaluation. This paper delivers a detailed exploration of using deep learning models for the morphological analysis of the Kazakh language, specifically highlighting Recurrent Neural Networks (RNN) and Tran… |
| https://openalex.org/W3171378634 | Coarse and Fine-Grained Hostility Detection in Hindi Posts Using Fine Tuned Multilingual Embeddings | 2021 | Communications in computer and information science | book-chapter | 9 | no | Arkadipta De, Venkatesh Elangovan, Kaushal Kumar Maurya, Maunendra Sankar Desarkar | Hindi, Hostility, Computer science, Offensive, Artificial intelligence, Unavailability, +14 more | https://doi.org/10.1007/978-3-030-73696-5_19 |  |
| https://openalex.org/W4372306494 | FlexiBERT: Are Current Transformer Architectures too Homogeneous and Rigid? | 2023 | Journal of Artificial Intelligence Research | article | 9 | yes | Shikhar Tuli, Bhishma Dedhia, Shreshth Tuli, Niraj K. Jha | Computer science, Hyperparameter, Leverage (statistics), Curse of dimensionality, Machine learning, Artificial intelligence, +2 more | https://doi.org/10.1613/jair.1.13942 | The existence of a plethora of language models makes the problem of selecting the best one for a custom task challenging. Most state-of-the-art methods leverage transformer-based models (e.g., BERT) or their variants. However, training such models and exploring their hyperparameter space is computationally expensive. Prior work proposes several neural architecture search (NAS) methods that employ performance predictors (e.g., surrogate models) to address this issue; however, such works limit analysis to homogeneous models that use fixed dimensionality throughout the network. This leads to sub-optimal architectures. To address this limitation, we propose a suite of heterogeneous and flexible models, namely FlexiBERT, that have varied encoder layers with a diverse set of possible operations and different hidden dimensions. For better-posed surrogate modeling in this expanded design space,… |
| https://openalex.org/W4385270094 | Zero-shot Prompting for Code Complexity Prediction Using GitHub Copilot | 2023 |  | article | 13 | no | Mohammed Latif Siddiq, Abdus Samee, Sk Ruhul Azgor, Md. Asif Haider, Shehabul Islam Sawraz, Joanna C. S. Santos | Computer science, Code (set theory), Source code, Zero (linguistics), Process (computing), Artificial intelligence, +5 more | https://doi.org/10.1109/nlbse59153.2023.00018 | Code generation models are gaining popularity because they can produce correct code from a prompt, speeding up the software development process. GitHub Copilot is currently one of the most commonly used tools for code generation. This tool is based on GPT3, a Large Language Model (LLM), and can perform zero-shot prompting tasks i.e., tasks for which the model is not specifically trained. In this paper, we describe a preliminary study that investigates whether GitHub Copilot can predict the runtime complexity of a given program using zero- shot prompting. In our study, we found that GitHub Copilot can correctly predict the runtime complexity 45.44% times in the first suggestion and 56.38 % times considering all suggestions. We also compared Copilot to other machine learning, neural network, and transformer-based approaches for code complexity prediction. We observed that Copilot outperfo… |
| https://openalex.org/W3008568373 | In Nomine Function: Naming Functions in Stripped Binaries with Neural Networks | 2019 | arXiv (Cornell University) | preprint | 12 | yes | Fiorella Artuso, Giuseppe Antonio Di Luna, Luca Massarelli, Leonardo Querzoni | Computer science, Transformer, Artificial intelligence, Artificial neural network, Function (biology), String (physics), +15 more | http://arxiv.org/abs/1912.07946 | In this paper we investigate the problem of automatically naming pieces of assembly code. Where by naming we mean assigning to an assembly function a string of words that would likely be assigned by a human reverse engineer. We formally and precisely define the framework in which our investigation takes place. That is we define the problem, we provide reasonable justifications for the choices that we made for the design of training and the tests. We performed an analysis on a large real-world corpora constituted by nearly 9 millions of functions taken from more than 22k softwares. In such framework we test baselines coming from the field of Natural Language Processing (e.g., Seq2Seq networks and Transformer). Interestingly, our evaluation shows promising results beating the state-of-the-art and reaching good performance. We investigate the applicability of tine-tuning (i.e., taking a mo… |
| https://openalex.org/W4404048128 | An Accurate and Efficient Approach to Knowledge Extraction from Scientific Publications Using Structured Ontology Models, Graph Neural Networks, and Large Language Models | 2024 | International Journal of Molecular Sciences | article | 14 | yes | Timofey Ivanisenko, П. С. Деменков, В. А. Иванисенко | Interpretability, Computer science, Artificial intelligence, Machine learning, Graph, Ontology, +8 more | https://doi.org/10.3390/ijms252111811 | The rapid growth of biomedical literature makes it challenging for researchers to stay current. Integrating knowledge from various sources is crucial for studying complex biological systems. Traditional text-mining methods often have limited accuracy because they don’t capture semantic and contextual nuances. Deep-learning models can be computationally expensive and typically have low interpretability, though efforts in explainable AI aim to mitigate this. Furthermore, transformer-based models have a tendency to produce false or made-up information—a problem known as hallucination—which is especially prevalent in large language models (LLMs). This study proposes a hybrid approach combining text-mining techniques with graph neural networks (GNNs) and fine-tuned large language models (LLMs) to extend biomedical knowledge graphs and interpret predicted edges based on published literature.… |
| https://openalex.org/W4322576830 | Deep Tabular Data Modeling With Dual-Route Structure-Adaptive Graph Networks | 2023 | IEEE Transactions on Knowledge and Data Engineering | article | 10 | no | Qinghua Zheng, Zhen Peng, Zhuohang Dang, Linchao Zhu, Ziqi Liu, Zhiqiang Zhang, Jun Zhou | Computer science, Artificial intelligence, Deep learning, Graph, Perceptron, Machine learning, +5 more | https://doi.org/10.1109/tkde.2023.3249186 | Thanks to the inherent spatial or sequential structures underlying the data like images and texts, deep architectures such as convolutional neural networks (CNNs) and the Transformer have been recognized as the preeminent approaches in image processing and language modeling. In the real world, there are a large number of tabular data without any explicit structures, which breaks the inductive bias of most neural networks like CNNs. Although multi-layer perceptrons (MLPs) obtain empirical success on tabular data, they cannot well explain the underlying relationship between multiple variables. Compared with other fields, research on deep models toward tabular data has received relatively less scrutiny. To bridge this gap, we propose Dual-Route Structure-Adaptive Graph Networks (DRSA-Net) to model the nonlinearity in tabular feature vectors without any prior. DRSA-Net adaptively learns a s… |
| https://openalex.org/W3008764429 | Assessment of Word-Level Neural Language Models for Sentence Completion | 2020 | Applied Sciences | article | 8 | yes | Heewoong Park, Jonghun Park | Sentence, Computer science, Recurrent neural network, Artificial intelligence, Sentence completion tests, Natural language processing, +8 more | https://doi.org/10.3390/app10041340 | The task of sentence completion, which aims to infer the missing text of a given sentence, was carried out to assess the reading comprehension level of machines as well as humans. In this work, we conducted a comprehensive study of various approaches for the sentence completion based on neural language models, which have been advanced in recent years. First, we revisited the recurrent neural network language model (RNN LM), achieving highly competitive results with an appropriate network structure and hyper-parameters. This paper presents a bidirectional version of RNN LM, which surpassed the previous best results on Microsoft Research (MSR) Sentence Completion Challenge and the Scholastic Aptitude Test (SAT) sentence completion questions. In parallel with directly applying RNN LM to sentence completion, we also employed a supervised learning framework that fine-tunes a large pre-traine… |
| https://openalex.org/W4288066876 | ProtGPT2 is a deep unsupervised language model for protein design | 2022 | Nature Communications | article | 687 | yes | Noelia Ferruz, Steffen Schmidt, Birte Höcker | Computer science, Protein design, Computational biology, Natural language, Protein structure, Similarity (geometry), +12 more | https://doi.org/10.1038/s41467-022-32007-7 |  |
| https://openalex.org/W4405142086 | Leveraging AI in ayurvedic agriculture: A RAG chatbot for comprehensive medicinal plant insights using hybrid deep learning approaches | 2024 | Telematics and Informatics Reports | article | 12 | yes | Biplov Paneru, Bipul Thapa, Bishwash Paneru, Bishwash Paneru, Bishwash Paneru | Chatbot, Deep learning, Agriculture, Computer science, Artificial intelligence, Traditional medicine, +4 more | https://doi.org/10.1016/j.teler.2024.100181 | Medicinal plants are offering a lot of potential for treatment of various chronic diseases as well as healing wounds, enhancing healthy living for consumers. The Nepalese and Indian agriculture systems are one of the main areas focusing on medicinal plant cultivation, and the abundant availability of these plants in these regions is driving growth in ayurvedic research. Traditional methods for detecting plants as well as generating insights on them are often inefficient and time-consuming due to the manual research need and expertise required in plant and biological lives. In this paper, we develop an advanced LLM (Large Language Model)-powered approach to reliably identify the available medicinal plants and their profitable insights for farmers. We compare multiple deep learning and transfer learning techniques, employing models such as deep convolutional neural networks and advanced t… |
| https://openalex.org/W3137147200 | A Survey of Quantization Methods for Efficient Neural Network Inference | 2022 |  | book-chapter | 946 | yes | Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W. Mahoney, Kurt Keutzer | Artificial neural network, Inference, Computer science, Quantization (signal processing), Artificial intelligence, Algorithm | https://doi.org/10.1201/9781003162810-13 | As soon as abstract mathematical computations were adapted to computation on digital computers, the problem of efficient representation, manipulation, and communication of the numerical values in those computations arose.Strongly related to the problem of numerical representation is the problem of quantization: in what manner should a set of continuous real-valued numbers be distributed over a fixed discrete set of numbers to minimize the number of bits required and also to maximize the accuracy of the attendant computations?This perennial problem of quantization is particularly relevant whenever memory and/or computational resources are severely restricted, and it has come to the forefront in recent years due to the remarkable performance of Neural Network models in computer vision, natural language processing, and related areas.Moving from floating-point representations to low-precisi… |
| https://openalex.org/W4396559861 | End-to-end automated speech recognition using a character based small scale transformer architecture | 2024 | Expert Systems with Applications | article | 7 | yes | Alexander Loubser, Pieter de Villiers, Allan De Freitas | Transformer, Computer science, Architecture, Word error rate, Speech recognition, Artificial intelligence, +5 more | https://doi.org/10.1016/j.eswa.2024.124119 | This study explores the feasibility of constructing a small-scale speech recognition system capable of competing with larger, modern automated speech recognition (ASR) systems in both performance and word error rate (WER). Our central hypothesis posits that a compact transformer-based ASR model can yield comparable results, specifically in terms of WER, to traditional ASR models while challenging contemporary ASR systems that boast significantly larger computational sizes. The aim is to extend ASR capabilities to under-resourced languages with limited corpora, catering to scenarios where practitioners face constraints in both data availability and computational resources. The model, comprising a compact convolutional neural network (CNN) and transformer architecture with 2.214 million parameters, challenges the conventional wisdom that large-scale transformer-based ASR systems are essen… |
| https://openalex.org/W2954226438 | SemEval-2019 Task 5: Multilingual Detection of Hate Speech Against Immigrants and Women in Twitter | 2019 |  | article | 840 | yes | Valerio Basile, Cristina Bosco, Elisabetta Fersini, Debora Nozza, Viviana Patti, Francisco M. Rangel Pardo, Paolo Rosso, Manuela Sanguinetti | SemEval, Computer science, Task (project management), Immigration, Natural language processing, Speech recognition, +5 more | https://doi.org/10.18653/v1/s19-2007 | The paper describes the organization of the SemEval 2019 Task 5 about the detection of hate speech against immigrants and women in Spanish and English messages extracted from Twitter. The task is organized in two related classification subtasks: a main binary subtask for detecting the presence of hate speech, and a finer-grained one devoted to identifying further features in hateful contents such as the aggressive attitude and the target harassed, to distinguish if the incitement is against an individual rather than a group. HatEval has been one of the most popular tasks in SemEval-2019 with a total of 108 submitted runs for Subtask A and 70 runs for Subtask B, from a total of 74 different teams. Data provided for the task are described by showing how they have been collected and annotated. Moreover, the paper provides an analysis and discussion about the participant systems and the res… |
| https://openalex.org/W4412100295 | Application of artificial intelligence large language models in drug target discovery | 2025 | Frontiers in Pharmacology | review | 7 | yes | Xinyu Liu, Jiafan Zhang, Xiaoran Wang, Maikun Teng, Guoying Wang, Xiaoming Zhou | Drug discovery, Computer science, Drug, Artificial intelligence, Computational biology, Natural language processing, +4 more | https://doi.org/10.3389/fphar.2025.1597351 | Drug target discovery is a fundamental aspect of contemporary drug research and development. However, the use of conventional biochemical screening, omics analysis, and related approaches is constrained by substantial technical complexity and significant resource requirements. With the advancement of artificial intelligence-based large language models, notable progress has been achieved in drug target identification. During target mining, large language models with natural language comprehension capabilities can efficiently integrate literature data resources and systematically analyze disease-associated biological pathways and potential targets. Notably, models specifically designed for biomolecular “language” have demonstrated advantages across multiple aspects. The genomics-focused large language model has significantly enhanced the accuracy of pathogenic gene variant identification… |
| https://openalex.org/W4389934542 | Hardware Accelerator Design for Sparse DNN Inference and Training: A Tutorial | 2023 | IEEE Transactions on Circuits & Systems II Express Briefs | article | 6 | no | Wendong Mao, Meiqi Wang, Xiaoru Xie, Xiao Wu, Zhongfeng Wang | Computer science, Inference, Artificial intelligence, Transformer, Machine learning, Artificial neural network, +13 more | https://doi.org/10.1109/tcsii.2023.3344681 | Deep neural networks (DNNs) are widely used in many fields, such as artificial intelligence generated content (AIGC) and robotics. To efficiently support these tasks, the model pruning technique is developed to compress the computational and memory-intensive DNNs. However, directly executing these sparse models on a common hardware accelerator can cause significant under-utilization, since invalid data resulting from the sparse patterns leads to unnecessary computations and irregular memory accesses. This brief analyzes the critical issues in accelerating sparse models, and provides an overview of typical hardware designs for various sparse DNNs, such as convolutional neural networks (CNNs), recurrent neural networks (RNNs), generative adversarial networks (GANs), and Transformers. Following the overview, we give a practical guideline of designing efficient accelerators for sparse DNNs… |
| https://openalex.org/W4387220126 | Negative Stances Detection from Multilingual Data Streams in Low-Resource Languages on Social Media Using BERT and CNN-Based Transfer Learning Model | 2023 | ACM Transactions on Asian and Low-Resource Language Information Processing | article | 7 | yes | Sanjay Kumar | Computer science, Social media, Convolutional neural network, Artificial intelligence, Transfer of learning, Encoder, +11 more | https://doi.org/10.1145/3625821 | Online social media allows users to connect with a large number of people across the globe and facilitate the exchange of information efficiently. These platforms cater to many of our day-to-day needs. However, at the same time, social media have been increasingly used to transmit negative stances such as derogatory language, hate speech, and cyberbullying. The task of identifying the negative stances from social media posts or comments or tweets is termed negative stance detection. One of the major challenges associated with negative stance detection is that most of the content published on social media is often in a multilingual format. This work aims to identify negative stances from multilingual data streams in low-resource languages on social media using a hybrid transfer learning and deep convolutional neural network approach. The proposed work starts by preprocessing the multilin… |
| https://openalex.org/W3184872676 | Automatic Classification for Ontology Generation by Pretrained Language Model | 2021 | Lecture notes in computer science | book-chapter | 6 | no | Atsushi Oba, Incheon Paik, Ayato Kuwana | Computer science, Ontology, Artificial intelligence, Natural language processing, Workflow, Transformer, +12 more | https://doi.org/10.1007/978-3-030-79457-6_18 |  |
| https://openalex.org/W4389132751 | Scaling deep learning for materials discovery | 2023 | Nature | article | 973 | yes | Amil Merchant, Simon Batzner, Samuel S. Schoenholz, Muratahan Aykol, Gowoon Cheon, Ekin D. Cubuk | Computer science, Deep learning, Convex hull, Scaling, Artificial intelligence, Nanotechnology, +4 more | https://doi.org/10.1038/s41586-023-06735-9 |  |
| https://openalex.org/W4320060374 | GraphBERT: Bridging Graph and Text for Malicious Behavior Detection on Social Media | 2022 | 2022 IEEE International Conference on Data Mining (ICDM) | article | 13 | no | Jiele Wu, Chunhui Zhang, Zheyuan Liu, Erchi Zhang, Steven Lloyd Wilson, Chuxu Zhang | Computer science, Social media, Bridging (networking), Graph embedding, Embedding, Graph, +7 more | https://doi.org/10.1109/icdm54844.2022.00065 | The development of social media (e.g., Twitter) allows users to make speeches with low cost and broad influence. Thus, social media has become a perfect place for users' malicious behaviors like committing hate crimes, spreading toxic information, abetting crimes, etc. Malicious behaviors are covert and widespread, with potential relevance regarding topic, person, place, and so on. Therefore, it is necessary to develop novel techniques to detect and disrupt malicious behavior on social media effectively. Previous research has shown promising results in extracting semantic text (speech) representation using natural language processing methods. Yet the latent relation between speeches and the connection between users behind speeches is rarely explored. In light of this, we propose a holistic model named Graph adaption BERT (GraphBERT) to detect malicious behaviors on Twitter with both sem… |
| https://openalex.org/W3034723486 | Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data | 2020 |  | article | 860 | yes | Emily M. Bender, Alexander Koller | Meaning (existential), Computer science, A priori and a posteriori, Natural language understanding, Theme (computing), Position paper, +10 more | https://doi.org/10.18653/v1/2020.acl-main.463 | The success of the large neural language models on many NLP tasks is exciting. However, we find that these successes sometimes lead to hype in which these models are being described as “understanding” language or capturing “meaning”. In this position paper, we argue that a system trained only on form has a priori no way to learn meaning. In keeping with the ACL 2020 theme of “Taking Stock of Where We’ve Been and Where We’re Going”, we argue that a clear understanding of the distinction between form and meaning will help guide the field towards better science around natural language understanding. |
| https://openalex.org/W4389991792 | Autonomous chemical research with large language models | 2023 | Nature | article | 654 | yes | Daniil A. Boiko, Robert MacKnight, Ben Kline, Gabriel dos Passos Gomes | Computer science, Documentation, Automation, Transformer, Artificial intelligence, The Internet, +11 more | https://doi.org/10.1038/s41586-023-06792-0 | Abstract Transformer-based large language models are making significant strides in various fields, such as natural language processing 1–5, biology 6,7, chemistry 8–10 and computer programming 11,12. Here, we show the development and capabilities of Coscientist, an artificial intelligence system driven by GPT-4 that autonomously designs, plans and performs complex experiments by incorporating large language models empowered by tools such as internet and documentation search, code execution and experimental automation. Coscientist showcases its potential for accelerating research across six diverse tasks, including the successful reaction optimization of palladium-catalysed cross-couplings, while exhibiting advanced capabilities for (semi-)autonomous experimental design and execution. Our findings demonstrate the versatility, efficacy and explainability of artificial intelligence systems… |
| https://openalex.org/W2970986510 | Knowledge Enhanced Contextual Word Representations | 2019 |  | article | 664 | yes | Matthew E. Peters, Mark E Neumann, Robert Logan, Roy Schwartz, Vidur Joshi, Sameer Singh, Noah A. Smith | Computer science, Natural language processing, Joint (building), Artificial intelligence, Word (group theory), Natural language, +11 more | https://doi.org/10.18653/v1/d19-1005 | Matthew E. Peters, Mark Neumann, Robert Logan, Roy Schwartz, Vidur Joshi, Sameer Singh, Noah A. Smith. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019. |
| https://openalex.org/W4389194717 | Indoor semantic segmentation based on Swin-Transformer | 2023 | Journal of Visual Communication and Image Representation | article | 9 | no | Yunping Zheng, Yuan Xu, Shiqiang Shu, Mudar Sarem | Segmentation, Computer science, Artificial intelligence, Computer vision, Transformer, RGB color model, +7 more | https://doi.org/10.1016/j.jvcir.2023.103991 |  |
| https://openalex.org/W4309847983 | PiTE: TCR-epitope Binding Affinity Prediction Pipeline using Transformer-based Sequence Encoder | 2022 |  | article | 13 | yes | Pengfei Zhang, Seojin Bang, Heewook Lee | Epitope, Encoder, Embedding, Computer science, Pipeline (software), Sequence (biology), +13 more | https://doi.org/10.1142/9789811270611_0032 | Accurate prediction of TCR binding affinity to a target antigen is important for development of immunotherapy strategies. Recent computational methods were built on various deep neural networks and used the evolutionary-based distance matrix BLOSUM to embed amino acids of TCR and epitope sequences to numeric values. A pre-trained language model of amino acids is an alternative embedding method where each amino acid in a peptide is embedded as a continuous numeric vector. Little attention has yet been given to summarize the amino-acid-wise embedding vectors to sequence-wise representations. In this paper, we propose PiTE, a two-step pipeline for the TCR-epitope binding affinity prediction. First, we use an amino acids embedding model pre-trained on a large number of unlabeled TCR sequences and obtain a real-valued representation from a string representation of amino acid sequences. Secon… |
| https://openalex.org/W4389520168 | Approximating Two-Layer Feedforward Networks for Efficient Transformers | 2023 |  | article | 6 | yes | Róbert Csordás, Kazuki Irie, Jürgen Schmidhuber | Computer science, Feed forward, Transformer, Feedforward neural network, Artificial neural network, Artificial intelligence, +6 more | https://doi.org/10.18653/v1/2023.findings-emnlp.49 | How to reduce compute and memory requirements of neural networks (NNs) without sacrificing performance? Many recent works use sparse Mixtures of Experts (MoEs) to build resource-efficient large language models (LMs). Here we introduce several novel perspectives on MoEs, presenting a general framework that *unifies* various methods to *approximate two-layer NNs* (e.g., feedforward blocks of Transformers), including product-key memories (PKMs). Leveraging insights from this framework, we propose methods to improve both MoEs and PKMs. Unlike prior work that compares MoEs with dense baselines under the *compute-equal* condition, our evaluation condition is *parameter-equal*, which is crucial to properly evaluate LMs. We show that our MoEs are competitive with the *dense* Transformer-XL on both the WikiText-103 and enwiki8 datasets at two different scales, while being much more resource effi… |
| https://openalex.org/W2998385486 | K-BERT: Enabling Language Representation with Knowledge Graph | 2020 | Proceedings of the AAAI Conference on Artificial Intelligence | article | 744 | yes | Weijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Qi Ju, Haotang Deng, Ping Wang | Computer science, Sentence, Domain knowledge, Domain (mathematical analysis), Representation (politics), Natural language processing, +9 more | https://doi.org/10.1609/aaai.v34i03.5681 | Pre-trained language representation models, such as BERT, capture a general language representation from large-scale corpora, but lack domain-specific knowledge. When reading a domain text, experts make inferences with relevant knowledge. For machines to achieve this capability, we propose a knowledge-enabled language representation model (K-BERT) with knowledge graphs (KGs), in which triples are injected into the sentences as domain knowledge. However, too much knowledge incorporation may divert the sentence from its correct meaning, which is called knowledge noise (KN) issue. To overcome KN, K-BERT introduces soft-position and visible matrix to limit the impact of knowledge. K-BERT can easily inject domain knowledge into the models by being equipped with a KG without pre-training by itself because it is capable of loading model parameters from the pre-trained BERT. Our investigation r… |
| https://openalex.org/W4366761428 | Few-Shot Learning for Clinical Natural Language Processing Using Siamese Neural Networks: Algorithm Development and Validation Study | 2023 | JMIR AI | article | 17 | yes | David Oniani, Premkumar Chandrasekar, Sonish Sivarajkumar, Yanshan Wang | Artificial intelligence, Computer science, Machine learning, Deep learning, Task (project management), Natural language processing, +10 more | https://doi.org/10.2196/44293 | Background Natural language processing (NLP) has become an emerging technology in health care that leverages a large amount of free-text data in electronic health records to improve patient care, support clinical decisions, and facilitate clinical and translational science research. Recently, deep learning has achieved state-of-the-art performance in many clinical NLP tasks. However, training deep learning models often requires large, annotated data sets, which are normally not publicly available and can be time-consuming to build in clinical domains. Working with smaller annotated data sets is typical in clinical NLP; therefore, ensuring that deep learning models perform well is crucial for real-world clinical NLP applications. A widely adopted approach is fine-tuning existing pretrained language models, but these attempts fall short when the training data set contains only a few annot… |
| https://openalex.org/W4293060246 | ReaderBench: Multilevel analysis of Russian text characteristics | 2022 | Russian Journal of Linguistics | article | 7 | yes | Dragos Corlatescu, Ştefan Ruşeţi, Mihai Dascălu | Computer science, Encoder, Natural language processing, Noun, Dependency (UML), Artificial intelligence, +15 more | https://doi.org/10.22363/2687-0088-30145 | This paper introduces an adaptation of the open source ReaderBench framework that now supports Russian multilevel analyses of text characteristics, while integrating both textual complexity indices and state-of-the-art language models, namely Bidirectional Encoder Representations from Transformers (BERT). The evaluation of the proposed processing pipeline was conducted on a dataset containing Russian texts from two language levels for foreign learners (A - Basic user and B - Independent user). Our experiments showed that the ReaderBench complexity indices are statistically significant in differentiating between the two classes of language level, both from: a) a statistical perspective, where a Kruskal-Wallis analysis was performed and features such as the “nmod” dependency tag or the number of nouns at the sentence level proved the be the most predictive; and b) a neural network perspec… |
| https://openalex.org/W2937703214 | Multi-Head Multi-Layer Attention to Deep Language Representations for Grammatical Error Detection | 2019 | arXiv (Cornell University) | preprint | 6 | yes | Masahiro Kaneko, Mamoru Komachi | Computer science, Encoder, Sentence, Layer (electronics), Representation (politics), Language model, +15 more | http://arxiv.org/abs/1904.07334 | It is known that a deep neural network model pre-trained with large-scale data greatly improves the accuracy of various tasks, especially when there are resource constraints. However, the information needed to solve a given task can vary, and simply using the output of the final layer is not necessarily sufficient. Moreover, to our knowledge, exploiting large language representation models to detect grammatical errors has not yet been studied. In this work, we investigate the effect of utilizing information not only from the final layer but also from intermediate layers of a pre-trained language representation model to detect grammatical errors. We propose a multi-head multi-layer attention model that determines the appropriate layers in Bidirectional Encoder Representation from Transformers (BERT). The proposed method achieved the best scores on three datasets for grammatical error det… |
| https://openalex.org/W3095277904 | Increasing Learning Efficiency of Self-Attention Networks through Direct Position Interactions, Learnable Temperature, and Convoluted Attention | 2020 |  | article | 5 | yes | Philipp Dufter, Martin Schmitt, Hinrich Schütze | Computer science, Transformer, Position (finance), Artificial intelligence, Artificial neural network, Task (project management), +11 more | https://doi.org/10.18653/v1/2020.coling-main.324 | Self-Attention Networks (SANs) are an integral part of successful neural architectures such as Transformer (Vaswani et al., 2017), and thus of pretrained language models such as BERT (Devlin et al., 2019) or GPT-3 (Brown et al., 2020). Training SANs on a task or pretraining them on language modeling requires large amounts of data and compute resources. We are searching for modifications to SANs that enable faster learning, i.e., higher accuracies after fewer update steps. We investigate three modifications to SANs: direct position interactions, learnable temperature, and convoluted attention. When evaluating them on part-of-speech tagging, we find that direct position interactions are an alternative to position embeddings, and convoluted attention has the potential to speed up the learning process. |
| https://openalex.org/W2986154550 | CamemBERT: a Tasty French Language Model | 2020 |  | preprint | 697 | yes | Louis Martin, Benjamin Müller, Pedro Ortiz Suárez, Yoann Dupont, Laurent Romary, Éric Villemonte de la Clergerie, Djamé Seddah, Benoît Sagot | Computer science, Natural language processing, Concatenation (mathematics), Parsing, Inference, Artificial intelligence, +13 more | https://doi.org/10.18653/v1/2020.acl-main.645 | International audience |
| https://openalex.org/W4390621557 | The potential role of AI-based Chatbots in Engineering Education. Experiences from a teaching perspective | 2023 |  | article | 16 | no | Miguel Morales, Héctor R. Amado-Salvatierra, Rocael Hernández-Rizzardini, Mónica De La Roca | Chatbot, Computer science, Conversation, Debugging, Artificial intelligence, Context (archaeology), +5 more | https://doi.org/10.1109/fie58773.2023.10343296 | The irruption of Artificial Intelligence (AI) based chatbot tools is undoubtedly at the frontiers of education. AI chatbots in education has emerged as a promising solution to enhance the quality of education and to improve learning outcomes. As all new technology does, it has begun to generate news about prohibition, ethical aspects, anti-plagiarism detection tools, and a series of policies from different educational systems. However, we should not deny the positive aspects of these tools if they are well used. AI-based chatbots have interesting potential to help both teachers and students, who must learn to use them well for their own benefit. This article provides an overview of AI-based chatbots, particularly ChatGPT, an artificial intelligence language model developed by OpenAI. GPT, or "Generative Pre-Training Transformer" is a neural network trained to generate "human-like text"… |
| https://openalex.org/W4210283052 | Towards Effective and Generalizable Fine-tuning for Pre-trained Molecular Graph Models | 2022 |  | preprint | 10 | yes | Jun Xia, Jiangbin Zheng, Cheng Tan, Ge Wang, Stan Z. Li | Computer science, Transformer, Machine learning, Artificial intelligence, Fine-tuning, Graph, +6 more | https://doi.org/10.1101/2022.02.03.479055 | Abstract Graph Neural Networks (GNNs) and Transformer have emerged as dominant tools for AI-driven drug discovery. Many state-of-the-art methods first pre-train GNNs or the hybrid of GNNs and Transformer on a large molecular database and then fine-tune on downstream tasks. However, different from other domains such as computer vision (CV) or natural language processing (NLP), getting labels for molecular data of downstream tasks often requires resource-intensive wet-lab experiments. Besides, the pre-trained models are often of extremely high complexity with huge parameters. These often cause the fine-tuned model to over-fit the training data of downstream tasks and significantly deteriorate the performance. To alleviate these critical yet under-explored issues, we propose two straightforward yet effective strategies to attain better generalization performance: 1. MolAug, which enriches… |
| https://openalex.org/W2979040987 | Comprehensive Review of Artificial Neural Network Applications to Pattern Recognition | 2019 | IEEE Access | article | 683 | yes | Oludare Isaac Abiodun, Muhammad Ubale Kiru, Aman Jantan, Abiodun Esther Omolara, Kemi Victoria Dada, Abubakar Malah Umar, Okafor Uchenwa Linus, Humaira Arshad, +2 more | Mean squared error, Artificial neural network, Computer science, Mean absolute percentage error, Artificial intelligence, Mean absolute error, +13 more | https://doi.org/10.1109/access.2019.2945545 | The era of artificial neural network (ANN) began with a simplified application in many fields and remarkable success in pattern recognition (PR) even in manufacturing industries. Although significant progress achieved and surveyed in addressing ANN application to PR challenges, nevertheless, some problems are yet to be resolved like whimsical orientation (the unknown path that cannot be accurately calculated due to its directional position). Other problem includes; object classification, location, scaling, neurons behavior analysis in hidden layers, rule, and template matching. Also, the lack of extant literature on the issues associated with ANN application to PR seems to slow down research focus and progress in the field. Hence, there is a need for state-of-the-art in neural networks application to PR to urgently address the above-highlights problems for more successes. The study furn… |
| https://openalex.org/W4390948086 | Revolutionizing Natural Language Processing with GPT-based Chatbots: A Review | 2023 | Technical Journal | review | 5 | yes | Smita Adhikari, Bhawana Dhakal | Computer science, Transformer, Automatic summarization, Natural language processing, Language model, Artificial intelligence, +9 more | https://doi.org/10.3126/tj.v3i1.61943 | OpenAI introduced a language model called GPT (Generative Pre-trained Transformer model. The algorithm learns to predict the following word in a phrase based on the context of the preceding words after being trained on a large text dataset. GPT employs a transformer architecture, a class of neural networks that have been demonstrated to perform exceptionally well on tasks involving natural language understanding. Pre-training was one of the fundamental advances of GPT, allowing the model to learn a variety of broad language representations from a vast amount of text input before being fine-tuned on tasks. Through this pre-training phase, the model can recognize pertinent information from the data that could be used for various upcoming tasks, such as information extraction, language translation, summarization, etc. Since its initial release, GPT has undergone several iterations since it… |
| https://openalex.org/W4386697749 | A foundation model for generalizable disease detection from retinal images | 2023 | Nature | article | 677 | yes | Yukun Zhou, Mark A. Chia, Siegfried K. Wagner, Murat Seçkin Ayhan, Dominic J. Williamson, Robbert Struyven, Timing Liu, Moucheng Xu, +86 more | Computer science, Artificial intelligence, Generalizability theory, Retinal, Machine learning, Adaptation (eye), +8 more | https://doi.org/10.1038/s41586-023-06555-x |  |
| https://openalex.org/W4401660139 | Improving protein function prediction by learning and integrating representations of protein sequences and function labels | 2024 | Bioinformatics Advances | article | 13 | yes | Frimpong Boadu, Jianlin Cheng | Protein function prediction, Function (biology), Protein function, Computer science, Artificial intelligence, Biology, +2 more | https://doi.org/10.1093/bioadv/vbae120 | Abstract Motivation As fewer than 1% of proteins have protein function information determined experimentally, computationally predicting the function of proteins is critical for obtaining functional information for most proteins and has been a major challenge in protein bioinformatics. Despite the significant progress made in protein function prediction by the community in the last decade, the general accuracy of protein function prediction is still not high, particularly for rare function terms associated with few proteins in the protein function annotation database such as the UniProt. Results We introduce TransFew, a new transformer model, to learn the representations of both protein sequences and function labels [Gene Ontology (GO) terms] to predict the function of proteins. TransFew leverages a large pre-trained protein language model (ESM2-t48) to learn function-relevant represent… |
| https://openalex.org/W3214250074 | Incident detection and classification in renewable energy news using pre-trained language models on deep neural networks | 2021 | Journal of Computational Methods in Sciences and Engineering | article | 10 | no | Qiqing Wang, Cunbin Li | Computer science, Artificial intelligence, Convolutional neural network, Renewable energy, Word2vec, Classifier (UML), +12 more | https://doi.org/10.3233/jcm-215594 | The surge of renewable energy systems can lead to increasing incidents that negatively impact economics and society, rendering incident detection paramount to understand the mechanism and range of those impacts. In this paper, a deep learning framework is proposed to detect renewable energy incidents from news articles containing accidents in various renewable energy systems. The pre-trained language models like Bidirectional Encoder Representations from Transformers (BERT) and word2vec are utilized to represent textual inputs, which are trained by the Text Convolutional Neural Networks (TCNNs) and Text Recurrent Neural Networks. Two types of classifiers for incident detection are trained and tested in this paper, one is a binary classifier for detecting the existence of an incident, the other is a multi-label classifier for identifying different incident attributes such as causal-effec… |
| https://openalex.org/W4385878593 | New Era of Artificial Intelligence in Education: Towards a Sustainable Multifaceted Revolution | 2023 | Sustainability | article | 780 | yes | Firuz Kamalov, David Santandreu Calonge, Ikhlaas Gurrib | Mainstream, Software deployment, Applications of artificial intelligence, Computer science, Conversation, Artificial intelligence, +9 more | https://doi.org/10.3390/su151612451 | The recent high performance of ChatGPT on several standardized academic tests has thrust the topic of artificial intelligence (AI) into the mainstream conversation about the future of education. As deep learning is poised to shift the teaching paradigm, it is essential to have a clear understanding of its effects on the current education system to ensure sustainable development and deployment of AI-driven technologies at schools and universities. This research aims to investigate the potential impact of AI on education through review and analysis of the existing literature across three major axes: applications, advantages, and challenges. Our review focuses on the use of artificial intelligence in collaborative teacher–student learning, intelligent tutoring systems, automated assessment, and personalized learning. We also report on the potential negative aspects, ethical issues, and pos… |
| https://openalex.org/W3176707157 | Deep learning for AI | 2021 | Communications of the ACM | article | 577 | yes | Yoshua Bengio, Yann LeCun, Geoffrey E. Hinton | Computer science, Artificial intelligence, Deep learning, Artificial neural network, Natural language processing, Cognitive science, +2 more | https://doi.org/10.1145/3448250 | How can neural networks learn the rich internal representations required for difficult tasks such as recognizing objects or understanding language? |
| https://openalex.org/W3034457371 | MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices | 2020 |  | preprint | 626 | yes | Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, Denny Zhou | Computer science, Bottleneck, Latency (audio), Task (project management), Inference, Phone, +10 more | https://doi.org/10.18653/v1/2020.acl-main.195 | Natural Language Processing (NLP) has recently achieved great success by using huge pre-trained models with hundreds of millions of parameters. However, these models suffer from heavy model sizes and high latency such that they cannot be deployed to resource-limited mobile devices. In this paper, we propose MobileBERT for compressing and accelerating the popular BERT model. Like the original BERT, MobileBERT is task-agnostic, that is, it can be generically applied to various downstream NLP tasks via simple fine-tuning. Basically, MobileBERT is a thin version of BERT_LARGE, while equipped with bottleneck structures and a carefully designed balance between self-attentions and feed-forward networks. To train MobileBERT, we first train a specially designed teacher model, an inverted-bottleneck incorporated BERT_LARGE model. Then, we conduct knowledge transfer from this teacher to MobileBERT… |
| https://openalex.org/W4312277530 | Short text automatic scoring system based on BERT-BiLSTM model | 2022 | JOURNAL OF SHENZHEN UNIVERSITY SCIENCE AND ENGINEERING | article | 7 | yes | Linzhong Xia, Jianfeng YE, Dean Luo, Mingxiang Guan, Jun Liu, Xuemei Cao | Computer science, Artificial intelligence, Natural language processing, Information retrieval | https://doi.org/10.3724/sp.j.1249.2022.03349 | Aiming at the problems of sparse features, polysemy of one word and less context related information in short text automatic scoring, a short text automatic scoring model based on bidirectional encoder representations from transformers - bidirectional long short-term memory (BERT-BiLSTM) is proposed. Firstly, the large-scale corpus is pre-trained with bidirectional encoder representations from transformers (BERT) language model to acquire the semantic features of the general language. Then the semantic features of short text and the semantics of keywords in a specific context are acquired through the short text data for the pre-fine tuning downstream specific tasks set pre-fined by BERT. And then the deep-seated context dependency is captured through bidirectional long short-term memory (BiLSTM). Finally, the obtained feature vectors are input into Softmax regression model for automatic… |
| https://openalex.org/W3176828726 | BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models | 2022 |  | preprint | 584 | yes | Elad Ben Zaken, Yoav Goldberg, Shauli Ravfogel | Computer science, Transformer, Language model, Artificial intelligence, Simple (philosophy), Process (computing), +13 more | https://doi.org/10.18653/v1/2022.acl-short.1 | We introduce BitFit, a sparse-finetuning method where only the bias-terms of the model (or a subset of them) are being modified. We show that with small-to-medium training data, applying BitFit on pre-trained BERT models is competitive with (and sometimes better than) fine-tuning the entire model. For larger data, the method is competitive with other sparse fine-tuning methods.Besides their practical utility, these findings are relevant for the question of understanding the commonly-used process of finetuning: they support the hypothesis that finetuning is mainly about exposing knowledge induced by language-modeling training, rather than learning new task-specific linguistic knowledge. |
| https://openalex.org/W4400062840 | Understanding state-of-the-art situation of transport planning strategies in earthquake-prone areas by using AI-supported literature review methodology | 2024 | Heliyon | review | 7 | yes | Ali Enes Dingil, Ondrej Přibyl | Variety (cybernetics), Originality, Emergency management, Computer science, Preparedness, Risk analysis (engineering), +8 more | https://doi.org/10.1016/j.heliyon.2024.e33645 |  |
| https://openalex.org/W4206095984 | Feature dimensionality reduction: a review | 2022 | Complex & Intelligent Systems | review | 659 | yes | Weikuan Jia, Meili Sun, Jian Lian, Sujuan Hou | Dimensionality reduction, Curse of dimensionality, Computer science, Artificial intelligence, Pattern recognition (psychology), Field (mathematics), +17 more | https://doi.org/10.1007/s40747-021-00637-x | Abstract As basic research, it has also received increasing attention from people that the “curse of dimensionality” will lead to increase the cost of data storage and computing; it also influences the efficiency and accuracy of dealing with problems. Feature dimensionality reduction as a key link in the process of pattern recognition has become one hot and difficulty spot in the field of pattern recognition, machine learning and data mining. It is one of the most challenging research fields, which has been favored by most of the scholars’ attention. How to implement “low loss” in the process of feature dimension reduction, keep the nature of the original data, find out the best mapping and get the optimal low dimensional data are the keys aims of the research. In this paper, two-dimensionality reduction methods, feature selection and feature extraction, are introduced; the current main… |
| https://openalex.org/W2951433247 | Evaluating Protein Transfer Learning with TAPE | 2019 |  | preprint | 672 | yes | Roshan Rao, Nicholas Bhattacharya, Neil Thomas, Yan Duan, Xi Chen, John Canny, Pieter Abbeel, Yun S. Song | Computer science, Artificial intelligence, Machine learning, Generalization, Transfer of learning, Set (abstract data type), +16 more | https://doi.org/10.1101/676825 | Abstract Protein modeling is an increasingly popular area of machine learning research. Semi-supervised learning has emerged as an important paradigm in protein modeling due to the high cost of acquiring supervised protein labels, but the current literature is fragmented when it comes to datasets and standardized evaluation techniques. To facilitate progress in this field, we introduce the Tasks Assessing Protein Embeddings (TAPE), a set of five biologically relevant semi-supervised learning tasks spread across different domains of protein biology. We curate tasks into specific training, validation, and test splits to ensure that each task tests biologically relevant generalization that transfers to real-life scenarios. We bench-mark a range of approaches to semi-supervised protein representation learning, which span recent work as well as canonical sequence learning techniques. We find… |
| https://openalex.org/W4225264859 | DeepLoc 2.0: multi-label subcellular localization prediction using protein language models | 2022 | Nucleic Acids Research | article | 638 | yes | Vineet Thumuluri, José Juan Almagro Armenteros, Alexander Rosenberg Johansen, Henrik Nielsen, Ole Winther | Interpretability, Biology, Sorting, Web server, Proteomics, Computational biology, +16 more | https://doi.org/10.1093/nar/gkac278 | Abstract The prediction of protein subcellular localization is of great relevance for proteomics research. Here, we propose an update to the popular tool DeepLoc with multi-localization prediction and improvements in both performance and interpretability. For training and validation, we curate eukaryotic and human multi-location protein datasets with stringent homology partitioning and enriched with sorting signal information compiled from the literature. We achieve state-of-the-art performance in DeepLoc 2.0 by using a pre-trained protein language model. It has the further advantage that it uses sequence input rather than relying on slower protein profiles. We provide two means of better interpretability: an attention output along the sequence and highly accurate prediction of nine different types of protein sorting signals. We find that the attention output correlates well with the po… |
| https://openalex.org/W4392209494 | Guided Transformer for Machine Translation: English to Hindi | 2023 |  | article | 3 | no | Akhilesh Bisht, Deepa Gupta, Shantipriya Parida | Machine translation, Transformer, Computer science, Hindi, Natural language processing, Translation (biology), +8 more | https://doi.org/10.1109/indicon59947.2023.10440876 | In the area of machine translation, the objective is to develop a model which can translate a piece of text from one language to another language. Large sized neural networks requires a large amount of training data to achieve a good performance level. The objective of this study is to build an explicit knowledge from the dependency parser, which can be integrated into transformer architecture. Adding an explicit knowledge to a neural machine translation network can enhance its learning process. The baseline transformer model and the proposed guided transformer model were tested on the same test data set of PMIndia Man Ki Baat session. With an addition of explicit knowledge, the proposed guided transformer model surpass the performance of baseline transformer model by +1.0 BLEU (approx.), +1.0 SacreBLEU (approx.), +1.48 Chrf, +1.44 Chrf+ and +1.39 Chrf++. |
| https://openalex.org/W2977261044 | Pre-Trained Language Model Transfer on Chinese Named Entity Recognition | 2019 |  | article | 6 | no | Huan Zhao, Mingquan Xu, Jie Cao | Computer science, Polysemy, Natural language processing, Artificial intelligence, Transformer, Language model, +17 more | https://doi.org/10.1109/hpcc/smartcity/dss.2019.00297 | The challenges of natural language processing (NLP) lie in the polysemy and insufficiency of human-labeled training data. Bidirectional Encoder Representations from Transformers (BERT) facilitates pre-training deep bidirectional representations on large-scale unannotated text on the web and has created state-of-the-art performance on various NLP tasks after simple fine-tuning. The representation and application of semantic knowledge are important steps in the entire process of NLP tasks. In this work, for the encoder of our model, we encode an input sequence into contextual representations using pre-trained language model and design a new model that combines neural network with BERT. Experimental results show that our method achieves new state-of-the-art. |
| https://openalex.org/W4312597583 | EEG Conformer: Convolutional Transformer for EEG Decoding and Visualization | 2022 | IEEE Transactions on Neural Systems and Rehabilitation Engineering | article | 665 | yes | Yonghao Song, Qingqing Zheng, Bingchuan Liu, Xiaorong Gao | Electroencephalography, Decoding methods, Transformer, Computer science, Visualization, Speech recognition, +8 more | https://doi.org/10.1109/tnsre.2022.3230250 | Due to the limited perceptual field, convolutional neural networks (CNN) only extract local temporal features and may fail to capture long-term dependencies for EEG decoding. In this paper, we propose a compact Convolutional Transformer, named EEG Conformer, to encapsulate local and global features in a unified EEG classification framework. Specifically, the convolution module learns the low-level local features throughout the one-dimensional temporal and spatial convolution layers. The self-attention module is straightforwardly connected to extract the global correlation within the local temporal features. Subsequently, the simple classifier module based on fully-connected layers is followed to predict the categories for EEG signals. To enhance interpretability, we also devise a visualization strategy to project the class activation mapping onto the brain topography. Finally, we have c… |
| https://openalex.org/W4403159822 | Toward Large-scale Spiking Neural Networks: A Comprehensive Survey and Future Directions | 2024 | arXiv (Cornell University) | preprint | 3 | yes | Yangfan Hu, Zheng Qian, Guoqi Li, Huajin Tang, Gang Pan | Scale (ratio), Artificial neural network, Data science, Computer science, Artificial intelligence, Geography, +1 more | http://arxiv.org/abs/2409.02111 | Deep learning has revolutionized artificial intelligence (AI), achieving remarkable progress in fields such as computer vision, speech recognition, and natural language processing. Moreover, the recent success of large language models (LLMs) has fueled a surge in research on large-scale neural networks. However, the escalating demand for computing resources and energy consumption has prompted the search for energy-efficient alternatives. Inspired by the human brain, spiking neural networks (SNNs) promise energy-efficient computation with event-driven spikes. To provide future directions toward building energy-efficient large SNN models, we present a survey of existing methods for developing deep spiking neural networks, with a focus on emerging Spiking Transformers. Our main contributions are as follows: (1) an overview of learning methods for deep spiking neural networks, categorized b… |
| https://openalex.org/W4401587429 | Integrated Method of Deep learning and Large Language Model in Speech Recognition | 2024 | Preprints.org | preprint | 4 | yes | Bo Guan, Jin Cao, Xingqi Wang, Zhuoyue Wang, Mingxiu Sui, Zixiang Wang | Computer science, Speech recognition, Language model, Word error rate, Cache language model, Artificial intelligence, +20 more | https://doi.org/10.20944/preprints202407.1520.v3 | This research aims to explore the integration method of deep learning and large language models in speech recognition to improve the system&amp;rsquo;s recognition accuracy and ability to handle complex contexts. Deep neural network (DNN), convolutional neural network (CNN), long short-term memory network (LSTM) and Transformer-based large language model are used to build an integrated acoustic and language model framework. Experiments on TIMIT, LibriSpeech and Common Voice datasets show that the ensemble model shows significant improvements in both word error rate (WER) and real-time factor (RTF) compared to traditional models. Especially in terms of adaptability to multiple languages and accent changes, the model shows superior performance. The conclusion shows that through technology integration, the performance of the speech recognition system in complex environments can be effectiv… |
| https://openalex.org/W4281703735 | Multimodal Classification of Safety-Report Observations | 2022 | Applied Sciences | article | 10 | yes | Georgios Paraskevopoulos, Petros Pistofidis, Georgios Banoutsos, Efthymios Georgiou, Vassilis Katsouros | Computer science, Categorization, Metadata, Modalities, Modality (human–computer interaction), Multimodal learning, +14 more | https://doi.org/10.3390/app12125781 | Modern businesses are obligated to conform to regulations to prevent physical injuries and ill health for anyone present on a site under their responsibility, such as customers, employees and visitors. Safety officers (SOs) are engineers, who perform site audits to businesses, record observations regarding possible safety issues and make appropriate recommendations. In this work, we develop a multimodal machine-learning architecture for the analysis and categorization of safety observations, given textual descriptions and images taken from the location sites. For this, we utilize a new multimodal dataset, Safety4All, which contains 5344 safety-related observations created by 86 SOs in 486 sites. An observation consists of a short issue description, written by the SOs, accompanied with images where the issue is shown, relevant metadata and a priority score. Our proposed architecture is b… |
| https://openalex.org/W3167683763 | GTAE: Graph Transformer–Based Auto-Encoders for Linguistic-Constrained Text Style Transfer | 2021 | ACM Transactions on Intelligent Systems and Technology | article | 10 | no | Yukai Shi, Sen Zhang, Chenxing Zhou, Xiaodan Liang, Xiaojun Yang, Liang Lin | Computer science, Transformer, Encoder, Artificial intelligence, Natural language processing, Sentence, +10 more | https://doi.org/10.1145/3448733 | Non-parallel text style transfer has attracted increasing research interests in recent years. Despite successes in transferring the style based on the encoder-decoder framework, current approaches still lack the ability to preserve the content and even logic of original sentences, mainly due to the large unconstrained model space or too simplified assumptions on latent embedding space. Since language itself is an intelligent product of humans with certain grammars and has a limited rule-based model space by its nature, relieving this problem requires reconciling the model capacity of deep neural networks with the intrinsic model constraints from human linguistic rules. To this end, we propose a method called Graph Transformer–based Auto-Encoder, which models a sentence as a linguistic graph and performs feature extraction and style transfer at the graph level, to maximally retain the co… |
| https://openalex.org/W4220978079 | Video captioning in Vietnamese using deep learning | 2022 | International Journal of Power Electronics and Drive Systems/International Jour… | article | 6 | yes | Dang Thi Phuc, Tran Quang Trieu, Nguyễn Văn Tính, Dau Sy Hieu | Closed captioning, Vietnamese, Computer science, Deep learning, Artificial intelligence, Transformer, +14 more | https://doi.org/10.11591/ijece.v12i3.pp3092-3103 | &lt;p&gt;&lt;span&gt;With the development of today's society, demand for applications using digital cameras jumps over year by year. However, analyzing large amounts of video data causes one of the most challenging issues. In addition to storing the data captured by the camera, intelligent systems are required to quickly analyze the data to correct important situations. In this paper, we use deep learning techniques to build automatic models that describe movements on video. To solve the problem, we use three deep learning models: sequence-to-sequence model based on recurrent neural network, sequence-to-sequence model with attention and transformer model. We evaluate the effectiveness of the approaches based on the results of three models. To train these models, we use microsoft research video description corpus (MSVD) dataset including 1970 videos and 85,550 captions translated into Vi… |
| https://openalex.org/W4378574344 | ChatGPT and Open-AI Models: A Preliminary Review | 2023 | Future Internet | review | 626 | yes | Konstantinos I. Roumeliotis, Nikolaos D. Tselikas | Computer science, Field (mathematics), Artificial intelligence, Process (computing), Reinforcement learning, Natural language understanding, +9 more | https://doi.org/10.3390/fi15060192 | According to numerous reports, ChatGPT represents a significant breakthrough in the field of artificial intelligence. ChatGPT is a pre-trained AI model designed to engage in natural language conversations, utilizing sophisticated techniques from Natural Language Processing (NLP), Supervised Learning, and Reinforcement Learning to comprehend and generate text comparable to human-generated text. This article provides an overview of the training process and fundamental functionality of ChatGPT, accompanied by a preliminary review of the relevant literature. Notably, this article presents the first comprehensive literature review of this technology at the time of publication, aiming to aggregate all the available pertinent articles to facilitate further developments in the field. Ultimately, the authors aim to offer an appraisal of the technology’s potential implications on existing knowled… |
| https://openalex.org/W3128592650 | Vision Transformers for Remote Sensing Image Classification | 2021 | Remote Sensing | article | 545 | yes | Yakoub Bazi, Laila Bashmal, Mohamad Mahmoud Al Rahhal, Reham Al-Dayil, Naif Al Ajlan | Computer science, Softmax function, Artificial intelligence, Pattern recognition (psychology), Embedding, Transformer, +12 more | https://doi.org/10.3390/rs13030516 | In this paper, we propose a remote-sensing scene-classification method based on vision transformers. These types of networks, which are now recognized as state-of-the-art models in natural language processing, do not rely on convolution layers as in standard convolutional neural networks (CNNs). Instead, they use multihead attention mechanisms as the main building block to derive long-range contextual relation between pixels in images. In a first step, the images under analysis are divided into patches, then converted to sequence by flattening and embedding. To keep information about the position, embedding position is added to these patches. Then, the resulting sequence is fed to several multihead attention layers for generating the final representation. At the classification stage, the first token sequence is fed to a softmax classification layer. To boost the classification performan… |
| https://openalex.org/W2922580172 | SemEval-2019 Task 6: Identifying and Categorizing Offensive Language in Social Media (OffensEval) | 2019 |  | article | 673 | yes | Marcos Zampieri, Shervin Malmasi, Preslav Nakov, Sara Rosenthal, Noura Farra, Ritesh Kumar | Offensive, SemEval, Task (project management), Computer science, Language identification, Natural language processing, +9 more | https://doi.org/10.18653/v1/s19-2010 | We present the results and the main findings of SemEval-2019 Task 6 on Identifying and Categorizing Offensive Language in Social Media (OffensEval). The task was based on a new dataset, the Offensive Language Identification Dataset (OLID), which contains over 14,000 English tweets, and it featured three sub-tasks. In sub-task A, systems were asked to discriminate between offensive and non-offensive posts. In sub-task B, systems had to identify the type of offensive content in the post. Finally, in sub-task C, systems had to detect the target of the offensive posts. OffensEval attracted a large number of participants and it was one of the most popular tasks in SemEval-2019. In total, nearly 800 teams signed up to participate in the task and 115 of them submitted results, which are presented and analyzed in this report. |
| https://openalex.org/W2900153411 | An abstract domain for certifying neural networks | 2019 | Proceedings of the ACM on Programming Languages | article | 586 | yes | Gagandeep Singh, Timon Gehr, Markus Püschel, Martin Vechev | Computer science, Artificial neural network, Affine transformation, Scalability, Robustness (evolution), Sigmoid function, +12 more | https://doi.org/10.1145/3290354 | We present a novel method for scalable and precise certification of deep neural networks. The key technical insight behind our approach is a new abstract domain which combines floating point polyhedra with intervals and is equipped with abstract transformers specifically tailored to the setting of neural networks. Concretely, we introduce new transformers for affine transforms, the rectified linear unit (ReLU), sigmoid, tanh, and maxpool functions. We implemented our method in a system called DeepPoly and evaluated it extensively on a range of datasets, neural architectures (including defended networks), and specifications. Our experimental results indicate that DeepPoly is more precise than prior work while scaling to large networks. We also show how to combine DeepPoly with a form of abstraction refinement based on trace partitioning. This enables us to prove, for the first time, the… |
| https://openalex.org/W3006552058 | Stress Test Evaluation of Transformer-based Models in Natural Language Understanding Tasks | 2020 | arXiv (Cornell University) | preprint | 6 | yes | Carlos Aspillaga, Andrés Carvallo, Vladimir Araujo | Transformer, Computer science, Robustness (evolution), Inference, Artificial intelligence, Adversarial system, +14 more | http://arxiv.org/abs/2002.06261 | There has been significant progress in recent years in the field of Natural Language Processing thanks to the introduction of the Transformer architecture. Current state-of-the-art models, via a large number of parameters and pre-training on massive text corpus, have shown impressive results on several downstream tasks. Many researchers have studied previous (non-Transformer) models to understand their actual behavior under different scenarios, showing that these models are taking advantage of clues or failures of datasets and that slight perturbations on the input data can severely reduce their performance. In contrast, recent models have not been systematically tested with adversarial-examples in order to show their robustness under severe stress conditions. For that reason, this work evaluates three Transformer-based models (RoBERTa, XLNet, and BERT) in Natural Language Inference (NL… |
| https://openalex.org/W3034266838 | Integrating Multimodal Information in Large Pretrained Transformers | 2020 |  | article | 562 | yes | Wasifur Rahman, Md. Kamrul Hasan, Sangwu Lee, AmirAli Bagher Zadeh, Chengfeng Mao, Louis–Philippe Morency, Ehsan Hoque | Computer science, Transformer, Artificial intelligence, Human–computer interaction, Natural language processing, Engineering, +2 more | https://doi.org/10.18653/v1/2020.acl-main.214 | Recent Transformer-based contextual word representations, including BERT and XLNet, have shown state-of-the-art performance in multiple disciplines within NLP. Fine-tuning the trained contextual models on task-specific datasets has been the key to achieving superior performance downstream. While fine-tuning these pre-trained models is straight-forward for lexical applications (applications with only language modality), it is not trivial for multimodal language (a growing area in NLP focused on modeling face-to-face communication). Pre-trained models don't have the necessary components to accept two extra modalities of vision and acoustic. In this paper, we proposed an attachment to BERT and XLNet called Multimodal Adaptation Gate (MAG). MAG allows BERT and XLNet to accept multimodal nonverbal data during fine-tuning. It does so by generating a shift to internal representation of BERT an… |
| https://openalex.org/W3118299338 | A Review on Linear Regression Comprehensive in Machine Learning | 2020 | Journal of Applied Science and Technology Trends | review | 1133 | yes | Dastan Hussen Maulud, Adnan Mohsin Abdulazeez | Proper linear model, Linear regression, Polynomial regression, Regression diagnostic, Regression analysis, Simple linear regression, +12 more | https://doi.org/10.38094/jastt1457 | Perhaps one of the most common and comprehensive statistical and machine learning algorithms are linear regression. Linear regression is used to find a linear relationship between one or more predictors. The linear regression has two types: simple regression and multiple regression (MLR). This paper discusses various works by different researchers on linear regression and polynomial regression and compares their performance using the best approach to optimize prediction and precision. Almost all of the articles analyzed in this review is focused on datasets; in order to determine a model's efficiency, it must be correlated with the actual values obtained for the explanatory variables. |
| https://openalex.org/W2970854433 | Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT | 2019 |  | article | 582 | yes | Shijie Wu, Mark Dredze | Computer science, Natural language processing, Joint (building), Artificial intelligence, Engineering, Architectural engineering | https://doi.org/10.18653/v1/d19-1077 | Shijie Wu, Mark Dredze. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019. |
| https://openalex.org/W3034617555 | A Novel Cascade Binary Tagging Framework for Relational Triple Extraction | 2020 |  | article | 592 | yes | Zhepei Wei, Jianlin Su, Yue Wang, Yuan Tian, Yi Chang | Computer science, Encoder, Sentence, Binary number, Cascade, Relationship extraction, +15 more | https://doi.org/10.18653/v1/2020.acl-main.136 | Extracting relational triples from unstructured text is crucial for large-scale knowledge graph construction. However, few existing works excel in solving the overlapping triple problem where multiple relational triples in the same sentence share the same entities. In this work, we introduce a fresh perspective to revisit the relational triple extraction task and propose a novel cascade binary tagging framework (CasRel) derived from a principled problem formulation. Instead of treating relations as discrete labels as in previous works, our new framework models relations as functions that map subjects to objects in a sentence, which naturally handles the overlapping problem. Experiments show that the CasRel framework already outperforms state-of-the-art methods even when its encoder module uses a randomly initialized BERT encoder, showing the power of the new tagging framework. It enjoys… |
| https://openalex.org/W3128412859 | Learning Modality-Specific Representations with Self-Supervised Multi-Task Learning for Multimodal Sentiment Analysis | 2021 | Proceedings of the AAAI Conference on Artificial Intelligence | article | 596 | yes | Wenmeng Yu, Hua Xu, Ziqi Yuan, Jiele Wu | Computer science, Consistency (knowledge bases), Artificial intelligence, Modality (human–computer interaction), Task (project management), Machine learning, +10 more | https://doi.org/10.1609/aaai.v35i12.17289 | Representation Learning is a significant and challenging task in multimodal learning. Effective modality representations should contain two parts of characteristics: the consistency and the difference. Due to the unified multimodal annota- tion, existing methods are restricted in capturing differenti- ated information. However, additional unimodal annotations are high time- and labor-cost. In this paper, we design a la- bel generation module based on the self-supervised learning strategy to acquire independent unimodal supervisions. Then, joint training the multimodal and uni-modal tasks to learn the consistency and difference, respectively. Moreover, dur- ing the training stage, we design a weight-adjustment strat- egy to balance the learning progress among different sub- tasks. That is to guide the subtasks to focus on samples with the larger difference between modality supervisions.… |
| https://openalex.org/W4313306264 | Sparse Graph Transformer With Contrastive Learning | 2022 | IEEE Transactions on Computational Social Systems | article | 9 | no | Chun-Yang Zhang, Wu-Peng Fang, Hai-Chun Cai, C. L. Philip Chen, Yue-Na Lin | Computer science, Theoretical computer science, Graph embedding, Artificial intelligence, Adjacency matrix, Transformer, +6 more | https://doi.org/10.1109/tcss.2022.3232117 | Information aggregation and propagation over networks via graph neural networks (GNNs) plays an important role in node or graph representation learning, which currently depend on the calculation with a fixed adjacency matrix, facing over-smoothing problem, and difficulty to stack multiple layers for high-level representations. In contrast, Transformer calculates an importance score for each node to learn its embedding via the attention mechanism and has achieved great successes in many natural language processing (NLP) and computer vision (CV) tasks. However, Transformer is inflexible to extend to graphs, as its input and output must have the same dimension. It will also become intractable to allocate attention over a large-scale graph due to distractions. Moreover, most graph Transformers are trained in supervised ways, which consume additional resources to annotate samples with potent… |
| https://openalex.org/W4387847256 | SANN: Programming Code Representation Using Attention Neural Network with Optimized Subtree Extraction | 2023 |  | article | 10 | no | Muntasir Hoq, Sushanth Reddy Chilla, Melika Ahmadi Ranjbar, Peter Brusilovsky, Bita Akram | Computer science, Interpretability, Correctness, Machine learning, Artificial intelligence, Code (set theory), +13 more | https://doi.org/10.1145/3583780.3615047 | Automated analysis of programming data using code representation methods offers valuable services for programmers, from code completion to clone detection to bug detection. Recent studies show the effectiveness of Abstract Syntax Trees (AST), pre-trained Transformer-based models, and graph-based embeddings in programming code representation. However, pre-trained large language models lack interpretability, while other embedding-based approaches struggle with extracting important information from large ASTs. This study proposes a novel Subtree-based Attention Neural Network (SANN) to address these gaps by integrating different components: an optimized sequential subtree extraction process using Genetic algorithm optimization, a two-way embedding approach, and an attention network. We investigate the effectiveness of SANN by applying it to two different tasks: program correctness predicti… |
| https://openalex.org/W3153469116 | CLIPScore: A Reference-free Evaluation Metric for Image Captioning | 2021 | Proceedings of the 2021 Conference on Empirical Methods in Natural Language Pro… | article | 794 | yes | Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, Yejin Choi | Closed captioning, Computer science, Metric (unit), Artificial intelligence, Information retrieval, Natural language processing, +6 more | https://doi.org/10.18653/v1/2021.emnlp-main.595 | Image captioning has conventionally relied on reference-based automatic evaluations, where machine captions are compared against captions written by humans. This is in contrast to the reference-free manner in which humans assess caption quality. In this paper, we report the surprising empirical finding that CLIP (Radford et al., 2021), a cross-modal model pretrained on 400M image+caption pairs from the web, can be used for robust automatic evaluation of image captioning without the need for references. Experiments spanning several corpora demonstrate that our new reference-free metric, CLIPScore, achieves the highest correlation with human judgements, outperforming existing reference-based metrics like CIDEr and SPICE. Information gain experiments demonstrate that CLIPScore, with its tight focus on image-text compatibility, is complementary to existing reference-based metrics that empha… |
| https://openalex.org/W3104609094 | Generating Radiology Reports via Memory-driven Transformer | 2020 |  | article | 515 | yes | Zhihong Chen, Yan Song, Tsung‐Hui Chang, Xiang Wan | Computer science, Workload, Artificial intelligence, Transformer, Medical imaging, Machine learning, +8 more | https://doi.org/10.18653/v1/2020.emnlp-main.112 | Medical imaging is frequently used in clinical practice and trials for diagnosis and treatment. Writing imaging reports is time-consuming and can be error-prone for inexperienced radiologists. Therefore, automatically generating radiology reports is highly desired to lighten the workload of radiologists and accordingly promote clinical automation, which is an essential task to apply artificial intelligence to the medical domain. In this paper, we propose to generate radiology reports with memory-driven Transformer, where a relational memory is designed to record key information of the generation process and a memory-driven conditional layer normalization is applied to incorporating the memory into the decoder of Transformer. Experimental results on two prevailing radiology report datasets, IU X-Ray and MIMIC-CXR, show that our proposed approach outperforms previous models with respect t… |
| https://openalex.org/W3197076153 | Vision Transformers For Weeds and Crops Classification Of High Resolution UAV Images | 2021 | arXiv (Cornell University) | preprint | 8 | yes | Reenul Reedha, Eric Dericquebourg, Raphaël Canals, Adel Hafiane | Computer science, Convolutional neural network, Deep learning, Artificial intelligence, Precision agriculture, Transformer, +10 more | http://arxiv.org/abs/2109.02716 | Crop and weed monitoring is an important challenge for agriculture and food production nowadays. Thanks to recent advances in data acquisition and computation technologies, agriculture is evolving to a more smart and precision farming to meet with the high yield and high quality crop production. Classification and recognition in Unmanned Aerial Vehicles (UAV) images are important phases for crop monitoring. Advances in deep learning models relying on Convolutional Neural Network (CNN) have achieved high performances in image classification in the agricultural domain. Despite the success of this architecture, CNN still faces many challenges such as high computation cost, the need of large labelled datasets,... Natural language processing's transformer architecture can be an alternative approach to deal with CNN's limitations. Making use of the self-attention paradigm, Vision Transformer… |
| https://openalex.org/W4393065402 | A survey on large language model based autonomous agents | 2024 | Frontiers of Computer Science | article | 805 | yes | Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, +5 more | Computer science, Artificial intelligence | https://doi.org/10.1007/s11704-024-40231-1 | Abstract Autonomous agents have long been a research focus in academic and industry communities. Previous research often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from human learning processes, and makes the agents hard to achieve human-like decisions. Recently, through the acquisition of vast amounts of Web knowledge, large language models (LLMs) have shown potential in human-level intelligence, leading to a surge in research on LLM-based autonomous agents. In this paper, we present a comprehensive survey of these studies, delivering a systematic review of LLM-based autonomous agents from a holistic perspective. We first discuss the construction of LLM-based autonomous agents, proposing a unified framework that encompasses much of previous work. Then, we present a overview of the diverse applications of LLM-based autono… |
| https://openalex.org/W4407389239 | Context-Aware Tomato Leaf Disease Detection Using Deep Learning in an Operational Framework | 2025 | Electronics | article | 12 | yes | Divas Karimanzira | Context (archaeology), Deep learning, Computer science, Artificial intelligence, Disease, Machine learning, +4 more | https://doi.org/10.3390/electronics14040661 | Tomato cultivation is a vital agricultural practice worldwide, yet it faces significant challenges due to various diseases that adversely affect crop yield and quality. This paper presents a novel tomato disease detection system within an operational framework that leverages an innovative deep learning-based classifier, specifically a Vision Transformer (ViT) integrated with cascaded group attention (CGA) and a modified Focaler-CIoU (Complete Intersection over Union) loss function. The proposed method aims to enhance the accuracy and robustness of disease detection by effectively capturing both local and global contextual information while addressing the challenges of sample imbalance in the dataset. To improve interpretability, we integrate Explainable Artificial Intelligence (XAI) techniques, enabling users to understand the rationale behind the model’s classifications. Additionally,… |
| https://openalex.org/W4366091323 | Deep learning modelling techniques: current progress, applications, advantages, and challenges | 2023 | Artificial Intelligence Review | article | 759 | yes | Shams Forruque Ahmed, Md. Sakib Bin Alam, Maruf Hassan, Mahtabin Rodela Rozbu, Taoseef Ishtiak, Nazifa Rafa, M. Mofijur, A. B. M. Shawkat Ali, +1 more | Computer science, Deep learning, Artificial intelligence, Machine learning, Field (mathematics), Convolutional neural network, +7 more | https://doi.org/10.1007/s10462-023-10466-8 | Abstract Deep learning (DL) is revolutionizing evidence-based decision-making techniques that can be applied across various sectors. Specifically, it possesses the ability to utilize two or more levels of non-linear feature transformation of the given data via representation learning in order to overcome limitations posed by large datasets. As a multidisciplinary field that is still in its nascent phase, articles that survey DL architectures encompassing the full scope of the field are rather limited. Thus, this paper comprehensively reviews the state-of-art DL modelling techniques and provides insights into their advantages and challenges. It was found that many of the models exhibit a highly domain-specific efficiency and could be trained by two or more methods. However, training DL models can be very time-consuming, expensive, and requires huge samples for better accuracy. Since DL i… |
| https://openalex.org/W2987283559 | Momentum Contrast for Unsupervised Visual Representation Learning | 2019 | arXiv (Cornell University) | preprint | 1007 | yes | Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, Ross Girshick | Pascal (unit), Contrast (vision), Artificial intelligence, Computer science, Unsupervised learning, Segmentation, +10 more | http://arxiv.org/abs/1911.05722 | We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks. |
| https://openalex.org/W4389519958 | Task-Attentive Transformer Architecture for Continual Learning of Vision-and-Language Tasks Using Knowledge Distillation | 2023 |  | article | 4 | yes | Yuliang Cai, Jesse Thomason, Mohammad Rostami | Computer science, Forgetting, Transformer, Scalability, Artificial intelligence, Artificial neural network, +19 more | https://doi.org/10.18653/v1/2023.findings-emnlp.466 | The size and the computational load of fine-tuning large-scale pre-trained neural network are becoming two major obstacles in adopting machine learning in many applications. Continual learning (CL) can serve as a remedy through enabling knowledge-transfer across sequentially arriving tasks which relaxes the need to fine-tune all network weights from scratch. However, existing CL algorithms primarily consider learning unimodal vision-only or language-only tasks. We develop a transformer-based CL architecture for learning bimodal vision-and-language tasks based on increasing the number of the learnable parameters dynamically and using knowledge distillation. The new additional parameters are used to specialize the network for each task. Our approach enables sharing information between the tasks while addressing the challenge of catastrophic forgetting. Our approach is scalable learning to… |
| https://openalex.org/W4392408986 | A Comprehensive Survey of Machine Translation Approaches | 2023 |  | article | 7 | no | Sahana Ganesh, Vedant Dhotre, Pranav Patil, Dipti Pawade | Computer science, Machine translation, Translation (biology), Artificial intelligence, Natural language processing, Chemistry, +3 more | https://doi.org/10.1109/icast59062.2023.10455003 | The field of machine translation (MT) has advanced over the years, with three major approaches dominating the field: Rule-Based Machine Translation (RBMT), Statistical Machine Translation (SMT), and Neural Machine Translation (NMT). This research paper provides an extensive review of these approaches, including their development, advantages, and disadvantages. Initially, RBMT represented a cutting-edge technology, performing translation using dictionaries and explicit language rules. However, dealing with intricate linguistic patterns was severely hindered by its rigidity and limited scalability, which gave rise to SMT. In order to provide more flexible translations, SMT used statistical models to extract patterns from large multilingual corpora. This method became well-known since it was data-driven, but it still had problems with domain adaptation and a lack of high-quality parallel d… |
| https://openalex.org/W4385416665 | The Power of Generative AI: A Review of Requirements, Models, Input–Output Formats, Evaluation Metrics, and Challenges | 2023 | Future Internet | review | 496 | yes | Ajay Bandi, Pydi Venkata Satya Ramesh Adapa, Yudu Eswar Vinay Pratap Kumar Kuchi | Computer science, Generative grammar, Field (mathematics), Taxonomy (biology), Artificial intelligence, Generative Design, +16 more | https://doi.org/10.3390/fi15080260 | Generative artificial intelligence (AI) has emerged as a powerful technology with numerous applications in various domains. There is a need to identify the requirements and evaluation metrics for generative AI models designed for specific tasks. The purpose of the research aims to investigate the fundamental aspects of generative AI systems, including their requirements, models, input–output formats, and evaluation metrics. The study addresses key research questions and presents comprehensive insights to guide researchers, developers, and practitioners in the field. Firstly, the requirements necessary for implementing generative AI systems are examined and categorized into three distinct categories: hardware, software, and user experience. Furthermore, the study explores the different types of generative AI models described in the literature by presenting a taxonomy based on architectur… |
| https://openalex.org/W4412769411 | A scoping review of artificial intelligence applications in clinical trial risk assessment | 2025 | npj Digital Medicine | review | 11 | yes | Douglas Teodoro, Nona Naderi, Anthony Yazdani, Boya Zhang, Alban Bornet | Artificial intelligence, Machine learning, Clinical trial, Risk assessment, Computer science, Artificial neural network, +4 more | https://doi.org/10.1038/s41746-025-01886-7 |  |
| https://openalex.org/W4288723191 | Lecture Notes on Neural Information Retrieval | 2022 | arXiv (Cornell University) | preprint | 5 | yes | Nicola Tonellotto | Computer science, Question answering, Focus (optics), Artificial neural network, Artificial intelligence, Transformer, +17 more | http://arxiv.org/abs/2207.13443 | These lecture notes focus on the recent advancements in neural information retrieval, with particular emphasis on the systems and models exploiting transformer networks. These networks, originally proposed by Google in 2017, have seen a large success in many natural language processing and information retrieval tasks. While there are many fantastic textbook on information retrieval and natural language processing as well as specialised books for a more advanced audience, these lecture notes target people aiming at developing a basic understanding of the main information retrieval techniques and approaches based on deep learning. These notes have been prepared for a IR graduate course of the MSc program in Artificial Intelligence and Data Engineering at the University of Pisa, Italy. |
| https://openalex.org/W4297144130 | GeoBERTSegmenter: Word Segmentation of Chinese Texts in the Geoscience Domain Using the Improved BERT Model | 2022 | Earth and Space Science | article | 8 | yes | Dongqi Wei, Zhihao Liu, Dexin Xu, Kai Ma, Liufeng Tao, Zhong Xie, Qinjun Qiu, Shengyong Pan | Computer science, Natural language processing, Segmentation, Artificial intelligence, Word (group theory), Deep learning, +13 more | https://doi.org/10.1029/2022ea002511 | Abstract Unlike English, there is no natural separator‐like gap between words in Chinese, which makes Chinese word segmentation (CWS) a difficult information processing problem. At present, geological texts contain a large number of unregistered geological terms, and the existing rule‐based methods and machine‐learning and deep learning algorithms still cannot be used to solve the problem of word segmentation in geosciences, especially for the large number of unregistered words. In this study, we propose GeoBERTSegmenter, which is a GeoBERT‐based (Geoscience‐Bidirectional Encoder Representation from Transformers) CWS model that is specifically designed with various linguistic irregularities in mind. In this method, a general model is extended to a BERT bidirectional recurrent neural network (BiLSTM) and conditional random field (GeoBERT + BiLSTM + CRF) model with a number of features de… |
| https://openalex.org/W4391545860 | Context-Aware Transfer Learning Approach to Detect Informative Social Media Content for Disaster Management | 2024 | International Journal of Advanced Computer Science and Applications | article | 8 | yes | Saima Saleem, Monica Mehrotra | Computer science, Social media, Context (archaeology), Transfer of learning, Content (measure theory), Artificial intelligence, +6 more | https://doi.org/10.14569/ijacsa.2024.0150167 | In the wake of disasters, timely access to accurate information about on-the-ground situation is crucial for effective disaster response. In this regard, social media (SM) like Twitter have emerged as an invaluable source of real-time user-generated data during such events. However, accurately detecting informative content from large amounts of unstructured user-generated data under such time-sensitive circumstances remains a challenging task. Existing methods predominantly rely on non-contextual language models, which fail to accurately capture the intricate context and linguistic nuances within the disaster-related tweets. While some recent studies have explored context-aware methods, they are based on computationally demanding transformer architectures. To strike a balance between effectiveness and computational efficiency, this study introduces a new context-aware transfer learning… |
| https://openalex.org/W4225983788 | Textual Adversarial Training of Machine Learning Model for Resistance to Adversarial Examples | 2022 | Security and Communication Networks | article | 7 | yes | Hyun Kwon, Sanghyun Lee | Adversarial system, Computer science, Artificial intelligence, Natural language processing, Inference, Context (archaeology), +10 more | https://doi.org/10.1155/2022/4511510 | Deep neural networks provide good performance for image recognition, speech recognition, text recognition, and pattern recognition. However, such networks are vulnerable to attack by adversarial examples. Adversarial examples are created by adding a small amount of noise to an original sample in such a way that no problem is perceptible to humans, yet the sample will be incorrectly recognized by a model. Adversarial examples have been studied mainly in the context of images, but research has expanded to include the text domain. In the textual context, an adversarial example is a sample of text in which certain important words have been changed so that the sample will be misclassified by a model even though to humans it is the same as the original text in terms of meaning and grammar. In the text domain, there have been relatively few studies on defenses against adversarial examples comp… |
| https://openalex.org/W4385801617 | Detection of SQL Injection and Cross-Site Scripting Based on Multi-Model CNN Combined with Bidirectional GRU and Multi-Head Self-Attention | 2023 |  | article | 6 | no | Wei-Chun Hsiao, Chih‐Hung Wang | Computer science, SQL injection, Cross-site scripting, Scripting language, SQL, Artificial intelligence, +16 more | https://doi.org/10.1109/iccci59363.2023.10210155 | Both SQL injection and cross-site scripting (XSS) are critical threats in the field of web application and API security. According to the literatures and our experiments, use individual deep learning architecture such as DNN (Dense Neural Network), LSTM (Long short-term memory), RNN (Recurrent Neural Network) or transformer to detect SQL injection and XSS that requires much time and large amounts of data for training. For overcoming the above issues to balance the training performance and detection accuracy, we integrate CNN and GRU with the attention mechanism inspired by modern language models to construct an innovative and efficient detection system that can achieve higher accuracy, smaller training dataset and shorter training time. Moreover, the experimental results were even better than other baseline techniques and close to what we had anticipated. |
| https://openalex.org/W4226289976 | A Survey on Dynamic Neural Networks for Natural Language Processing | 2022 | arXiv (Cornell University) | preprint | 3 | yes | Canwen Xu, Julian McAuley | Computer science, Artificial neural network, Computation, Inference, Artificial intelligence, Scaling, +10 more | http://arxiv.org/abs/2202.07101 | Effectively scaling large Transformer models is a main driver of recent advances in natural language processing. Dynamic neural networks, as an emerging research direction, are capable of scaling up neural networks with sub-linear increases in computation and time by dynamically adjusting their computational path based on the input. Dynamic neural networks could be a promising solution to the growing parameter numbers of pretrained language models, allowing both model pretraining with trillions of parameters and faster inference on mobile devices. In this survey, we summarize progress of three types of dynamic neural networks in NLP: skimming, mixture of experts, and early exit. We also highlight current challenges in dynamic neural networks and directions for future research. |
| https://openalex.org/W4394581391 | Distantly Supervised Explainable Stance Detection via Chain-of-Thought Supervision | 2024 | Mathematics | article | 4 | yes | Daijun Ding, Genan Dai, Cheng Peng, Xiaojiang Peng, Bowen Zhang, Hu Huang | Interpretability, Computer science, Transformer, Artificial intelligence, Machine learning, Classifier (UML), +8 more | https://doi.org/10.3390/math12071119 | Investigating public attitudes on social media is crucial for opinion mining systems. Stance detection aims to predict the attitude towards a specific target expressed in a text. However, effective neural stance detectors require substantial training data, which are challenging to curate due to the dynamic nature of social media. Moreover, deep neural networks (DNNs) lack explainability, rendering them unsuitable for scenarios requiring explanations. We propose a distantly supervised explainable stance detection framework (DS-ESD), comprising an instruction-based chain-of-thought (CoT) method, a generative network, and a transformer-based stance predictor. The CoT method employs prompt templates to extract stance detection explanations from a very large language model (VLLM). The generative network learns the input-explanation mapping, and a transformer-based stance classifier is traine… |
| https://openalex.org/W3179485843 | Language models enable zero-shot prediction of the effects of mutations on protein function | 2021 |  | preprint | 631 | yes | Joshua Meier, Roshan Rao, Robert Verkuil, Jason Liu, Tom Sercu, Alexander Rives | Sequence (biology), Inference, Computer science, Variation (astronomy), Function (biology), Task (project management), +13 more | https://doi.org/10.1101/2021.07.09.450648 | Abstract Modeling the effect of sequence variation on function is a fundamental problem for understanding and designing proteins. Since evolution encodes information about function into patterns in protein sequences, unsupervised models of variant effects can be learned from sequence data. The approach to date has been to fit a model to a family of related sequences. The conventional setting is limited, since a new model must be trained for each prediction task. We show that using only zero-shot inference, without any supervision from experimental data or additional training, protein language models capture the functional effects of sequence variation, performing at state-of-the-art. |
| https://openalex.org/W3104415840 | LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention | 2020 |  | article | 548 | yes | Ikuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda, Yūji Matsumoto | Computer science, Natural language processing, Transformer, Question answering, Artificial intelligence, Entity linking, +11 more | https://doi.org/10.18653/v1/2020.emnlp-main.523 | Entity representations are useful in natural language tasks involving entities. In this paper, we propose new pretrained contextualized representations of words and entities based on the bidirectional transformer. The proposed model treats words and entities in a given text as independent tokens, and outputs contextualized representations of them. Our model is trained using a new pretraining task based on the masked language model of BERT. The task involves predicting randomly masked words and entities in a large entity-annotated corpus retrieved from Wikipedia. We also propose an entity-aware self-attention mechanism that is an extension of the self-attention mechanism of the transformer, and considers the types of tokens (words or entities) when computing attention scores. The proposed model achieves impressive empirical performance on a wide range of entity-related tasks. In particul… |
| https://openalex.org/W4297677352 | A multiclass Q-NLP sentiment analysis experiment using DisCoCat | 2022 | arXiv (Cornell University) | preprint | 6 | yes | Victor D. Martínez, Guilhaume Leroy-Meline | Sentiment analysis, Computer science, Artificial intelligence, Scalability, Natural language processing, Transformer, +9 more | http://arxiv.org/abs/2209.03152 | Sentiment analysis is a branch of Natural Language Processing (NLP) which goal is to assign sentiments or emotions to particular sentences or words. Performing this task is particularly useful for companies wishing to take into account customer feedback through chatbots or verbatim. This has been done extensively in the literature using various approaches, ranging from simple models to deep transformer neural networks. In this paper, we will tackle sentiment analysis in the Noisy Intermediate Scale Computing (NISQ) era, using the DisCoCat model of language. We will first present the basics of quantum computing and the DisCoCat model. This will enable us to define a general framework to perform NLP tasks on a quantum computer. We will then extend the two-class classification that was performed by Lorenz et al. (2021) to a four-class sentiment analysis experiment on a much larger dataset,… |
| https://openalex.org/W4304080405 | SER30K | 2022 | Proceedings of the 30th ACM International Conference on Multimedia | article | 13 | no | Shengzhe Liu, Xin Zhang, Jufeng Yang | Computer science, Convolutional neural network, Artificial intelligence, Popularity, Focus (optics), Facial expression, +8 more | https://doi.org/10.1145/3503161.3548407 | With the popularity of instant messaging applications, online chatting plays an essential role in our daily life. The prevailing use of stickers to express emotions in online chatting leads to the necessity of multimodal sticker emotion recognition. Considering the lack of sticker emotion data, we collect a large-scale sticker emotion recognition dataset named SER30K. It consists of a total of 1,887 sticker themes with total 30,739 sticker images. Some commonly used images, such as realistic images and facial expression images, have been well studied in the field of emotion analysis. However, it is still challenging to understand the emotion of sticker images. Since the characteristics in stickers from the same theme are similar, we can only accurately predict the emotion by capturing the local information (e.g., expressions, poses) and understanding the global information (e.g., relati… |
| https://openalex.org/W4387606006 | MMBERT: a unified framework for biomedical named entity recognition | 2023 | Medical & Biological Engineering & Computing | article | 9 | no | Lei Fu, Zuquan Weng, Jiheng Zhang, Haihe Xie, Yiqing Cao | Named-entity recognition, Computer science, Biomedical text mining, Convolutional neural network, Artificial intelligence, Natural language processing, +16 more | https://doi.org/10.1007/s11517-023-02934-8 |  |
| https://openalex.org/W4396707288 | Fault transformer: An automatic fault detection algorithm on seismic images using a transformer enhanced neural network | 2024 | Interpretation | article | 4 | no | Tong Zhou, Yue Ma, Yuhan Sui, Nasher M. AlBinHassan | Transformer, Artificial neural network, Computer science, Algorithm, Artificial intelligence, Fault detection and isolation, +7 more | https://doi.org/10.1190/int-2023-0120.1 | Abstract Seismic fault detection is a key step in seismic interpretation and reservoir characterization that often requires a large amount of human labor and interpretation time. Therefore, automatic seismic fault detection is critical for improving the efficiency of seismic data processing and interpretation. Existing artificial intelligence methods are mostly based on convolutional neural networks with a U-shaped encoder-decoder structure, known as U-net. However, the convolution is limited in modeling long-range correlative features. Instead, transformers, using self-attention mechanisms, avoid the local nature of the convolution, which has the potential to extract long-distance correlations. Transformers are proven to perform well in natural language processing, image classification, and segmentation tasks in precision and recall. Here, we develop a new deep neural network with tran… |
| https://openalex.org/W4382394253 | Pre-Training MLM Using Bert for the Albanian Language | 2023 | SEEU Review | article | 4 | yes | Labehat Kryeziu, Visar Shehu | Computer science, Transformer, Artificial intelligence, Natural language processing, Language model, Human language, +11 more | https://doi.org/10.2478/seeur-2023-0035 | Abstract Knowing that language is often used as a classifier of human intelligence and the development of systems that understand human language remains a challenge all the time (Kryeziu &amp; Shehu, 2022). Natural Language Processing is a very active field of study, where transformers have a key role. Transformers function based on neural networks and they are increasingly showing promising results. One of the first major contributions to transfer learning in Natural Language Processing was the use of pre-trained word embeddings in 2010 (Joseph, Lev, &amp; Yoshua, 2010). Pre-trained models like ELMo (Matthew, et al., 2018) and BERT (Delvin, et al., 2019) are trained on large corpora of unlabeled text and as a result learning from text representations has achieved good performance on many of the underlying tasks on datasets from different domains. Pre-training in the language model has… |
| https://openalex.org/W4206693420 | Medical image segmentation using deep learning: A survey | 2022 | IET Image Processing | article | 708 | yes | Risheng Wang, Tao Lei, Ruixia Cui, Bingtao Zhang, Hongying Meng, Asoke K. Nandi | Computer science, Artificial intelligence, Image segmentation, Segmentation, Computer vision, Deep learning, +3 more | https://doi.org/10.1049/ipr2.12419 | Abstract Deep learning has been widely used for medical image segmentation and a large number of papers has been presented recording the success of deep learning in the field. A comprehensive thematic survey on medical image segmentation using deep learning techniques is presented. This paper makes two original contributions. Firstly, compared to traditional surveys that directly divide literatures of deep learning on medical image segmentation into many groups and introduce literatures in detail for each group, we classify currently popular literatures according to a multi‐level structure from coarse to fine. Secondly, this paper focuses on supervised and weakly supervised learning approaches, without including unsupervised approaches since they have been introduced in many old surveys and they are not popular currently. For supervised learning approaches, we analyse literatures in thr… |
| https://openalex.org/W4408296193 | Text-Guided Synthesis in Medical Multimedia Retrieval: A Framework for Enhanced Colonoscopy Image Classification and Segmentation | 2025 | Algorithms | article | 8 | yes | Ojonugwa Oluwafemi Ejiga Peter, Opeyemi Adeniran, Adetokunbo MacGregor John-Otumu, Fahmi Khalifa, Md Mahmudur Rahman | Computer science, Colonoscopy, Multimedia, Segmentation, Image (mathematics), Image retrieval, +6 more | https://doi.org/10.3390/a18030155 | The lack of extensive, varied, and thoroughly annotated datasets impedes the advancement of artificial intelligence (AI) for medical applications, especially colorectal cancer detection. Models trained with limited diversity often display biases, especially when utilized on disadvantaged groups. Generative models (e.g., DALL-E 2, Vector-Quantized Generative Adversarial Network (VQ-GAN)) have been used to generate images but not colonoscopy data for intelligent data augmentation. This study developed an effective method for producing synthetic colonoscopy image data, which can be used to train advanced medical diagnostic models for robust colorectal cancer detection and treatment. Text-to-image synthesis was performed using fine-tuned Visual Large Language Models (LLMs). Stable Diffusion and DreamBooth Low-Rank Adaptation produce images that look authentic, with an average Inception scor… |
| https://openalex.org/W4389543594 | Classification of Patient Portal Messages with BERT-based Language Models | 2023 |  | article | 8 | no | Ren Yang, Dezhi Wu, Aditya Khurana, George Mastorakos, Sunyang Fu, Nansu Zong, Jungwei Fan, Hongfang Liu, +1 more | Computer science, Triage, Artificial intelligence, Workload, Natural language processing, Patient portal, +13 more | https://doi.org/10.1109/ichi57859.2023.00033 | The patient portal is a secure platform that enables patients to conveniently access to their medical records and communicate with and seek support from their care teams for various healthcare issues. The care teams are receiving a rising number of patient portal messages (PPMs) generated by patients as the increase of patient engagement in patient portal, especially during the COVID-19 pandemic. It opens great opportunities to develop artificial intelligence (AI) solutions to supports the care teams and reduce their workload. Previous studies have shown the potential of machine learning and natural language processing (NLP) for classification of PPMs to automate message triage. Recent progress in neural network architectures (e.g., the Transformer) and the emergence of large-scale pre-trained language models are pushing the state-of-the-art for multiple NLP tasks including text classif… |
| https://openalex.org/W4281704015 | jTrans: Jump-Aware Transformer for Binary Code Similarity | 2022 | arXiv (Cornell University) | preprint | 7 | yes | Hao Wang, Wenjie Qu, Gilad Katz, Wenyu Zhu, Zeyu Gao, Han Qiu, Jianwei Zhuge, Chao Zhang | Computer science, Transformer, Binary number, Binary code, Code (set theory), Control flow, +11 more | http://arxiv.org/abs/2205.12713 | Binary code similarity detection (BCSD) has important applications in various fields such as vulnerability detection, software component analysis, and reverse engineering. Recent studies have shown that deep neural networks (DNNs) can comprehend instructions or control-flow graphs (CFG) of binary code and support BCSD. In this study, we propose a novel Transformer-based approach, namely jTrans, to learn representations of binary code. It is the first solution that embeds control flow information of binary code into Transformer-based language models, by using a novel jump-aware representation of the analyzed binaries and a newly-designed pre-training task. Additionally, we release to the community a newly-created large dataset of binaries, BinaryCorp, which is the most diverse to date. Evaluation results show that jTrans outperforms state-of-the-art (SOTA) approaches on this more challen… |
| https://openalex.org/W4361982879 | An Investigation of Structures Responsible for Gender Bias in BERT and DistilBERT | 2023 | Lecture notes in computer science | book-chapter | 9 | yes | Thibaud Leteno, Antoine Gourru, Charlotte Laclau, Christophe Gravier | Computer science, Inference, Context (archaeology), Transformer, Language model, Artificial intelligence, +17 more | http://arxiv.org/abs/2401.06495 |  |
| https://openalex.org/W4401456610 | Perplexity of utterances in untreated first-episode psychosis: an ultra–high field MRI dynamic causal modelling study of the semantic network | 2024 | Journal of Psychiatry and Neuroscience | article | 12 | yes | María Francisca Alonso-Sánchez, Wolfram Hinzen, Rui He, Joseph S. Gati, Lena Palaniyappan | Perplexity, Psychosis, Psychology, Natural language processing, Artificial intelligence, Computer science, +2 more | https://doi.org/10.1503/jpn.240031 | Background: Psychosis involves a distortion of thought content, which is partly reflected in anomalous ways in which words are semantically connected into utterances in speech. We sought to explore how these linguistic anomalies are realized through putative circuit-level abnormalities in the brain’s semantic network. Methods: Using a computational large-language model, Bidirectional Encoder Representations from Transformers (BERT), we quantified the contextual expectedness of a given word sequence (perplexity) across 180 samples obtained from descriptions of 3 pictures by patients with first-episode schizophrenia (FES) and controls matched for age, parental social status, and sex, scanned with 7 T ultra–high field functional magnetic resonance imaging (fMRI). Subsequently, perplexity was used to parametrize a spectral dynamic causal model (DCM) of the effective connectivity within (int… |
| https://openalex.org/W3037032032 | Energy and Policy Considerations for Modern Deep Learning Research | 2020 | Proceedings of the AAAI Conference on Artificial Intelligence | article | 614 | yes | Emma Strubell, Ananya Ganesh, Andrew McCallum | Computer science, Artificial intelligence, Computation, Field (mathematics), Machine learning, Artificial neural network, +9 more | https://doi.org/10.1609/aaai.v34i09.7123 | The field of artificial intelligence has experienced a dramatic methodological shift towards large neural networks trained on plentiful data. This shift has been fueled by recent advances in hardware and techniques enabling remarkable levels of computation, resulting in impressive advances in AI across many applications. However, the massive computation required to obtain these exciting results is costly both financially, due to the price of specialized hardware and electricity or cloud compute time, and to the environment, as a result of non-renewable energy used to fuel modern tensor processing hardware. In a paper published this year at ACL, we brought this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training and tuning neural network models for NLP (Strubell, Ganesh, and McCallum 2019). In this extended abstract, we b… |
| https://openalex.org/W3109530205 | Applying a Hybrid Sequential Model to Chinese Sentence Correction | 2020 | Symmetry | article | 5 | yes | Jun Chen, Xanno Kharis Sigalingging, Jenq‐Shiou Leu, Jun‐ichi Takada | Computer science, Sentence, Artificial intelligence, Transformer, Inference, Recurrent neural network, +13 more | https://doi.org/10.3390/sym12121939 | In recent years, Chinese has become one of the most popular languages globally. The demand for automatic Chinese sentence correction has gradually increased. This research can be adopted to Chinese language learning to reduce the cost of learning and feedback time, and help writers check for wrong words. The traditional way to do Chinese sentence correction is to check if the word exists in the predefined dictionary. However, this kind of method cannot deal with semantic error. As deep learning becomes popular, an artificial neural network can be applied to understand the sentence’s context to correct the semantic error. However, there are still many issues that need to be discussed. For example, the accuracy and the computation time required to correct a sentence are still lacking, so maybe it is still not the time to adopt the deep learning based Chinese sentence correction system to… |
| https://openalex.org/W4386392966 | Ten Years of Generative Adversarial Nets (GANs): A survey of the state-of-the-art | 2023 | arXiv (Cornell University) | preprint | 8 | yes | Tanujit Chakraborty, Ujjwal Reddy K S, Shraddha M. Naik, Madhurima Panja, Bayapureddy Manvitha | Generative grammar, Computer science, Minimax, Discriminative model, Generative adversarial network, Artificial intelligence, +9 more | http://arxiv.org/abs/2308.16316 | Since their inception in 2014, Generative Adversarial Networks (GANs) have rapidly emerged as powerful tools for generating realistic and diverse data across various domains, including computer vision and other applied areas. Consisting of a discriminative network and a generative network engaged in a Minimax game, GANs have revolutionized the field of generative modeling. In February 2018, GAN secured the leading spot on the ``Top Ten Global Breakthrough Technologies List'' issued by the Massachusetts Science and Technology Review. Over the years, numerous advancements have been proposed, leading to a rich array of GAN variants, such as conditional GAN, Wasserstein GAN, CycleGAN, and StyleGAN, among many others. This survey aims to provide a general overview of GANs, summarizing the latent architecture, validation metrics, and application areas of the most widely recognized variants. W… |
| https://openalex.org/W4387409504 | Topic model for long document extractive summarization with sentence-level features and dynamic memory unit | 2023 | Expert Systems with Applications | article | 8 | no | Chunlong Han, Jianzhou Feng, Haotian Qi | Automatic summarization, Computer science, Sentence, Leverage (statistics), Artificial intelligence, Multi-document summarization, +8 more | https://doi.org/10.1016/j.eswa.2023.121873 |  |
| https://openalex.org/W3162090017 | FNet: Mixing Tokens with Fourier Transforms | 2022 | Proceedings of the 2022 Conference of the North American Chapter of the Associa… | article | 458 | yes | James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontañón | Ilya, Computer science, Mixing (physics), Fourier transform, Artificial intelligence, Cognitive science, +5 more | https://doi.org/10.18653/v1/2022.naacl-main.319 | James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2022. |
| https://openalex.org/W4295122555 | Deep Residual Learning for Image Recognition: A Survey | 2022 | Applied Sciences | article | 818 | yes | Muhammad Shafiq, Zhaoquan Gu | Residual, Deep learning, Computer science, Artificial intelligence, Deep neural networks, Meaning (existential), +7 more | https://doi.org/10.3390/app12188972 | Deep Residual Networks have recently been shown to significantly improve the performance of neural networks trained on ImageNet, with results beating all previous methods on this dataset by large margins in the image classification task. However, the meaning of these impressive numbers and their implications for future research are not fully understood yet. In this survey, we will try to explain what Deep Residual Networks are, how they achieve their excellent results, and why their successful implementation in practice represents a significant advance over existing techniques. We also discuss some open questions related to residual learning as well as possible applications of Deep Residual Networks beyond ImageNet. Finally, we discuss some issues that still need to be resolved before deep residual learning can be applied on more complex problems. |
| https://openalex.org/W4392607709 | Generalizations of Wearable Device Placements and Sentences in Sign Language Recognition With Transformer-Based Model | 2024 | IEEE Transactions on Mobile Computing | article | 6 | no | Qingshan Wang, Zhiwen Zheng, Qi Wang, Dazhu Deng, Jiangtao Zhang | Computer science, Transformer, Sign language, Wearable computer, Natural language processing, Speech recognition, +8 more | https://doi.org/10.1109/tmc.2024.3373472 | Sign language is widely used among deaf. Many existing studies on Sign Language Recognition (SLR) focus on addressing communication barriers between deaf and hearing people. However, current studies face two challenges: the recognition result is highly dependent on the wearable device placements, and the existence of sentences in the training set. To address the challenges, this paper proposed EasyHear – a Transformer-based Chinese SLR system with a generalization approach. For generalization of wearable device placements, an anchor-based signal rotation correction algorithm is proposed to eliminate the impact of variations in wearing positions. In terms of generalization of sentences, a gesture code is constructed to reflect the closeness of gestures after defining an intimate entropy of gestures. Moreover, a semantic code is developed by training a neural network on a large corpus to… |
| https://openalex.org/W4205177092 | Identifying Electronic Nicotine Delivery System Brands and Flavors on Instagram: Natural Language Processing Analysis | 2022 | Journal of Medical Internet Research | article | 9 | yes | Robert Chew, Michael Wenger, Jamie Guillory, James Nonnemaker, Annice Kim | Convolutional neural network, Computer science, Conditional random field, Named-entity recognition, Natural language processing, Artificial intelligence, +6 more | https://doi.org/10.2196/30257 | Background Electronic nicotine delivery system (ENDS) brands, such as JUUL, used social media as a key component of their marketing strategy, which led to massive sales growth from 2015 to 2018. During this time, ENDS use rapidly increased among youths and young adults, with flavored products being particularly popular among these groups. Objective The aim of our study is to develop a named entity recognition (NER) model to identify potential emerging vaping brands and flavors from Instagram post text. NER is a natural language processing task for identifying specific types of words (entities) in text based on the characteristics of the entity and surrounding words. Methods NER models were trained on a labeled data set of 2272 Instagram posts coded for ENDS brands and flavors. We compared three types of NER models—conditional random fields, a residual convolutional neural network, and a… |
| https://openalex.org/W3135588948 | Toward Causal Representation Learning | 2021 | Proceedings of the IEEE | article | 909 | yes | Bernhard Schölkopf, Francesco Locatello, Stefan Bauer, Nan Rosemary Ke, Nal Kalchbrenner, Anirudh Goyal, Yoshua Bengio | Representation (politics), Computer science, Artificial intelligence, Cognitive psychology, Psychology, Cognitive science, +3 more | https://doi.org/10.1109/jproc.2021.3058954 | The two fields of machine learning and graphical causality arose and are developed separately. However, there is, now, cross-pollination and increasing interest in both fields to benefit from the advances of the other. In this article, we review fundamental concepts of causal inference and relate them to crucial open problems of machine learning, including transfer and generalization, thereby assaying how causality can contribute to modern machine learning research. This also applies in the opposite direction: we note that most work in causality starts from the premise that the causal variables are given. A central problem for AI and causality is, thus, causal representation learning, that is, the discovery of high-level causal variables from low-level observations. Finally, we delineate some implications of causality for machine learning and propose key research areas at the intersecti… |
| https://openalex.org/W2983040767 | Unsupervised Cross-lingual Representation Learning at Scale | 2020 |  | preprint | 539 | yes | Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Édouard Grave, Myle Ott, +2 more | Computer science, Terabyte, Language model, Transformer, Natural language processing, Artificial intelligence, +4 more | https://doi.org/10.18653/v1/2020.acl-main.747 | This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resourc… |
| https://openalex.org/W3154157180 | Using GPT-2 to Create Synthetic Data to Improve the Prediction Performance of NLP Machine Learning Classification Models | 2021 | arXiv (Cornell University) | preprint | 8 | yes | Dewayne Whitfield | Artificial intelligence, Computer science, Synthetic data, Machine learning, Convolutional neural network, Transfer of learning, +5 more | http://arxiv.org/abs/2104.10658 | Classification Models use input data to predict the likelihood that the subsequent input data will fall into predetermined categories. To perform effective classifications, these models require large datasets for training. It is becoming common practice to utilize synthetic data to boost the performance of Machine Learning Models. It is reported that Shell is using synthetic data to build models to detect problems that rarely occur; for example Shell created synthetic data to help models to identify deteriorating oil lines. It is common practice for Machine Learning Practitioners to generate synthetic data by rotating, flipping, and cropping images to increase the volume of image data to train Convolutional Neural Networks. The purpose of this paper is to explore creating and utilizing synthetic NLP data to improve the performance of Natural Language Processing Machine Learning Classifi… |
| https://openalex.org/W4392851491 | VulnArmor: mitigating software vulnerabilities with code resolution and detection techniques | 2024 | International Journal of Information Technology | article | 5 | no | Parul V. Sindhwad, Prateek Ranka, Siddhi Muni, Faruk Kazi | Computer science, Secure coding, Software, Code (set theory), Resolution (logic), Computer security, +8 more | https://doi.org/10.1007/s41870-024-01775-4 |  |
| https://openalex.org/W2969662548 | Hate speech detection: Challenges and solutions | 2019 | PLoS ONE | article | 543 | yes | Sean MacAvaney, Hao-Ren Yao, Eugene Yang, Katina Russell, Nazli Goharian, Ophir Frieder | Interpretability, Computer science, Task (project management), Artificial intelligence, Data science, Machine learning, +3 more | https://doi.org/10.1371/journal.pone.0221152 | As online content continues to grow, so does the spread of hate speech. We identify and examine challenges faced by online automatic approaches for hate speech detection in text. Among these difficulties are subtleties in language, differing definitions on what constitutes hate speech, and limitations of data availability for training and testing of these systems. Furthermore, many recent approaches suffer from an interpretability problem-that is, it can be difficult to understand why the systems make the decisions that they do. We propose a multi-view SVM approach that achieves near state-of-the-art performance, while being simpler and producing more easily interpretable decisions than neural methods. We also discuss both technical and practical challenges that remain for this task. |
| https://openalex.org/W4386552375 | RenAIssance: A Survey into AI Text-to-Image Generation in the Era of Large Model | 2023 | arXiv (Cornell University) | preprint | 9 | yes | Fengxiang Bie, Yibo Yang, Zhongzhu Zhou, Adam Ghanem, Minjia Zhang, Zhewei Yao, Xiaoxia Wu, Connor Holmes, +5 more | Computer science, Generative grammar, Autoregressive model, Generative model, Image (mathematics), Fidelity, +9 more | http://arxiv.org/abs/2309.00810 | Text-to-image generation (TTI) refers to the usage of models that could process text input and generate high fidelity images based on text descriptions. Text-to-image generation using neural networks could be traced back to the emergence of Generative Adversial Network (GAN), followed by the autoregressive Transformer. Diffusion models are one prominent type of generative model used for the generation of images through the systematic introduction of noises with repeating steps. As an effect of the impressive results of diffusion models on image synthesis, it has been cemented as the major image decoder used by text-to-image models and brought text-to-image generation to the forefront of machine-learning (ML) research. In the era of large models, scaling up model size and the integration with large language models have further improved the performance of TTI models, resulting the generat… |
| https://openalex.org/W3129271630 | Joint Pre-Trained Chinese Named Entity Recognition Based on Bi-Directional Language Model | 2021 | International Journal of Pattern Recognition and Artificial Intelligence | article | 7 | no | Changxia Ma, Chen Zhang | Computer science, Conditional random field, Named-entity recognition, Artificial intelligence, Feature engineering, Joint (building), +19 more | https://doi.org/10.1142/s0218001421530037 | The current named entity recognition (NER) is mainly based on joint convolution or recurrent neural network. In order to achieve high performance, these networks need to provide a large amount of training data in the form of feature engineering corpus and lexicons. Chinese NER is very challenging because of the high contextual relevance of Chinese characters, that is, Chinese characters and phrases may have many possible meanings in different contexts. To this end, we propose a model that leverages a pre-trained and bi-directional encoder representations-from-transformers language model and a joint bi-directional long short-term memory (Bi-LSTM) and conditional random fields (CRF) model for Chinese NER. The underlying network layer embeds Chinese characters and outputs character-level representations. The output is then fed into a bidirectional long short-term memory to capture contextu… |
| https://openalex.org/W4385839827 | UAV-YOLOv8: A Small-Object-Detection Model Based on Improved YOLOv8 for UAV Aerial Photography Scenarios | 2023 | Sensors | article | 750 | yes | Gang Wang, Yanfei Chen, Pei An, Hanyu Hong, Jinghu Hu, Tiange Huang | Computer science, Object detection, Block (permutation group theory), Feature (linguistics), Artificial intelligence, Focus (optics), +17 more | https://doi.org/10.3390/s23167190 | Unmanned aerial vehicle (UAV) object detection plays a crucial role in civil, commercial, and military domains. However, the high proportion of small objects in UAV images and the limited platform resources lead to the low accuracy of most of the existing detection models embedded in UAVs, and it is difficult to strike a good balance between detection performance and resource consumption. To alleviate the above problems, we optimize YOLOv8 and propose an object detection model based on UAV aerial photography scenarios, called UAV-YOLOv8. Firstly, Wise-IoU (WIoU) v3 is used as a bounding box regression loss, and a wise gradient allocation strategy makes the model focus more on common-quality samples, thus improving the localization ability of the model. Secondly, an attention mechanism called BiFormer is introduced to optimize the backbone network, which improves the model’s attention to… |
| https://openalex.org/W4213077304 | Geometry-enhanced molecular representation learning for property prediction | 2022 | Nature Machine Intelligence | article | 513 | yes | Xiaomin Fang, Lihang Liu, Jieqiong Lei, Donglong He, Shanzhuo Zhang, Jingbo Zhou, Fan Wang, Hua Wu, +1 more | Molecular graph, Representation (politics), Computer science, Property (philosophy), Graph, Artificial intelligence, +13 more | https://doi.org/10.1038/s42256-021-00438-4 | Abstract Effective molecular representation learning is of great importance to facilitate molecular property prediction. Recent advances for molecular representation learning have shown great promise in applying graph neural networks to model molecules. Moreover, a few recent studies design self-supervised learning methods for molecular representation to address insufficient labelled molecules; however, these self-supervised frameworks treat the molecules as topological graphs without fully utilizing the molecular geometry information. The molecular geometry, also known as the three-dimensional spatial structure of a molecule, is critical for determining molecular properties. To this end, we propose a novel geometry-enhanced molecular representation learning method (GEM). The proposed GEM has a specially designed geometry-based graph neural network architecture as well as several dedica… |
| https://openalex.org/W4387568733 | Knowledge Base Question Answering via Semantic Analysis | 2023 | Electronics | article | 6 | yes | Yibo Liu, Haisu Zhang, Teng Zong, Jian Wu, Wei Dai | Question answering, Computer science, Knowledge base, Interpretability, Open Knowledge Base Connectivity, Knowledge extraction, +8 more | https://doi.org/10.3390/electronics12204224 | Knowledge Question Answering is one of the important research directions in the field of robot intelligence. It is mainly based on background knowledge to analyze users’ questions and generate answers. It is one of the important application methods of knowledge graph technology. Compared with the traditional expert system of question and answer, it has the advantage of a large-scale background knowledge base and the traceability and interpretability of the question-answering process. Compared with the current ChatGPT (Chat Generative Pre-trained Transformer) technology, it has advantages in the proprietary segmentation field. Aiming at the problem of the accuracy of existing knowledge question-answering methods being low, this paper studies the method of semantic analysis for knowledge question-answering under the support of a knowledge database, proposes a knowledge question-answering… |
| https://openalex.org/W2995514860 | Modeling aspects of the language of life through transfer-learning protein sequences | 2019 | BMC Bioinformatics | article | 601 | yes | Michael Heinzinger, Ahmed Elnaggar, Yu Wang, Christian Dallago, Dmitrii Nechaev, Florian Matthes, Burkhard Rost | Computer science, Human proteome project, Artificial intelligence, Protein sequencing, Word2vec, Proteome, +13 more | https://doi.org/10.1186/s12859-019-3220-8 |  |
| https://openalex.org/W4392132271 | Predicting semantic category of answers for question answering systems using transformers: a transfer learning approach | 2024 | Multimedia Tools and Applications | article | 5 | no | C M Suneera, Jay Prakash, Varun Sai Alaparthi | Computer science, Transformer, Question answering, Artificial intelligence, Natural language processing, Transfer of learning, +11 more | http://dx.doi.org/10.1007/s11042-024-18609-x |  |
| https://openalex.org/W4401454468 | Detecting Data Races in OpenMP with Deep Learning and Large Language Models | 2024 |  | article | 5 | no | May Alsofyani, Liqiang Wang | Computer science, Deep learning, Artificial intelligence, Natural language processing, Programming language, Data modeling, +1 more | https://doi.org/10.1145/3677333.3678160 | Transformer-based neural network models are increasingly employed to handle software engineering issues, such as bug localization and program repair. These models, equipped with a self-attention mechanism, excel at understanding source code context and semantics. Recently, large language models (LLMs) have emerged as a promising alternative for analyzing and understanding code structure. In this paper, we propose two novel methods for detecting data race bugs in OpenMP programs. The first method is based on a transformer encoder trained from scratch. The second method leverages LLMs, specifically extending GPT-4 Turbo through the use of prompt engineering and fine-tuning techniques. For training and testing our approach, we utilized two datasets comprising different OpenMP directives. Our experiments show that the transformer encoder achieves competitive accuracy compared to LLMs, wheth… |
| https://openalex.org/W4393148714 | T2I-Adapter: Learning Adapters to Dig Out More Controllable Ability for Text-to-Image Diffusion Models | 2024 | Proceedings of the AAAI Conference on Artificial Intelligence | article | 589 | yes | Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, Ying Shan | Adapter (computing), Dig, Computer science, Image (mathematics), Artificial intelligence, Computer graphics (images), +2 more | https://doi.org/10.1609/aaai.v38i5.28226 | The incredible generative ability of large-scale text-to-image (T2I) models has demonstrated strong power of learning complex structures and meaningful semantics. However, relying solely on text prompts cannot fully take advantage of the knowledge learned by the model, especially when flexible and accurate controlling (e.g., structure and color) is needed. In this paper, we aim to ``dig out" the capabilities that T2I models have implicitly learned, and then explicitly use them to control the generation more granularly. Specifically, we propose to learn low-cost T2I-Adapters to align internal knowledge in T2I models with external control signals, while freezing the original large T2I models. In this way, we can train various adapters according to different conditions, achieving rich control and editing effects in the color and structure of the generation results. Further, the proposed T2… |
| https://openalex.org/W4297243391 | scBERT as a large-scale pretrained deep language model for cell type annotation of single-cell RNA-seq data | 2022 | Nature Machine Intelligence | article | 511 | yes | Fan Yang, Wenchuan Wang, Fang Wang, Yuan Fang, Duyu Tang, Junzhou Huang, Hui Lü, Jianhua Yao | Annotation, Computer science, Interpretability, Robustness (evolution), Artificial intelligence, Overfitting, +11 more | https://doi.org/10.1038/s42256-022-00534-z |  |
| https://openalex.org/W3202863625 | Incorporating Convolution Designs into Visual Transformers | 2021 | arXiv (Cornell University) | preprint | 5 | yes | Kun Yuan, Shaopeng Guo, Ziwei Liu, Aojun Zhou, Fengwei Yu, Wei Wu | Transformer, Computer science, Convolutional neural network, Encoder, Locality, Computer engineering, +9 more | http://arxiv.org/abs/2103.11816 | Motivated by the success of Transformers in natural language processing (NLP) tasks, there emerge some attempts (e.g., ViT and DeiT) to apply Transformers to the vision domain. However, pure Transformer architectures often require a large amount of training data or extra supervision to obtain comparable performance with convolutional neural networks (CNNs). To overcome these limitations, we analyze the potential drawbacks when directly borrowing Transformer architectures from NLP. Then we propose a new \textbf{Convolution-enhanced image Transformer (CeiT)} which combines the advantages of CNNs in extracting low-level features, strengthening locality, and the advantages of Transformers in establishing long-range dependencies. Three modifications are made to the original Transformer: \textbf{1)} instead of the straightforward tokenization from raw input images, we design an \textbf{Image-… |
| https://openalex.org/W4380786284 | Video Sign Language Recognition using Pose Extraction and Deep Learning Models | 2023 |  | dissertation | 4 | yes | Shayla Luong | Computer science, Convolutional neural network, Artificial intelligence, Overfitting, Sign language, Deep learning, +10 more | https://doi.org/10.31979/etd.jm4c-myd4 | Sign language recognition (SLR) has long been a studied subject and research field within the Computer Vision domain. Appearance-based and pose-based approaches are two ways to tackle SLR tasks. Various models from traditional to current state-of-the-art including HOG-based features, Convolutional Neural Network, Recurrent Neural Network, Transformer, and Graph Convolutional Network have been utilized to tackle the area of SLR. While classifying alphabet letters in sign language has shown high accuracy rates, recognizing words presents its set of difficulties including the large vocabulary size, the subtleties in body motions and hand orientations, and regional dialects and variations. The emergence of deep learning has created opportunities for improved word-level sign recognition, but challenges such as overfitting and limited training data remain. Techniques such as data augmentation… |
| https://openalex.org/W4399376085 | DCT-ViT: High-Frequency Pruned Vision Transformer With Discrete Cosine Transform | 2024 | IEEE Access | article | 10 | yes | Jongho Lee, Hyun Kim | Discrete cosine transform, Lapped transform, Computer science, Computer vision, Transformer, Transform coding, +6 more | https://doi.org/10.1109/access.2024.3410231 | Transformers have demonstrated notable efficacy in computer vision, extending beyond their initial success in natural language processing. The application of vision transformers (ViTs) to resource-constrained mobile and edge devices is hampered by their extensive computational demands and large parameter sets. To address this, research has explored pruning redundant components of ViTs. Given that the computational burden of ViTs scales quadratically with token count, previous efforts have aimed to decrease the number of tokens or to linearize the computational cost of self-attention. However, such methods often incur significant accuracy losses due to the disruption of critical information pathways within the ViT, which primarily focuses on shape rather than texture, potentially aligning its image interpretation more closely with human perception than convolutional neural network (CNN)… |
| https://openalex.org/W4321854066 | Detecting software vulnerabilities using Language Models | 2023 | arXiv (Cornell University) | preprint | 6 | yes | Marwan Omar | Computer science, Deep learning, Convolutional neural network, Benchmark (surveying), Software deployment, Artificial intelligence, +15 more | http://arxiv.org/abs/2302.11773 | Recently, deep learning techniques have garnered substantial attention for their ability to identify vulnerable code patterns accurately. However, current state-of-the-art deep learning models, such as Convolutional Neural Networks (CNN), and Long Short-Term Memories (LSTMs) require substantial computational resources. This results in a level of overhead that makes their implementation unfeasible for deployment in realtime settings. This study presents a novel transformer-based vulnerability detection framework, referred to as VulDetect, which is achieved through the fine-tuning of a pre-trained large language model, (GPT) on various benchmark datasets of vulnerable code. Our empirical findings indicate that our framework is capable of identifying vulnerable software code with an accuracy of up to 92.65%. Our proposed technique outperforms SyseVR and VulDeBERT, two state-of-the-art vuln… |
| https://openalex.org/W3105816068 | On the Sentence Embeddings from Pre-trained Language Models | 2020 |  | article | 522 | yes | Bohan Li, Hao Zhou, Junxian He, Mingxuan Wang, Yiming Yang, Lei Li | Computer science, Natural language processing, Sentence, Semantic similarity, Artificial intelligence, Embedding, +6 more | https://doi.org/10.18653/v1/2020.emnlp-main.733 | Pre-trained contextual representations like BERT have achieved great success in natural language processing. However, the sentence embeddings from the pre-trained language models without fine-tuning have been found to poorly capture semantic meaning of sentences. In this paper, we argue that the semantic information in the BERT embeddings is not fully exploited. We first reveal the theoretical connection between the masked language model pre-training objective and the semantic similarity task theoretically, and then analyze the BERT sentence embeddings empirically. We find that BERT always induces a non-smooth anisotropic semantic space of sentences, which harms its performance of semantic similarity. To address this issue, we propose to transform the anisotropic sentence embedding distribution to a smooth and isotropic Gaussian distribution through normalizing flows that are learned wi… |
| https://openalex.org/W4393028575 | Evaluating Emotional Detection &amp; Classification Capabilities of GPT-2 &amp; GPT-Neo Using Textual Data | 2024 |  | article | 10 | no | Bhawna Jain, Gunika Goyal, Mehak Sharma | Computer science, Artificial intelligence | https://doi.org/10.1109/confluence60223.2024.10463396 | Mental health has emerged as a pressing global health public concern due to the rising prevalence of mental illnesses and the notable gap in the detection and diagnosis of these conditions. With the widespread use of text as a means of communication, analysing mental health conditions from text has become essential. Artificial intelligence (AI) for healthcare has seen rapid growth and substantial research. In recent decades, the field of emotion recognition through AI has witnessed a notable shift from statistically shallow models to deep neural networks-based models, but they often grapple with challenges related to data scarcity, interpretability, and context-awareness, which has prompted the use of Large Language Models (LLMs) as a viable alternative. However, the introduction of LLMs like Generative Pretrained Transformer (GPT)-2 & GPT-Neo has astounded the research community with i… |
| https://openalex.org/W3204937802 | Deep Generative Modelling: A Comparative Review of VAEs, GANs, Normalizing Flows, Energy-Based and Autoregressive Models | 2021 | Durham Research Online (Durham University) | review | 553 | yes | Sam Bond-Taylor | Computer science, Autoregressive model, Compendium, Generative grammar, Artificial intelligence, Artificial neural network, +7 more | https://durham-repository.worktribe.com/output/1232078 | Deep generative models are a class of techniques that train deep neural networks to model the distribution of training samples. Research has fragmented into various interconnected approaches, each of which make trade-offs including run-time, diversity, and architectural restrictions. In particular, this compendium covers energy-based models, variational autoencoders, generative adversarial networks, autoregressive models, normalizing flows, in addition to numerous hybrid approaches. These techniques are compared and contrasted, explaining the premises behind each and how they are interrelated, while reviewing current state-of-the-art advances and implementations. |
| https://openalex.org/W4286500588 | Evolutionary-scale prediction of atomic level protein structure with a language model | 2022 |  | preprint | 518 | yes | Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, +7 more | Computer science, Inference, Scale (ratio), Language model, Protein structure prediction, Artificial intelligence, +7 more | https://doi.org/10.1101/2022.07.20.500902 | Abstract Artificial intelligence has the potential to open insight into the structure of proteins at the scale of evolution. It has only recently been possible to extend protein structure prediction to two hundred million cataloged proteins. Characterizing the structures of the exponentially growing billions of protein sequences revealed by large scale gene sequencing experiments would necessitate a break-through in the speed of folding. Here we show that direct inference of structure from primary sequence using a large language model enables an order of magnitude speed-up in high resolution structure prediction. Leveraging the insight that language models learn evolutionary patterns across millions of sequences, we train models up to 15B parameters, the largest language model of proteins to date. As the language models are scaled they learn information that enables prediction of the th… |
| https://openalex.org/W4327725011 | Can We Read Neural Networks? Epistemic Implications of Two Historical Computer Science Papers | 2023 | American Literature | article | 9 | no | Fabian Offert | Relation (database), Computer science, Artificial intelligence, Representation (politics), Field (mathematics), Humanism, +13 more | https://doi.org/10.1215/00029831-10575218 | This review looks at two technical papers from the field of computer science that, at the time of writing, should be considered historical. Although their respective technical approaches have since been replaced with newer, better, and more efficient ones, when looking back through the lens of critical AI studies they mark the beginning of a type of theoretical reflection within computer science that distinctly links technical machine learning research to research in the humanities.Machine learning models are cultural artifacts. They are trained on (limited) real-world data and often designed to make decisions with real-world impacts. The relation of a machine learning model to the world is thus a relation of interest. What kind of representations do machine learning models produce? As the world is necessarily mirrored in a machine learning system to some degree, as there exists, with W… |
| https://openalex.org/W4285335535 | Pre-Trained Neural Language Models for Automatic Mobile App User Feedback Answer Generation | 2021 |  | article | 7 | no | Yue Cao, Fatemeh H. Fard | Computer science, Transformer, Artificial neural network, Machine learning, Language model, Artificial intelligence, +7 more | https://doi.org/10.1109/asew52652.2021.00033 | Studies show that developers' answers to the mobile app users' feedbacks on app stores can increase the apps' star rating. To help app developers generate answers that are related to the users' issues, recent studies develop models to generate the answers automatically. Aims: The app response generation models use deep neural networks and require training data. Pre-Trained neural language Models (PTM) used in Natural Language Processing (NLP) take advantage of the information they learned from a large corpora in an unsupervised manner, and can reduce the amount of required training data. In this paper, we evaluate PTMs to generate replies to the mobile app user feedbacks. Method: We train a Transformer model from scratch and fine tune two PTMs to evaluate the generated responses, which are compared to RRGEN, a current app response model. We also evaluate the models with different portio… |
| https://openalex.org/W4309394245 | Deep Emotion Recognition in Textual Conversations: A Survey | 2022 | arXiv (Cornell University) | preprint | 6 | yes | Patrícia Pereira, Helena Moniz, João Paulo Carvalho | Interpretability, Computer science, Benchmark (surveying), Sarcasm, Context (archaeology), Annotation, +17 more | http://arxiv.org/abs/2211.09172 | Emotion Recognition in Conversations (ERC) is a key step towards successful human-machine interaction. While the field has seen tremendous advancement in the last few years, new applications and implementation scenarios present novel challenges and opportunities. These range from leveraging the conversational context, speaker, and emotion dynamics modelling, to interpreting common sense expressions, informal language, and sarcasm, addressing challenges of real-time ERC, recognizing emotion causes, different taxonomies across datasets, multilingual ERC, and interpretability. This survey starts by introducing ERC, elaborating on the challenges and opportunities of this task. It proceeds with a description of the emotion taxonomies and a variety of ERC benchmark datasets employing such taxonomies. This is followed by descriptions comparing the most prominent works in ERC with explanations… |
| https://openalex.org/W4280563486 | A Transformer-Based Capsule Network for 3D Part–Whole Relationship Learning | 2022 | Entropy | article | 6 | yes | Yu Chen, Jieyu Zhao, Qilu Qiu | Computer science, Artificial intelligence, Transformer, Feature learning, ENCODE, Pattern recognition (psychology), +8 more | https://doi.org/10.3390/e24050678 | Learning the relationship between the part and whole of an object, such as humans recognizing objects, is a challenging task. In this paper, we specifically design a novel neural network to explore the local-to-global cognition of 3D models and the aggregation of structural contextual features in 3D space, inspired by the recent success of Transformer in natural language processing (NLP) and impressive strides in image analysis tasks such as image classification and object detection. We build a 3D shape Transformer based on local shape representation, which provides relation learning between local patches on 3D mesh models. Similar to token (word) states in NLP, we propose local shape tokens to encode local geometric information. On this basis, we design a shape-Transformer-based capsule routing algorithm. By applying an iterative capsule routing algorithm, local shape information can b… |
| https://openalex.org/W3006087551 | Informed Machine Learning - A Taxonomy and Survey of Integrating Prior Knowledge into Learning Systems | 2021 | IEEE Transactions on Knowledge and Data Engineering | article | 748 | yes | Laura von Rueden, Sebastian Mayer, Katharina Beckh, Bogdan Georgiev, Sven Giesselbach, Raoul Heese, Birgit Kirsch, Michał Walczak, +6 more | Computer science, Machine learning, Taxonomy (biology), Artificial intelligence, Knowledge representation and reasoning, Active learning (machine learning), +5 more | https://doi.org/10.1109/tkde.2021.3079836 | Despite its great success, machine learning can have its limits when dealing\nwith insufficient training data. A potential solution is the additional\nintegration of prior knowledge into the training process which leads to the\nnotion of informed machine learning. In this paper, we present a structured\noverview of various approaches in this field. We provide a definition and\npropose a concept for informed machine learning which illustrates its building\nblocks and distinguishes it from conventional machine learning. We introduce a\ntaxonomy that serves as a classification framework for informed machine\nlearning approaches. It considers the source of knowledge, its representation,\nand its integration into the machine learning pipeline. Based on this taxonomy,\nwe survey related research and describe how different knowledge representations\nsuch as algebraic equations, logic rules, or… |
| https://openalex.org/W4310273071 | Graph neural networks for materials science and chemistry | 2022 | Communications Materials | review | 640 | yes | Patrick Reiser, Marlen Neubert, André Eberhard, Luca Torresi, Chen Zhou, Chen Shao, Houssam Metni, Clint van Hoesel, +3 more | Computer science, Graph, Relevance (law), Representation (politics), Data science, Cheminformatics, +7 more | https://doi.org/10.1038/s43246-022-00315-6 |  |
| https://openalex.org/W4317936753 | BERT-RCNN: an Automatic Classification of App Reviews using Transfer Learning based RCNN Deep Model | 2023 | Research Square (Research Square) | preprint | 8 | yes | Kamaljit Kaur, Parminder Kaur | Computer science, Artificial intelligence, Machine learning, Deep learning, Word embedding, Convolutional neural network, +3 more | https://doi.org/10.21203/rs.3.rs-2503700/v1 | <title>Abstract</title> App reviews considered as the major concern over the internet. It influences the user’s mind before purchasing the app. However, such user reviews might contain technical information about the app that can be valuable for the developers and software companies. Due to pervasive use of mobile apps, large amount of data is created by users on daily basis. Manual identification and classification of such reviews is very time-consuming and laborious task. Hence, automating this process is essential for helping developers in managing these reviews efficiently. Recently, traditional machine learning and deep learning oriented approaches have been exploited for classification of app review into software requirements. Most of the machine learning techniques utilizes traditional word techniques to extract the features from the textual reviews. In addition, deep learning te… |
| https://openalex.org/W3217259997 | A Comparative Study of Transformers on Word Sense Disambiguation | 2021 | Communications in computer and information science | book-chapter | 5 | no | Avi Chawla, Nidhi Mulay, Vikas Bishnoi, Gaurav Dhama, Anil Kumar Singh | Contextualization, Word-sense disambiguation, Computer science, Transformer, Natural language processing, Artificial intelligence, +9 more | https://doi.org/10.1007/978-3-030-92307-5_87 |  |
| https://openalex.org/W3155534120 | Rethinking Network Pruning -- under the Pre-train and Fine-tune Paradigm | 2021 | arXiv (Cornell University) | preprint | 6 | yes | Dongkuan Xu, Ian En-Hsu Yen, Jinxi Zhao, Zhibin Xiao | Computer science, Pruning, Convolutional neural network, FLOPS, Benchmark (surveying), Artificial intelligence, +11 more | http://arxiv.org/abs/2104.08682 | Transformer-based pre-trained language models have significantly improved the performance of various natural language processing (NLP) tasks in the recent years. While effective and prevalent, these models are usually prohibitively large for resource-limited deployment scenarios. A thread of research has thus been working on applying network pruning techniques under the pretrain-then-finetune paradigm widely adopted in NLP. However, the existing pruning results on benchmark transformers, such as BERT, are not as remarkable as the pruning results in the literature of convolutional neural networks (CNNs). In particular, common wisdom in pruning CNN states that sparse pruning technique compresses a model more than that obtained by reducing number of channels and layers (Elsen et al., 2020; Zhu and Gupta, 2017), while existing works on sparse pruning of BERT yields inferior results than its… |
| https://openalex.org/W4398617921 | On compositional generalization of transformer-based neural machine translation | 2024 | Information Fusion | article | 6 | no | Yongjing Yin, Lian Fu, Yafu Li, Yue Zhang | Computer science, Transformer, Machine translation, Generalization, Translation (biology), Artificial intelligence, +9 more | https://doi.org/10.1016/j.inffus.2024.102491 |  |
| https://openalex.org/W4404153563 | An Explainable Artificial Intelligence Text Classifier for Suicidality Prediction in Youth Crisis Text Line Users: Development and Validation Study | 2024 | JMIR Public Health and Surveillance | article | 7 | yes | Julia Thomas, Antonia Lucht, Jacob Segler, Richard Wundrack, Marcel Miché, Roselind Lieb, Lars Kuchinke, Gunther Meinlschmidt | Preprint, Artificial intelligence, Computer science, Classifier (UML), Computer security, World Wide Web | https://doi.org/10.2196/63809 | Background Suicide represents a critical public health concern, and machine learning (ML) models offer the potential for identifying at-risk individuals. Recent studies using benchmark datasets and real-world social media data have demonstrated the capability of pretrained large language models in predicting suicidal ideation and behaviors (SIB) in speech and text. Objective This study aimed to (1) develop and implement ML methods for predicting SIBs in a real-world crisis helpline dataset, using transformer-based pretrained models as a foundation; (2) evaluate, cross-validate, and benchmark the model against traditional text classification approaches; and (3) train an explainable model to highlight relevant risk-associated features. Methods We analyzed chat protocols from adolescents and young adults (aged 14-25 years) seeking assistance from a German crisis helpline. An ML model was d… |
| https://openalex.org/W2972006294 | A Survey of Deep Learning-Based Object Detection | 2019 | IEEE Access | article | 1241 | yes | Licheng Jiao, Fan Zhang, Fang Liu, Shuyuan Yang, Lingling Li, Zhixi Feng, Rong Qu | Computer science, Object detection, Artificial intelligence, Deep learning, Computer vision, Pattern recognition (psychology) | https://doi.org/10.1109/access.2019.2939201 | Object detection is one of the most important and challenging branches of\ncomputer vision, which has been widely applied in peoples life, such as\nmonitoring security, autonomous driving and so on, with the purpose of locating\ninstances of semantic objects of a certain class. With the rapid development of\ndeep learning networks for detection tasks, the performance of object detectors\nhas been greatly improved. In order to understand the main development status\nof object detection pipeline, thoroughly and deeply, in this survey, we first\nanalyze the methods of existing typical detection models and describe the\nbenchmark datasets. Afterwards and primarily, we provide a comprehensive\noverview of a variety of object detection methods in a systematic manner,\ncovering the one-stage and two-stage detectors. Moreover, we list the\ntraditional and new applications. Some representative b… |
| https://openalex.org/W4382567099 | Using Large Language Models to Provide Formative Feedback in Intelligent Textbooks | 2023 | Communications in computer and information science | book-chapter | 7 | no | Wesley Morris, Scott A. Crossley, Langdon Holmes, Chaohua Ou, Danielle S. McNamara, Mihai Dascălu | Rubric, Formative assessment, Automatic summarization, Computer science, Grading (engineering), Reading comprehension, +18 more | https://doi.org/10.1007/978-3-031-36336-8_75 |  |
| https://openalex.org/W4382699829 | Multidimensional Affective Analysis for Low-Resource Languages: A Use Case with Guarani-Spanish Code-Switching Language | 2023 | Cognitive Computation | article | 6 | yes | Marvin M. Agüero-Torales, A.G. López‐Herrera, David Vilares | Computer science, Artificial intelligence, Language model, Sentiment analysis, Code (set theory), Natural language processing, +13 more | https://hdl.handle.net/10481/98843 | &amp;lt;p&amp;gt;This paper focuses on text-based affective computing for Jopara, a code-switching language that combines Guarani and Spanish. First, we collected a dataset of tweets primarily written in Guarani and annotated them for three widely used dimensions in sentiment analysis: (a) emotion recognition, (b) humor detection, and (c) offensive language identification. Then, we developed several neural network models, including large language models specifically designed for Guarani, and compared their performance against off-the-shelf multilingual and Spanish pre-trained models for the aforementioned dimensions. Our experiments show that language models incorporating Guarani during pre-training or pre-fine-tuning consistently achieve the best results, despite limited resources (a single 24-GB GPU and only 800K tokens). Notably, even a Guarani BERT model with just two layers of Tran… |
| https://openalex.org/W3017637887 | BEHRT: Transformer for Electronic Health Records | 2020 | Scientific Reports | article | 477 | yes | Yikuan Li, Shishir Rao, José Roberto Ayala Solares, Abdelǎali Hassaïne, Rema Ramakrishnan, Dexter Canoy, Yajie Zhu, Kazem Rahimi, +1 more | Health records, Medical diagnosis, Artificial intelligence, Computer science, Machine learning, Deep learning, +13 more | https://doi.org/10.1038/s41598-020-62922-y | Abstract Today, despite decades of developments in medicine and the growing interest in precision healthcare, vast majority of diagnoses happen once patients begin to show noticeable signs of illness. Early indication and detection of diseases, however, can provide patients and carers with the chance of early intervention, better disease management, and efficient allocation of healthcare resources. The latest developments in machine learning (including deep learning) provides a great opportunity to address this unmet need. In this study, we introduce BEHRT: A deep neural sequence transduction model for electronic health records (EHR), capable of simultaneously predicting the likelihood of 301 conditions in one’s future visits. When trained and evaluated on the data from nearly 1.6 million individuals, BEHRT shows a striking improvement of 8.0–13.2% (in terms of average precision scores… |
| https://openalex.org/W4387635905 | LLM-augmented Preference Learning from Natural Language | 2023 | arXiv (Cornell University) | preprint | 4 | yes | Inwon Kang, Sikai Ruan, Tyler Ho, Jui-Chien Lin, Farhad Mohsin, Oshani Seneviratne, Lirong Xia | Transformer, Leverage (statistics), Computer science, Artificial intelligence, Language model, Natural language processing, +7 more | http://arxiv.org/abs/2310.08523 | Finding preferences expressed in natural language is an important but challenging task. State-of-the-art(SotA) methods leverage transformer-based models such as BERT, RoBERTa, etc. and graph neural architectures such as graph attention networks. Since Large Language Models (LLMs) are equipped to deal with larger context lengths and have much larger model sizes than the transformer-based model, we investigate their ability to classify comparative text directly. This work aims to serve as a first step towards using LLMs for the CPC task. We design and conduct a set of experiments that format the classification task into an input prompt for the LLM and a methodology to get a fixed-format response that can be automatically evaluated. Comparing performances with existing methods, we see that pre-trained LLMs are able to outperform the previous SotA models with no fine-tuning involved. Our re… |
| https://openalex.org/W4390490761 | Explainability for Large Language Models: A Survey | 2024 | ACM Transactions on Intelligent Systems and Technology | article | 450 | yes | Haiyan Zhao, Hanjie Chen, Fan Yang, Ninghao Liu, Huiqi Deng, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, +1 more | Computer science, Language model, Data science, Artificial intelligence, Natural language processing | https://doi.org/10.1145/3639372 | Large language models (LLMs) have demonstrated impressive capabilities in natural language processing. However, their internal mechanisms are still unclear and this lack of transparency poses unwanted risks for downstream applications. Therefore, understanding and explaining these models is crucial for elucidating their behaviors, limitations, and social impacts. In this article, we introduce a taxonomy of explainability techniques and provide a structured overview of methods for explaining Transformer-based language models. We categorize techniques based on the training paradigms of LLMs: traditional fine-tuning-based paradigm and prompting-based paradigm. For each paradigm, we summarize the goals and dominant approaches for generating local explanations of individual predictions and global explanations of overall model knowledge. We also discuss metrics for evaluating generated explan… |
| https://openalex.org/W2944165510 | Object Detection in 20 Years: A Survey | 2019 | arXiv (Cornell University) | preprint | 560 | yes | Zhengxia Zou, Zhenwei Shi, Yuhong Guo, Jieping Ye, Ye, Jieping | Milestone, Object detection, Computer science, Perspective (graphical), Field (mathematics), Artificial intelligence, +11 more | http://arxiv.org/abs/1905.05055 | Object detection, as of one the most fundamental and challenging problems in computer vision, has received great attention in recent years. Over the past two decades, we have seen a rapid technological evolution of object detection and its profound impact on the entire computer vision field. If we consider today's object detection technique as a revolution driven by deep learning, then back in the 1990s, we would see the ingenious thinking and long-term perspective design of early computer vision. This paper extensively reviews this fast-moving research field in the light of technical evolution, spanning over a quarter-century's time (from the 1990s to 2022). A number of topics have been covered in this paper, including the milestone detectors in history, detection datasets, metrics, fundamental building blocks of the detection system, speed-up techniques, and the recent state-of-the-ar… |
| https://openalex.org/W2970279348 | Findings of the 2019 Conference on Machine Translation (WMT19) | 2019 |  | article | 470 | yes | Loïc Barrault, Ondřej Bojar, Marta R. Costa‐jussà, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, +7 more | Machine translation, Translation (biology), Art history, Computer science, Art, Volume (thermodynamics), +8 more | https://doi.org/10.18653/v1/w19-5301 | Loïc Barrault, Ondřej Bojar, Marta R. Costa-jussà, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Philipp Koehn, Shervin Malmasi, Christof Monz, Mathias Müller, Santanu Pal, Matt Post, Marcos Zampieri. Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1). 2019. |
| https://openalex.org/W4321020806 | A Comparative Study of Effective Approaches for Arabic Text Classification | 2023 | SSRN Electronic Journal | article | 3 | yes | Mohamed BERRIMI, Mourad Oussalah, Abdelouahab Moussaouı, Mohamed Saı̈di | Arabic, Natural language processing, Linguistics, Artificial intelligence, Computer science, Philosophy | http://dx.doi.org/10.2139/ssrn.4361591 |  |
| https://openalex.org/W4392018827 | Online biomedical named entities recognition by data and knowledge-driven model | 2024 | Artificial Intelligence in Medicine | article | 6 | no | Lulu Cao, Chaochen Wu, Guan Luo, Chao Guo, Anni Zheng | Named-entity recognition, Computer science, Biomedical text mining, Artificial intelligence, Natural language processing, Knowledge base, +8 more | https://doi.org/10.1016/j.artmed.2024.102813 |  |
| https://openalex.org/W4225299823 | Bert Distillation to Enhance the Performance of Machine Learning Models for Sentiment Analysis on Movie Review Data | 2022 | 2022 9th International Conference on Computing for Sustainable Global Developme… | article | 5 | no | Monir Yahya Salmony, Arman Rasool Faridi | Computer science, Artificial intelligence, Machine learning, Transformer, Deep learning, Sentiment analysis, +8 more | https://doi.org/10.23919/indiacom54597.2022.9763262 | Recently, Deep Neural Networks DNN, transformers, and ensemble models have accomplished incredible SOTA performance in numerous fields, including Computer Vision CV, Natural Language Processing NLP, etc. Drawbacks often accompany these advantages, such as heavyweight with nearly millions (and billions) of parameters and computationally too expensive, which stand as a barrier for these models to be deployed on the client-side of a web application, edge devices, or embedded devices. Distillation is one of the active model compression methods that draws the researcher's attention, which is used to overcome these drawbacks (big size and inference computation time). It fills the gap between learnability and expressive power, which is achieved by training a small, lightweight machine learning or deep learning model based on extra knowledge obtained from a large and high-weight, already traine… |
| https://openalex.org/W3092309115 | Interlocking Backpropagation: Improving depthwise model-parallelism | 2020 | arXiv (Cornell University) | preprint | 5 | yes | Aidan N. Gomez, Oscar Key, Stephen Gou, Nick Frosst, Jeff Dean, Yarin Gal, Gal, Yarin | Computer science, Backpropagation, Task (project management), Transformer, Artificial neural network, Artificial intelligence, +9 more | http://arxiv.org/abs/2010.04116 | The number of parameters in state of the art neural networks has drastically increased in recent years. This surge of interest in large scale neural networks has motivated the development of new distributed training strategies enabling such models. One such strategy is model-parallel distributed training. Unfortunately, model-parallelism can suffer from poor resource utilisation, which leads to wasted resources. In this work, we improve upon recent developments in an idealised model-parallel optimisation setting: local learning. Motivated by poor resource utilisation in the global setting and poor task performance in the local setting, we introduce a class of intermediary strategies between local and global learning referred to as interlocking backpropagation. These strategies preserve many of the compute-efficiency advantages of local optimisation, while recovering much of the task per… |
| https://openalex.org/W3172120910 | Improving Open Information Extraction with Distant Supervision Learning | 2021 | Neural Processing Letters | article | 5 | no | Jiabao Han, Hongzhi Wang | Computer science, Leverage (statistics), Information extraction, Artificial intelligence, Machine learning, Transformer, +18 more | https://doi.org/10.1007/s11063-021-10548-0 |  |
| https://openalex.org/W4229067736 | Using distributional thesaurus to enhance transformer-based contextualized representations for low resource languages | 2022 | Proceedings of the 37th ACM/SIGAPP Symposium on Applied Computing | article | 3 | no | G. S. Venkatesh, Abhik Jana, Steffen Remus, Özge Sevgili, G. Srinivasaraghavan, Chris Biemann | Bengali, Computer science, Natural language processing, Transformer, Amharic, Artificial intelligence, +14 more | https://doi.org/10.1145/3477314.3507077 | Transformer-based language models recently gained large popularity in Natural Language Processing (NLP) because of their diverse applicability in various tasks where they reach state-of-the-art performance. Even though for resource-rich languages like English, performance is very high, there is still headroom for improvement for low resource languages. In this paper, we propose a methodology to incorporate Distributional Thesaurus information using a Graph Neural Network on top of pretrained Transformer models to improve the state-of-the-art performance for tasks like semantic textual similarity, sentiment analysis, paraphrasing, and discourse analysis. In this study, we attempt various NLP tasks using our proposed methodology for five languages - English, German, Hindi, Bengali, and Amharic - and show that by using our approach, the performance improvement over transformer models incre… |
| https://openalex.org/W4407425079 | PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language Model Inference | 2025 | arXiv (Cornell University) | preprint | 9 | yes | Yufeng Gu, Alireza Khadem, Sumanth Umesh, Ning Liang, Xavier Servot, Onur Mutlu, Ravishankar Iyer, Reetuparna Das | Computer science, Inference, Language model, Encoder, Software deployment, Bandwidth (computing), +10 more | http://arxiv.org/abs/2502.07578 | Large Language Model (LLM) inference uses an autoregressive manner to generate one token at a time, which exhibits notably lower operational intensity compared to earlier Machine Learning (ML) models such as encoder-only transformers and Convolutional Neural Networks. At the same time, LLMs possess large parameter sizes and use key-value caches to store context information. Modern LLMs support context windows with up to 1 million tokens to generate versatile text, audio, and video content. A large key-value cache unique to each prompt requires a large memory capacity, limiting the inference batch size. Both low operational intensity and limited batch size necessitate a high memory bandwidth. However, contemporary hardware systems for ML model deployment, such as GPUs and TPUs, are primarily optimized for compute throughput. This mismatch challenges the efficient deployment of advanced L… |
| https://openalex.org/W4224226311 | Multi-Label Classification in Patient-Doctor Dialogues With the RoBERTa-WWM-ext + CNN (Robustly Optimized Bidirectional Encoder Representations From Transformers Pretraining Approach With Whole Word Masking Extended Com… | 2022 | JMIR Medical Informatics | article | 6 | yes | Yuanyuan Sun, Dongping Gao, Xifeng Shen, Meiting Li, Jiale Nan, Weining Zhang | Computer science, Encoder, Sentence, Transformer, Natural language processing, Artificial intelligence, +7 more | https://doi.org/10.2196/35606 | Background With the prevalence of online consultation, many patient-doctor dialogues have accumulated, which, in an authentic language environment, are of significant value to the research and development of intelligent question answering and automated triage in recent natural language processing studies. Objective The purpose of this study was to design a front-end task module for the network inquiry of intelligent medical services. Through the study of automatic labeling of real doctor-patient dialogue text on the internet, a method of identifying the negative and positive entities of dialogues with higher accuracy has been explored. Methods The data set used for this study was from the Spring Rain Doctor internet online consultation, which was downloaded from the official data set of Alibaba Tianchi Lab. We proposed a composite abutting joint model, which was able to automatically cl… |
| https://openalex.org/W4385573860 | Understanding Domain Learning in Language Models Through Subpopulation Analysis | 2022 |  | article | 3 | yes | Zheng Zhao, Yftah Ziser, Shay B. Cohen | Computer science, Language model, Transformer, Artificial intelligence, Domain (mathematical analysis), Domain model, +12 more | http://dx.doi.org/10.18653/v1/2022.blackboxnlp-1.16 | We investigate how different domains are encoded in modern neural network architectures. We analyze the relationship between natural language domains, model size, and the amount of training data used. The primary analysis tool we develop is based on subpopulation analysis with Singular Vector Canonical Correlation Analysis (SVCCA), which we apply to Transformer-based language models (LMs). We compare the latent representations of such a language model at its different layers from a pair of models: a model trained on multiple domains (an experimental model) and a model trained on a single domain (a control model). Through our method, we find that increasing the model capacity impacts how domain information is stored in upper and lower layers differently. In addition, we show that larger experimental models simultaneously embed domain-specific information as if they were conjoined control… |
| https://openalex.org/W4392154824 | Training Object Detectors from Scratch: An Empirical Study in the Era of Vision Transformer | 2024 | International Journal of Computer Vision | article | 4 | yes | Weixiang Hong, Wang Ren, Jiangwei Lao, Lele Xie, Liheng Zhong, Jian Wang, Jingdong Chen, Honghai Liu, +1 more | Transformer, Computer science, Detector, Scratch, Artificial intelligence, Convolutional neural network, +8 more | https://doi.org/10.1007/s11263-024-01988-x | Abstract Modeling in computer vision has long been dominated by convolutional neural networks (CNNs). Recently, in light of the excellent performance of self-attention mechanism in the language field, transformers tailored for visual data have drawn significant attention and triumphed over CNNs in various vision tasks. These vision transformers heavily rely on large-scale pre-training to achieve competitive accuracy, which not only hinders the freedom of architectural design in downstream tasks like object detection, but also causes learning bias and domain mismatch in the fine-tuning stages. To this end, we aim to get rid of the “pre-train and fine-tune” paradigm of vision transformer and train transformer based object detector from scratch. Some earlier works in the CNNs era have successfully trained CNNs based detectors without pre-training, unfortunately, their findings do not gener… |
| https://openalex.org/W4391937116 | Natural Language Coding (NLC) for Autonomous Stock Trading: A New Dimension in No-Code/Low-Code (NCLC) AI | 2023 |  | article | 5 | no | Yulia Kumar, Wenxiao Li, Kuan Huang, Michael Thompson, Brendan Hannon | Computer science, Code (set theory), Coding (social sciences), Stock (firearms), Natural language, Dimension (graph theory), +8 more | https://doi.org/10.1109/qrs-c60940.2023.00047 | In the evolving field of AI, the advent of Large Language Models (LLMs) and Natural Language Coding (NLC) represents a revolutionary step in programming and computational linguistics. This research, conducted by early-career scholars and their mentors, constructs a self-reliant stock trading bot using Transformer Neural Networks, AutoGPT, and the Alpaca API, exploring the potentials of a No-Code/Code-Free AI framework rooted in NLC. This investigation assesses the effectiveness of this approach in stock trading influenced by social media dynamics and compares it with traditional trading mechanisms. Initially focused on the financial analysis of the platform "X", formerly Twitter, the study has adapted to the continuous transformations in social media landscapes and advancements in AI. The results highlight the extensive capabilities of LLMs in tasks such as code generation, data analyti… |
| https://openalex.org/W3035529900 | Relational Graph Attention Network for Aspect-based Sentiment Analysis | 2020 |  | preprint | 592 | yes | Kai Wang, Weizhou Shen, Yunyi Yang, Xiaojun Quan, Rui Wang | Computer science, Sentiment analysis, ENCODE, Sentence, Parsing, Artificial intelligence, +20 more | https://doi.org/10.18653/v1/2020.acl-main.295 | Aspect-based sentiment analysis aims to determine the sentiment polarity towards a specific aspect in online reviews. Most recent efforts adopt attention-based neural network models to implicitly connect aspects with opinion words. However, due to the complexity of language and the existence of multiple aspects in a single sentence, these models often confuse the connections. In this paper, we address this problem by means of effective encoding of syntax information. Firstly, we define a unified aspect-oriented dependency tree structure rooted at a target aspect by reshaping and pruning an ordinary dependency parse tree. Then, we propose a relational graph attention network (R-GAT) to encode the new tree structure for sentiment prediction. Extensive experiments are conducted on the SemEval 2014 and Twitter datasets, and the experimental results confirm that the connections between aspec… |
| https://openalex.org/W4387870294 | Harnessing Natural Language Processing for Mental Health Detection in Malay Text: A Review | 2023 |  | review | 7 | no | Zaaba Ahmad, Ruhaila Maskat, Azlinah Mohamed | Malay, Mental health, Computer science, Psychological intervention, Artificial intelligence, Natural language processing, +5 more | https://doi.org/10.1109/aidas60501.2023.10284653 | This paper comprehensively examines mental health detection in the Malay language using natural language processing (NLP) techniques. With global implications, mental health holds significant importance in Malay-speaking regions. NLP, a specialised branch of artificial intelligence, shows promise in deciphering mental health issues from text data. The review begins by exploring traditional NLP approaches like word frequencies, illustrating their role in mental health research. It then focuses on advanced techniques such as embeddings, neural networks, and transformer-based language models. The paper discusses prevalent mental health disorders in Malay-speaking communities and the challenges in their detection. It also highlights distinctive features of Malay mental health datasets crucial for NLP model development. The review delves into studies utilising NLP to analyse mental health co… |
| https://openalex.org/W4404809372 | NAS-PED: Neural Architecture Search for Pedestrian Detection | 2024 | IEEE Transactions on Pattern Analysis and Machine Intelligence | article | 8 | no | Yi Tang, Min Liu, Baopu Li, Yaonan Wang, Wanli Ouyang | Pedestrian detection, Artificial intelligence, Computer science, Pedestrian, Architecture, Object detection, +8 more | https://doi.org/10.1109/tpami.2024.3507918 | Pedestrian detection currently suffers from two issues in crowded scenes: occlusion and dense boundary prediction, making it still challenging in complex real-world scenarios. In recent years, Convolutional Neural Networks (CNN) and Vision Transformers (ViT) have shown their superiorities in addressing these issues, where ViTs capture global feature dependency to infer occlusion parts and CNNs make accurate dense predictions by local detailed features. Nevertheless, limited by the narrow receptive field, CNNs fail to infer occlusion parts, while ViTs tend to ignore local features that are vital to distinguish different pedestrians in the crowd. Therefore, it is essential to combine the advantages of CNN and ViT for pedestrian detection. However, manually designing a specific CNN and ViT hybrid network requires enormous time and resources for trial and error. To address this issue, we pr… |
| https://openalex.org/W4221166942 | UniXcoder: Unified Cross-Modal Pre-training for Code Representation | 2022 | Proceedings of the 60th Annual Meeting of the Association for Computational Lin… | article | 490 | yes | Daya Guo, Shuai Lu, Nan Duan, Yanlin Wang, Ming Zhou, Jian Yin | Computer science, Code generation, Code (set theory), Representation (politics), Encoder, ENCODE, +25 more | https://doi.org/10.18653/v1/2022.acl-long.499 | Pre-trained models for programming languages have recently demonstrated great success on code intelligence. To support both code-related understanding and generation tasks, recent works attempt to pre-train unified encoder-decoder models. However, such encoder-decoder framework is sub-optimal for auto-regressive tasks, especially code completion that requires a decoder-only manner for efficient inference. In this paper, we present UniXcoder, a unified cross-modal pre-trained model for programming language. The model utilizes mask attention matrices with prefix adapters to control the behavior of the model and leverages cross-modal contents like AST and code comment to enhance code representation. To encode AST that is represented as a tree in parallel, we propose a one-to-one mapping method to transform AST in a sequence structure that retains all structural information from the tree. F… |
| https://openalex.org/W3035625205 | A Unified MRC Framework for Named Entity Recognition | 2020 |  | preprint | 605 | yes | Xiaoya Li, Jingrong Feng, Yuxian Meng, Qinghong Han, Fei Wu, Jiwei Li | Named-entity recognition, Computer science, Task (project management), Question answering, Entity linking, Artificial intelligence, +12 more | https://doi.org/10.18653/v1/2020.acl-main.519 | The task of named entity recognition (NER) is normally divided into nested NER and flat NER depending on whether named entities are nested or not.Models are usually separately developed for the two tasks, since sequence labeling models, the most widely used backbone for flat NER, are only able to assign a single label to a particular token, which is unsuitable for nested NER where a token may be assigned several labels. In this paper, we propose a unified framework that is capable of handling both flat and nested NER tasks. Instead of treating the task of NER as a sequence labeling problem, we propose to formulate it as a machine reading comprehension (MRC) task. For example, extracting entities with the per label is formalized as extracting answer spans to the question “which person is mentioned in the text".This formulation naturally tackles the entity overlapping issue in nested NER:… |
| https://openalex.org/W4360980141 | Examining Science Education in ChatGPT: An Exploratory Study of Generative Artificial Intelligence | 2023 | Journal of Science Education and Technology | article | 1049 | yes | Grant Cooper | Transformative learning, Science education, Rubric, Engineering ethics, Narrative, Generative grammar, +8 more | https://doi.org/10.1007/s10956-023-10039-y | Abstract The advent of generative artificial intelligence (AI) offers transformative potential in the field of education. The study explores three main areas: (1) How did ChatGPT answer questions related to science education? (2) What are some ways educators could utilise ChatGPT in their science pedagogy? and (3) How has ChatGPT been utilised in this study, and what are my reflections about its use as a research tool? This exploratory research applies a self-study methodology to investigate the technology. Impressively, ChatGPT’s output often aligned with key themes in the research. However, as it currently stands, ChatGPT runs the risk of positioning itself as the ultimate epistemic authority, where a single truth is assumed without a proper grounding in evidence or presented with sufficient qualifications. Key ethical concerns associated with AI include its potential environmental im… |
| https://openalex.org/W3034379414 | FLAT: Chinese NER Using Flat-Lattice Transformer | 2020 |  | article | 416 | yes | Xiaonan Li, Hang Yan, Xipeng Qiu, Xuanjing Huang | Lattice (music), Computer science, Transformer, Computation, Inference, Algorithm, +6 more | https://doi.org/10.18653/v1/2020.acl-main.611 | Recently, the character-word lattice structure has been proved to be effective for Chinese named entity recognition (NER) by incorporating the word information. However, since the lattice structure is complex and dynamic, the lattice-based models are hard to fully utilize the parallel computation of GPUs and usually have a low inference speed. In this paper, we propose FLAT: Flat-LAttice Transformer for Chinese NER, which converts the lattice structure into a flat structure consisting of spans. Each span corresponds to a character or latent word and its position in the original lattice. With the power of Transformer and well-designed position encoding, FLAT can fully leverage the lattice information and has an excellent parallel ability. Experiments on four datasets show FLAT outperforms other lexicon-based models in performance and efficiency. |
| https://openalex.org/W3122241445 | What Makes Good In-Context Examples for GPT-3? | 2022 |  | preprint | 457 | yes | Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, Weizhu Chen | Deep learning, Zhàng, Context (archaeology), Artificial intelligence, Computer science, Chen, +8 more | https://doi.org/10.18653/v1/2022.deelio-1.10 | Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, Weizhu Chen. Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures. 2022. |
| https://openalex.org/W4385430679 | RT-1: Robotics Transformer for Real-World Control at Scale | 2023 |  | article | 435 | yes | Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, +43 more | Robotics, Artificial intelligence, Transformer, Computer science, Scale (ratio), Control engineering, +6 more | https://doi.org/10.15607/rss.2023.xix.025 | By transferring knowledge from large, diverse, taskagnostic datasets, modern machine learning models can solve specific downstream tasks either zero-shot or with small taskspecific datasets to a high level of performance.While this capability has been demonstrated in other fields such as computer vision, natural language processing or speech recognition, it remains to be shown in robotics, where the generalization capabilities of the models are particularly critical due to the difficulty of collecting real-world robotic data.We argue that one of the keys to the success of such general robotic models lies with open-ended task-agnostic training, combined with highcapacity architectures that can absorb all of the diverse, robotic data.In this paper, we present a model class, dubbed Robotics Transformer, that exhibits promising scalable model properties.We verify our conclusions in a study… |
| https://openalex.org/W4321488354 | Causal Language Model Aided Sequential Decoding With Natural Redundancy | 2023 | IEEE Transactions on Communications | article | 8 | no | Zhaorui Zhu, Hongyi Yu, Caiyao Shen, Jianping Du, Zhixiang Shen, Zhenyu Wang | Computer science, Decoding methods, Language model, Natural language, Exploit, Lexical analysis, +9 more | https://doi.org/10.1109/tcomm.2023.3247733 | A high-performance communication receiver desires a sufficient and accurate recognition of transmitted sources. In this paper, we propose a sequential decoding algorithm for the robust reception of sources with natural redundancy (NR) over the AWGN channel. To fully exploit the abundant NR in the exemplified English text sources, a causal language modeling (CLM) with a powerful Transformer decoder neural network (NN) structure is adopted for modeling the joint probability distribution. For versatility of byte-level tokenization, the UTF-8 encoding scheme is considered for the Wikipedia corpus, namely the English edition of the Wiki-40B dataset. The tree search-based M-algorithm (MA) is integrated with CLM, denoted as CLM-MA algorithm, to synthesize the a priori probability of information sequence and likelihood values of polluted symbols. Both simulation experiments and hardware platfor… |
| https://openalex.org/W3203366473 | MoEfication: Conditional Computation of Transformer Models for Efficient Inference | 2021 | arXiv (Cornell University) | preprint | 6 | yes | Zhengyan Zhang, Yankai Lin, Zhiyuan Liu, Peng Li, Maosong Sun, Jie Zhou | Computation, Inference, Computer science, Transformer, Artificial intelligence, Language model, +5 more | https://arxiv.org/pdf/2110.01786.pdf | Transformer-based pre-trained language models can achieve superior performance on most NLP tasks due to large parameter capacity, but also lead to huge computation cost. Fortunately, we find by empirical study that, most inputs only activate a tiny ratio of neurons during inference. Hence, we explore to accelerate large-model inference by conditional computation based on the sparse activation phenomenon. We propose to transform a large model into its mixture-of-experts (MoE) version with equal model size, namely MoEfication. Model MoEfication consists of two steps: (1) splitting the parameters of feed-forward neural networks (FFNs) into multiple parts as experts, and (2) building expert routers to decide which experts will be used for each input. To further improve the performance of MoEfied models, we can also fine-tune the models on downstream tasks, namely parameter calibration. Expe… |
| https://openalex.org/W4412094103 | Comparative Study on Energy Consumption of Neural Networks by Scaling of Weight-Memory Energy Versus Computing Energy for Implementing Low-Power Edge Intelligence | 2025 | Electronics | article | 4 | yes | I. Yoon, Jihwan Mun, Kyeong‐Sik Min | Energy consumption, Artificial neural network, Enhanced Data Rates for GSM Evolution, Scaling, Computer science, Energy (signal processing), +11 more | https://doi.org/10.3390/electronics14132718 | Energy consumption has emerged as a critical design constraint in deploying high-performance neural networks, especially on edge devices with limited power resources. In this paper, a comparative study is conducted for two prevalent deep learning paradigms—convolutional neural networks (CNNs), exemplified by ResNet18, and transformer-based large language models (LLMs), represented by GPT3-small, Llama-7B, and GPT3-175B. By analyzing how the scaling of memory energy versus computing energy affects the energy consumption of neural networks with different batch sizes (1, 4, 8, 16), it is shown that ResNet18 transitions from a memory energy-limited regime at low batch sizes to a computing energy-limited regime at higher batch sizes due to its extensive convolution operations. On the other hand, GPT-like models remain predominantly memory-bound, with large parameter tensors and frequent key–… |
| https://openalex.org/W3025222916 | An evaluation of recent neural sequence tagging models in Turkish named entity recognition | 2021 | Expert Systems with Applications | preprint | 2 | yes | Gizem Aras, Didem Makaroğlu, Şeniz Demir, A. Çakır | Named-entity recognition, Conditional random field, Computer science, Transformer, Turkish, Natural language processing, +15 more | https://doi.org/10.1016/j.eswa.2021.115049 |  |
| https://openalex.org/W3139434170 | TransFG: A Transformer Architecture for Fine-Grained Recognition | 2022 | Proceedings of the AAAI Conference on Artificial Intelligence | article | 452 | yes | Ju He, Jie-Neng Chen, Shuai Liu, Adam Kortylewski, Cheng Yang, Yutong Bai, Changhu Wang | Transformer, Computer science, Discriminative model, Security token, Artificial intelligence, Locality, +11 more | https://doi.org/10.1609/aaai.v36i1.19967 | Fine-grained visual classification (FGVC) which aims at recognizing objects from subcategories is a very challenging task due to the inherently subtle inter-class differences. Most existing works mainly tackle this problem by reusing the backbone network to extract features of detected discriminative regions. However, this strategy inevitably complicates the pipeline and pushes the proposed regions to contain most parts of the objects thus fails to locate the really important parts. Recently, vision transformer (ViT) shows its strong performance in the traditional classification task. The self-attention mechanism of the transformer links every patch token to the classification token. In this work, we first evaluate the effectiveness of the ViT framework in the fine-grained recognition setting. Then motivated by the strength of the attention link can be intuitively considered as an indic… |
| https://openalex.org/W3127657277 | Artificial intelligence-enhanced electrocardiography in cardiovascular disease management | 2021 | Nature Reviews Cardiology | review | 782 | yes | Konstantinos C. Siontis, Peter A. Noseworthy, Zachi I. Attia, Paul A. Friedman | Medicine, Disease, Precision medicine, Deep learning, Atrial fibrillation, Artificial intelligence, +9 more | https://doi.org/10.1038/s41569-020-00503-2 |  |
| https://openalex.org/W4311393073 | Self-Supervised Learning for Solar Radio Spectrum Classification | 2022 | Universe | article | 7 | yes | Siqi Li, Guowu Yuan, Jian Chen, Chengming Tan, Hao Zhou | Transfer of learning, Supervised learning, Artificial intelligence, Computer science, Solar radio, Space weather, +5 more | https://doi.org/10.3390/universe8120656 | Solar radio observation is an important way to study the Sun. Solar radio bursts contain important information about solar activity. Therefore, real-time automatic detection and classification of solar radio bursts are of great value for subsequent solar physics research and space weather warnings. Traditional image classification methods based on deep learning often require considerable training data. To address insufficient solar radio spectrum images, transfer learning is generally used. However, the large difference between natural images and solar spectrum images has a large impact on the transfer learning effect. In this paper, we propose a self-supervised learning method for solar radio spectrum classification. Our method uses self-supervised training with a self-masking approach in natural language processing. Self-supervised learning is more conducive to learning the essential… |
| https://openalex.org/W4385573704 | Enriching Deep Learning with Frame Semantics for Empathy Classification in Medical Narrative Essays | 2022 |  | article | 4 | yes | Priyanka Dey, Roxana Gîrju | Empathy, Narrative, Computer science, Semantics (computer science), Cognition, Natural language processing, +17 more | https://doi.org/10.18653/v1/2022.louhi-1.23 | Empathy is a vital component of health care and plays a key role in the training of future doctors. Paying attention to medical students’ self-reflective stories of their interactions with patients can encourage empathy and the formation of professional identities that embody desirable values such as integrity and respect. We present a computational approach and linguistic analysis of empathic language in a large corpus of 440 essays written by pre-med students as narrated simulated patient – doctor interactions. We analyze the discourse of three kinds of empathy: cognitive, affective, and prosocial as highlighted by expert annotators. We also present various experiments with state-of-the-art recurrent neural networks and transformer models for classifying these forms of empathy. To further improve over these results, we develop a novel system architecture that makes use of frame semant… |
| https://openalex.org/W4411423477 | Enhancing Air Compressor Fault Diagnosis: A Comparative Study of GPT‐2 and Traditional Machine Learning Models | 2025 | Macromolecular Symposia | article | 4 | yes | Nima Rezazadeh, Donato Perfetto, Francesco Caputo, Alessandro De Luca | Interpretability, Hyperparameter, Computer science, Machine learning, Artificial intelligence, Artificial neural network, +14 more | https://doi.org/10.1002/masy.70057 | Abstract This study investigates the application of GPT‐2, a transformer‐based model, for fault diagnosis in reciprocating air compressors, highlighting its ability to capture complex patterns in acoustic signals. Two approaches are compared: the first involves extracting time, frequency, and complexity‐based features and classifying them using traditional machine learning models, with a narrow neural network achieving the best performance. The second approach reformulates these features as sequential data for GPT‐2, which, through meticulous hyperparameter optimization, delivered superior diagnostic accuracy. Additionally, SHapley Additive exPlanations analysis was employed to enhance model interpretability by identifying the most influential features, providing valuable insights into the fault diagnosis process. While GPT‐2 demonstrated notable performance gains over conventional mode… |
| https://openalex.org/W4389708971 | 4M: Massively Multimodal Masked Modeling | 2023 | arXiv (Cornell University) | preprint | 8 | yes | David Mizrahi, Roman Bachmann, Oğuzhan Fatih Kar, Teresa Yeo, Mingfei Gao, Afshin Dehghan, Amir Zamir | Computer science, Modalities, Artificial intelligence, Scalability, Modality (human–computer interaction), Set (abstract data type), +11 more | http://arxiv.org/abs/2312.06647 | Current machine learning models for vision are often highly specialized and limited to a single modality and task. In contrast, recent large language models exhibit a wide range of capabilities, hinting at a possibility for similarly versatile models in computer vision. In this paper, we take a step in this direction and propose a multimodal training scheme called 4M. It consists of training a single unified Transformer encoder-decoder using a masked modeling objective across a wide range of input/output modalities - including text, images, geometric, and semantic modalities, as well as neural network feature maps. 4M achieves scalability by unifying the representation space of all modalities through mapping them into discrete tokens and performing multimodal masked modeling on a small randomized subset of tokens. 4M leads to models that exhibit several key capabilities: (1) they can pe… |
| https://openalex.org/W4295539513 | Improving N-Best Rescoring in Under-Resourced Code-Switched Speech Recognition Using Pretraining and Data Augmentation | 2022 | Languages | article | 5 | yes | Joshua Jansen van Vüren, Thomas Niesler | Computer science, Transformer, Bantu languages, Speech recognition, Language model, Code (set theory), +11 more | https://doi.org/10.3390/languages7030236 | In this study, we present improvements in N-best rescoring of code-switched speech achieved by n-gram augmentation as well as optimised pretraining of long short-term memory (LSTM) language models with larger corpora of out-of-domain monolingual text. Our investigation specifically considers the impact of the way in which multiple monolingual datasets are interleaved prior to being presented as input to a language model. In addition, we consider the application of large pretrained transformer-based architectures, and present the first investigation employing these models in English-Bantu code-switched speech recognition. Our experimental evaluation is performed on an under-resourced corpus of code-switched speech comprising four bilingual code-switched sub-corpora, each containing a Bantu language (isiZulu, isiXhosa, Sesotho, or Setswana) and English. We find in our experiments that, by… |
| https://openalex.org/W4387994502 | The Expressive Power of Low-Rank Adaptation | 2023 | arXiv (Cornell University) | preprint | 5 | yes | Yuchen Zeng, Kangwook Lee | Rank (graph theory), Embedding, Computer science, Language model, Transformer, Algorithm, +11 more | http://arxiv.org/abs/2310.17513 | Low-Rank Adaptation (LoRA), a parameter-efficient fine-tuning method that leverages low-rank adaptation of weight matrices, has emerged as a prevalent technique for fine-tuning pre-trained models such as large language models and diffusion models. Despite its huge success in practice, the theoretical underpinnings of LoRA have largely remained unexplored. This paper takes the first step to bridge this gap by theoretically analyzing the expressive power of LoRA. We prove that, for fully connected neural networks, LoRA can adapt any model $f$ to accurately represent any smaller target model $\overline{f}$ if LoRA-rank $\geq(\text{width of }f) \times \frac{\text{depth of }\overline{f}}{\text{depth of }f}$. We also quantify the approximation error when LoRA-rank is lower than the threshold. For Transformer networks, we show any model can be adapted to a target model of the same size with ra… |
| https://openalex.org/W4295037258 | Transformer-based Video Summarization With Spatial-Temporal Representation | 2022 |  | article | 8 | no | Suru Feng, Yuxiang Xie, Yingmei Wei, Jie Yan, Qi Wang | Automatic summarization, Computer science, Artificial intelligence, Convolutional neural network, Deep learning, Video processing, +8 more | https://doi.org/10.1109/bigdia56350.2022.9874248 | Video summarization is an important topic studied by researchers. With the application of deep learning, CNN and RNN have also been used to generate video summarization. However, because a video contains many frames and the video timing span is large, the spatial-temporal architecture of the video is complex, but it is necessary to abstract the spatial-temporal structure information to generate a summarization, and it is also the focus of the researchers recently. Based on previous researchers' research, we put forward a new way for video summary generation, which consists of three deep neural network models. First, a 2D convolutional CNN is used to process video frames, convert a short video into a vector form that can be flexibly calculated, and then use 1D convolution to perform sequence analysis on the timing information of data, and then use the Transformer encorder model that is c… |
| https://openalex.org/W2943495267 | Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences | 2019 |  | preprint | 394 | yes | Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott, +3 more | Artificial intelligence, Representation (politics), Computer science, Unsupervised learning, Machine learning, Generative model, +15 more | https://doi.org/10.1101/622803 | Abstract In the field of artificial intelligence, a combination of scale in data and model capacity enabled by un-supervised learning has led to major advances in representation learning and statistical generation. In the life sciences, the anticipated growth of sequencing promises unprecedented data on natural sequence diversity. Protein language modeling at the scale of evolution is a logical step toward predictive and generative artificial intelligence for biology. To this end we use unsupervised learning to train a deep contextual language model on 86 billion amino acids across 250 million protein sequences spanning evolutionary diversity. The resulting model contains information about biological properties in its representations. The representations are learned from sequence data alone. The learned representation space has a multi-scale organization reflecting structure from the le… |
| https://openalex.org/W2998183051 | Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT | 2020 | Proceedings of the AAAI Conference on Artificial Intelligence | article | 404 | yes | Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W. Mahoney, Kurt Keutzer | Computer science, Hessian matrix, Transformer, Quantization (signal processing), Embedding, Encoder, +9 more | https://doi.org/10.1609/aaai.v34i05.6409 | Transformer based architectures have become de-facto models used for a range of Natural Language Processing tasks. In particular, the BERT based models achieved significant accuracy gain for GLUE tasks, CoNLL-03 and SQuAD. However, BERT based models have a prohibitive memory footprint and latency. As a result, deploying BERT based models in resource constrained environments has become a challenging task. In this work, we perform an extensive analysis of fine-tuned BERT models using second order Hessian information, and we use our results to propose a novel method for quantizing BERT models to ultra low precision. In particular, we propose a new group-wise quantization scheme, and we use Hessian-based mix-precision method to compress the model further. We extensively test our proposed method on BERT downstream tasks of SST-2, MNLI, CoNLL-03, and SQuAD. We can achieve comparable performan… |
| https://openalex.org/W4399354847 | A Comparative Study of CNN, ResNet, and Vision Transformers for Multi-Classification of Chest Diseases | 2024 | arXiv (Cornell University) | preprint | 4 | yes | Ananya Jain, Aviral Bhardwaj, K. R. Murali, Isha Surani | Residual neural network, Artificial intelligence, Transformer, Computer science, Pattern recognition (psychology), Deep learning, +3 more | http://arxiv.org/abs/2406.00237 | Large language models, notably utilizing Transformer architectures, have emerged as powerful tools due to their scalability and ability to process large amounts of data. Dosovitskiy et al. expanded this architecture to introduce Vision Transformers (ViT), extending its applicability to image processing tasks. Motivated by this advancement, we fine-tuned two variants of ViT models, one pre-trained on ImageNet and another trained from scratch, using the NIH Chest X-ray dataset containing over 100,000 frontal-view X-ray images. Our study evaluates the performance of these models in the multi-label classification of 14 distinct diseases, while using Convolutional Neural Networks (CNNs) and ResNet architectures as baseline models for comparison. Through rigorous assessment based on accuracy metrics, we identify that the pre-trained ViT model surpasses CNNs and ResNet in this multilabel class… |
| https://openalex.org/W3031696893 | Attention in Natural Language Processing | 2020 | IEEE Transactions on Neural Networks and Learning Systems | article | 607 | yes | Andrea Galassi, Marco Lippi, Paolo Torroni | Computer science, Categorization, Artificial intelligence, Variety (cybernetics), Natural language, Natural language understanding, +3 more | https://doi.org/10.1109/tnnls.2020.3019893 | Attention is an increasingly popular mechanism used in a wide range of neural architectures. The mechanism itself has been realized in a variety of formats. However, because of the fast-paced advances in this domain, a systematic overview of attention is still missing. In this article, we define a unified model for attention architectures in natural language processing, with a focus on those designed to work with vector representations of the textual data. We propose a taxonomy of attention models according to four dimensions: the representation of the input, the compatibility function, the distribution function, and the multiplicity of the input and/or output. We present the examples of how prior information can be exploited in attention models and discuss ongoing research efforts and open challenges in the area, providing the first extensive categorization of the vast body of literatu… |
| https://openalex.org/W4367598041 | Comparing Vision Transformers and Convolutional Neural Networks for Image Classification: A Literature Review | 2023 | Applied Sciences | review | 470 | yes | José Maurício, Inês Domingues, Jorge Bernardino | Computer science, Convolutional neural network, Artificial intelligence, Contextual image classification, Transformer, Weighting, +8 more | https://doi.org/10.3390/app13095521 | Transformers are models that implement a mechanism of self-attention, individually weighting the importance of each part of the input data. Their use in image classification tasks is still somewhat limited since researchers have so far chosen Convolutional Neural Networks for image classification and transformers were more targeted to Natural Language Processing (NLP) tasks. Therefore, this paper presents a literature review that shows the differences between Vision Transformers (ViT) and Convolutional Neural Networks. The state of the art that used the two architectures for image classification was reviewed and an attempt was made to understand what factors may influence the performance of the two deep learning architectures based on the datasets used, image size, number of target classes (for the classification problems), hardware, and evaluated architectures and top results. The obje… |
| https://openalex.org/W4386821109 | Guest Editorial Neurosymbolic AI for Sentiment Analysis | 2023 | IEEE Transactions on Affective Computing | editorial | 3 | yes | Frank Xing, Björn W. Schuller, Iti Chaturvedi, Erik Cambria, Amir Hussain | Word2vec, Computer science, Sentiment analysis, Artificial intelligence, Artificial neural network, Spotting, +9 more | http://dx.doi.org/10.1109/taffc.2023.3310856 | Neural network-based methods, especially deep learning, have been a burgeoning area in AI research and have been successful in tackling the expanding data volume as we move into a digital age. Today, the neural network-based methods are not only used for low-level cognitive tasks, such as recognizing objects and spotting keywords, but they have also been deployed in various industrial information systems to assist high-level decision-making. In natural language processing, there have been two milestones for the past decade: one is word2vec [1], a group of neural models that learn word embeddings (vector representations of words) from large datasets; and one is the most recent GPT-based models [2], which combine reinforcement learning with a generative transformer in order to enable multi-round end-to-end conversations. While producing highly accurate predictions on datasets and generati… |
| https://openalex.org/W3155235415 | Introducing A large Tunisian Arabizi Dialectal Dataset for Sentiment Analysis | 2021 |  | article | 5 | no | Chayma Fourati, Hatem Haddad, Abir Messaoudi, Moez BenHajhmida, Aymen Ben Elhaj Mabrouk, Malek Naski | Computer science, Artificial intelligence, Natural language processing, Classifier (UML), Encoder, Arabic, +9 more | https://www.aclweb.org/anthology/2021.wanlp-1.25/ | On various Social Media platforms, people, tend to use the informal way to communicate, or write posts and comments: their local dialects. In Africa, more than 1500 dialects and languages exist. Particularly, Tunisians talk and write informally using Latin letters and numbers rather than Arabic ones. In this paper, we introduce a large common-crawl-based Tunisian Arabizi dialectal dataset dedicated for Sentiment Analysis. The dataset consists of a total of 100k comments (about movies, politic, sport, etc.) annotated manually by Tunisian native speakers as Positive, negative and Neutral. We evaluate our dataset on sentiment analysis task using the Bidirectional Encoder Representations from Transformers (BERT) as a contextual language model in its multilingual version (mBERT) as an embedding technique then combining mBERT with Convolutional Neural Network (CNN) as classifier. The dataset… |
| https://openalex.org/W4283170666 | Taxonomy of Risks posed by Language Models | 2022 | 2022 ACM Conference on Fairness, Accountability, and Transparency | article | 491 | yes | Laura Weidinger, Jonathan Uesato, Maribeth Rauh, Conor Griffin, Po-Sen Huang, John Mellor, Amelia Glaese, Myra Cheng, +15 more | Misinformation, Taxonomy (biology), Harm, Risk analysis (engineering), Futures studies, Computer science, +14 more | https://doi.org/10.1145/3531146.3533088 | Responsible innovation on large-scale Language Models (LMs) re- quires foresight into and in-depth understanding of the risks these models may pose. This paper develops a comprehensive taxon- omy of ethical and social risks associated with LMs. We identify twenty-one risks, drawing on expertise and literature from com- puter science, linguistics, and the social sciences. We situate these risks in our taxonomy of six risk areas: I. Discrimination, Hate speech and Exclusion, II. Information Hazards, III. Misinformation Harms, IV. Malicious Uses, V. Human-Computer Interaction Harms, and VI. Environmental and Socioeconomic harms. For risks that have already been observed in LMs, the causal mechanism leading to harm, evidence of the risk, and approaches to risk mitigation are discussed. We further describe and analyse risks that have not yet been observed but are anticipated based on assessm… |
| https://openalex.org/W4386489065 | Analyzing Chatgpt Based on Large Language Model from Industrial Perspective | 2023 | SSRN Electronic Journal | preprint | 6 | yes | Lakshita Aggarwal, Urvi Vasisht, Rahul Kanwar, Arun Kumar, Puneet Goswami | Perspective (graphical), Computer science, Linguistics, Artificial intelligence, Philosophy | https://doi.org/10.2139/ssrn.4563696 |  |
| https://openalex.org/W4281763794 | A systematic evaluation of large language models of code | 2022 |  | article | 461 | yes | Frank F. Xu, Uri Alon, Graham Neubig, Vincent J. Hellendoorn | Computer science, Programming language, Source code, Code (set theory), Natural language, Open source, +5 more | https://doi.org/10.1145/3520312.3534862 | Large language models (LMs) of code have recently shown tremendous promise in completing code and synthesizing code from natural language descriptions. However, the current state-of-the-art code LMs (e.g., Codex) are not publicly available, leaving many questions about their model and data design decisions. We aim to fill in some of these blanks through a systematic evaluation of the largest existing models: Codex, GPT-J, GPT-Neo, GPT-NeoX-20B, and CodeParrot, across various programming languages. Although Codex itself is not open-source, we find that existing opensource models do achieve close results in some programming languages, although targeted mainly for natural language modeling. We further identify an important missing piece in the form of a large open-source model trained exclusively on a multi-lingual corpus of code. We release a new model, PolyCoder, with 2.7B parameters bas… |
| https://openalex.org/W4214879735 | FarSick: A Persian Semantic Textual Similarity And Natural Language Inference Dataset | 2021 |  | article | 4 | no | Zahra Ghasemi, Mohammad Ali Keyvanrad | Computer science, Artificial intelligence, Natural language processing, Semantic similarity, Automatic summarization, Machine translation, +9 more | https://doi.org/10.1109/iccke54056.2021.9721521 | Semantic textual similarity(STS) and natural language inference(NLI) are important tasks in natural language processing(NLP) such as information retrieval, text classification, subject extraction, text summarization, machine translation and plagiarism detection. Lack of appropriate datasets in the Persian language is a major obstacle to progress in this area. Therefore, in this paper, we present FarSick, a new dataset for STS and NLI tasks in the Persian language. FarSick is the first relatively large-scale STS dataset for the Persian language. It includes 9804 pairs of Persian sentences with labels for similarity and inference for each pair of sentences. This dataset is collected by translating and editing the sentences of SICK dataset. We also measured the performance of traditional, statistical and deep learning models on it, e.g. transformers, Convolution Neural Networks, Bidirectio… |
| https://openalex.org/W3175362188 | ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer | 2021 |  | article | 477 | yes | Yuanmeng Yan, Rumei Li, Sirui Wang, Fuzheng Zhang, Wei Wu, Weiran Xu | Natural language processing, Computer science, Sentence, Zhàng, Artificial intelligence, Linguistics, +12 more | https://doi.org/10.18653/v1/2021.acl-long.393 | Yuanmeng Yan, Rumei Li, Sirui Wang, Fuzheng Zhang, Wei Wu, Weiran Xu. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021. |
| https://openalex.org/W4410393770 | Taming Mambas for 3D Medical Image Segmentation | 2025 | IEEE Access | article | 3 | yes | Luca Lumetti, Vittorio Pipoli, Kevin Marchesini, Elisa Ficarra, Costantino Grana, Federico Bolelli | Computer science, Computer vision, Image segmentation, Artificial intelligence, Scale-space segmentation, Segmentation | https://doi.org/10.1109/access.2025.3570461 | Recently, the field of 3D medical segmentation has been dominated by deep learning models employing Convolutional Neural Networks (CNNs) and Transformer-based architectures, each with its distinctive strengths and limitations. CNNs are constrained by a local receptive field, whereas Transformers are hindered by their substantial memory requirements as well as their data hunger, making them not ideal for processing 3D medical volumes at a fine-grained level. For these reasons, fully convolutional neural networks, as nnU-Net, still dominate the scene when segmenting medical structures in large 3D medical volumes. Despite numerous advancements toward developing Transformer variants with subquadratic time and memory complexity, these models still fall short in content-based reasoning. A recent breakthrough is Mamba, a Recurrent Neural Network (RNN) based on State-Space Models (SSMs), outper… |
| https://openalex.org/W3133667170 | Enhancing Prosodic Features by Adopting Pre-trained Language Model in Bahasa Indonesia Speech Synthesis | 2020 |  | article | 6 | no | Lixuan Zhao, Jian Yang, Qinglai Qin | Computer science, Speech recognition, Speech synthesis, Transformer, Encoder, Artificial neural network, +8 more | https://doi.org/10.1145/3446132.3446196 | Deep neural network text-to-speech (TTS) systems can produce high-quality audio. However, modern TTS systems usually need a sizable of studio-quality <text, audio> pairs as input. In view of the insufficient research on Bahasa Indonesia, available data are usually worse in term of both quality and size. The End-to-End(E2E) TTS systems trained on those corpora are difficult to generate satisfactory speech, especially the prosodic features are not obvious. Therefore, we propose a method to enhance the prosodic features of synthesized speech based on GST-Tacotron2 model, and pre-trained language model with the BERT (Bidirectional Encoder Representation from Transformers) model. The BERT learned from large number of unlabeled text data contains rich linguistic information, which can help TTS systems produce the more obvious prosodic features. The subjective evaluation of our experimental re… |
| https://openalex.org/W4407095871 | Classification of Pediatric Dental Diseases from Panoramic Radiographs using Natural Language Transformer and Deep Learning Models | 2025 |  | preprint | 4 | yes | Tuan D. Pham | Radiography, Transformer, Computer science, Orthodontics, Artificial intelligence, Deep learning, +6 more | https://doi.org/10.1101/2025.01.30.25321418 | Abstract Accurate classification of pediatric dental diseases from panoramic radiographs is crucial for early diagnosis and treatment planning. This study explores a text-based approach using a natural language transformer to generate textual descriptions of radiographs, which are then classified using deep learning models. Three models were evaluated: a one-dimensional convolutional neural network (1D-CNN), a long short-term memory (LSTM) network, and a pretrained bidirectional encoder representations from transformer (BERT) model for binary disease classification. Results showed that BERT achieved 77% accuracy, excelling in detecting periapical infections but struggling with caries identification. The 1D-CNN outperformed BERT with 84% accuracy, providing a more balanced classification, while the LSTM model achieved only 57% accuracy. Both 1D-CNN and BERT surpassed three pretrained CNN… |
| https://openalex.org/W3025165719 | Conformer: Convolution-augmented Transformer for Speech Recognition | 2020 | arXiv (Cornell University) | preprint | 377 | yes | Anmol Gulati, James Qin, Chung‐Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, +3 more | Transformer, Computer science, Convolutional neural network, Language model, Speech recognition, Artificial neural network, +8 more | http://arxiv.org/abs/2005.08100 | Recently Transformer and Convolution neural network (CNN) based models have shown promising results in Automatic Speech Recognition (ASR), outperforming Recurrent neural networks (RNNs). Transformer models are good at capturing content-based global interactions, while CNNs exploit local features effectively. In this work, we achieve the best of both worlds by studying how to combine convolution neural networks and transformers to model both local and global dependencies of an audio sequence in a parameter-efficient way. To this regard, we propose the convolution-augmented transformer for speech recognition, named Conformer. Conformer significantly outperforms the previous Transformer and CNN based models achieving state-of-the-art accuracies. On the widely used LibriSpeech benchmark, our model achieves WER of 2.1%/4.3% without using a language model and 1.9%/3.9% with an external langua… |
| https://openalex.org/W4385452929 | From ChatGPT to ThreatGPT: Impact of Generative AI in Cybersecurity and Privacy | 2023 | IEEE Access | article | 609 | yes | Maanak Gupta, Charankumar Akiri, Kshitiz Aryal, Eli Parker, Lopamudra Praharaj | Computer science, Computer security, Information privacy, Generative grammar, Internet privacy, Artificial intelligence | https://doi.org/10.1109/access.2023.3300381 | Undoubtedly, the evolution of Generative AI (GenAI) models has been the highlight of digital transformation in the year 2022. As the different GenAI models like ChatGPT and Google Bard continue to foster their complexity and capability, it&#x2019;s critical to understand its consequences from a cybersecurity perspective. Several instances recently have demonstrated the use of GenAI tools in both the defensive and offensive side of cybersecurity, and focusing on the social, ethical and privacy implications this technology possesses. This research paper highlights the limitations, challenges, potential risks, and opportunities of GenAI in the domain of cybersecurity and privacy. The work presents the vulnerabilities of ChatGPT, which can be exploited by malicious users to exfiltrate malicious information bypassing the ethical constraints on the model. This paper demonstrates successful ex… |
| https://openalex.org/W4229011615 | A Topic Modeling Comparison Between LDA, NMF, Top2Vec, and BERTopic to Demystify Twitter Posts | 2022 | Frontiers in Sociology | article | 769 | yes | Roman Egger, Joanne Yu | Latent Dirichlet allocation, Social media, Topic model, Data science, Computer science, Computational sociology, +16 more | https://doi.org/10.3389/fsoc.2022.886498 | The richness of social media data has opened a new avenue for social science research to gain insights into human behaviors and experiences. In particular, emerging data-driven approaches relying on topic models provide entirely new perspectives on interpreting social phenomena. However, the short, text-heavy, and unstructured nature of social media content often leads to methodological challenges in both data collection and analysis. In order to bridge the developing field of computational science and empirical social research, this study aims to evaluate the performance of four topic modeling techniques; namely latent Dirichlet allocation (LDA), non-negative matrix factorization (NMF), Top2Vec, and BERTopic. In view of the interplay between human relations and digital media, this research takes Twitter posts as the reference point and assesses the performance of different algorithms c… |
| https://openalex.org/W7118770907 | A Novel Image Captioning Technique Using Deep Learning Methodology | 2025 | ICCK Transactions on Machine Intelligence | article | 4 | yes | Azam Khan, Jaswinder Singh | Closed captioning, Computer science, Artificial intelligence, Deep learning, Convolutional neural network, Boosting (machine learning), +19 more | https://doi.org/10.62762/tmi.2025.886122 | The capacity of robots to produce captions for images independently is a big step forward in the field of artificial intelligence and language understanding. This paper looks at an advanced picture captioning system that uses deep learning techniques, notably convolutional neural networks (CNNs) and recurrent neural networks (RNNs), to provide contextually appropriate and meaningful descriptions of visual content. The suggested technique extracts features using the DenseNet201 model, which allows for a more thorough and hierarchical comprehension of picture components. These collected characteristics are subsequently processed by a long short-term memory (LSTM) network, a specific RNN variation designed to capture sequential dependencies in language, resulting in captions that are coherent and fluent.The model is trained and assessed on the well-known Flickr8k dataset, attaining competi… |
| https://openalex.org/W4313163641 | Medical Named Entity Recognition using Surrounding Sequences Matching | 2022 | Procedia Computer Science | article | 13 | yes | Mohamed Yassine Landolsi, Lotfi Romdhane, Lobna Hlaoua | Computer science, Named-entity recognition, Conditional random field, Sentence, Natural language processing, Artificial intelligence, +20 more | https://doi.org/10.1016/j.procs.2022.09.122 | Since the development of information technologies, there is a huge amount of electronic documents that was written by medical specialists and are rich of useful information needed to make critical decisions in several medical tasks. Thus, a doctor must have a big knowledge and he is responsible for every decision he takes for patients. In fact, the doctor should read, with full concentration, many electronic narrative documents to collect the necessary information. Unfortunately, it's too tiring to read all necessary information about drugs, diseases and patient due to the large amount of documents that are increasing every day. Consequently, so many medical errors can happen and even can cause fatalities. On the other hand, information extraction is such a good field that can handle this problem. One of the most important main task in this field is the Named Entity Recognition (NER) an… |
| https://openalex.org/W3198381997 | Deep Learning for Anomaly Detection in Time-Series Data: Review, Analysis, and Guidelines | 2021 | IEEE Access | article | 500 | yes | Kukjin Choi, Jihun Yi, Changhwa Park, Sungroh Yoon | Anomaly detection, Computer science, Deep learning, Time series, Benchmark (surveying), Field (mathematics), +15 more | https://doi.org/10.1109/access.2021.3107975 | As industries become automated and connectivity technologies advance, a wide range of systems continues to generate massive amounts of data. Many approaches have been proposed to extract principal indicators from the vast sea of data to represent the entire system state. Detecting anomalies using these indicators on time prevent potential accidents and economic losses. Anomaly detection in multivariate time series data poses a particular challenge because it requires simultaneous consideration of temporal dependencies and relationships between variables. Recent deep learning-based works have made impressive progress in this field. They are highly capable of learning representations of the large-scaled sequences in an unsupervised manner and identifying anomalies from the data. However, most of them are highly specific to the individual use case and thus require domain knowledge for appr… |
| https://openalex.org/W4382462760 | Exploring CLIP for Assessing the Look and Feel of Images | 2023 | Proceedings of the AAAI Conference on Artificial Intelligence | article | 441 | yes | Jianyi Wang, Kelvin C. K. Chan, Chen Change Loy | Computer science, Perception, Task (project management), Quality (philosophy), Prior probability, Artificial intelligence, +12 more | https://doi.org/10.1609/aaai.v37i2.25353 | Measuring the perception of visual content is a long-standing problem in computer vision. Many mathematical models have been developed to evaluate the look or quality of an image. Despite the effectiveness of such tools in quantifying degradations such as noise and blurriness levels, such quantification is loosely coupled with human language. When it comes to more abstract perception about the feel of visual content, existing methods can only rely on supervised models that are explicitly trained with labeled data collected via laborious user study. In this paper, we go beyond the conventional paradigms by exploring the rich visual language prior encapsulated in Contrastive Language-Image Pre-training (CLIP) models for assessing both the quality perception (look) and abstract perception (feel) of images without explicit task-specific training. In particular, we discuss effective prompt d… |
| https://openalex.org/W4312915485 | A Survey of Abstractive Text Summarization Utilising Pretrained Language Models | 2022 | Lecture notes in computer science | book-chapter | 6 | no | Ayesha Ayub Syed, Ford Lumban Gaol, Alfred Boediman, Tokuro Matsuo, Widodo Budiharto | Automatic summarization, Computer science, Artificial intelligence, Natural language processing, Language model, Salient, +6 more | https://doi.org/10.1007/978-3-031-21743-2_42 |  |
| https://openalex.org/W4290876361 | GraphMAE: Self-Supervised Masked Graph Autoencoders | 2022 | Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data M… | article | 470 | yes | Zhenyu Hou, Xiao Liu, Yukuo Cen, Yuxiao Dong, Hongxia Yang, Chunjie Wang, Jie Tang | Computer science, Autoencoder, Generative grammar, Artificial intelligence, Graph, Machine learning, +9 more | https://doi.org/10.1145/3534678.3539321 | Self-supervised learning (SSL) has been extensively explored in recent years. Particularly, generative SSL has seen emerging success in natural language processing and other fields, such as the wide adoption of BERT and GPT. Despite this, contrastive learning---which heavily relies on structural data augmentation and complicated training strategies---has been the dominant approach in graph SSL, while the progress of generative SSL on graphs, especially graph autoencoders (GAEs), has thus far not reached the potential as promised in other fields. In this paper, we identify and examine the issues that negatively impact the development of GAEs, including their reconstruction objective, training robustness, and error metric. We present a masked graph autoencoder GraphMAE (code is publicly available at https://github.com/THUDM/GraphMAE) that mitigates these issues for generative self-supervi… |
| https://openalex.org/W4384407445 | Monitoring Transformer Condition with MLP Machine Learning Model | 2023 | Journal of Energy - Energija | article | 2 | yes | Dino Žanić, Alan Župan | Transformer, Computer science, Electromagnetic coil, Artificial neural network, Reliability engineering, Engineering, +3 more | http://dx.doi.org/10.37798/2023722417 | Failures of large power transformers in transmission system are always followed by significant costs, which is especially problematic because they present an unplanned expenditure. Aside from derailing financial plans, these events can lead to lower system reliability. This paper describes the development and potential application of transformer model based on multilayer perceptron class of artificial neural networks. Model is built in Python programming language and data collected over the span of one year for a single transformer. Three input features (oil temperature, winding current and outside temperature) are used in the input layer, with the goal of predicting the winding temperature in the transformer. Predicted temperature of the windings can then be compared with the actual winding temperature, which can serve as an indicator of transformers internal condition. Two types of tr… |
| https://openalex.org/W3176851559 | LayoutLMv2: Multi-modal Pre-training for Visually-rich Document Understanding | 2021 |  | article | 409 | yes | Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei Florêncio, +4 more | Zhàng, Modal, Computer science, Natural language processing, Volume (thermodynamics), Artificial intelligence, +16 more | https://doi.org/10.18653/v1/2021.acl-long.201 | Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Wanxiang Che, Min Zhang, Lidong Zhou. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021. |
| https://openalex.org/W4367051110 | Comparing scientific abstracts generated by ChatGPT to real abstracts with detectors and blinded human reviewers | 2023 | npj Digital Medicine | article | 593 | yes | Catherine A. Gao, Frederick M. Howard, Nikolay S. Markov, Emma Dyer, Siddhi Ramesh, Yuan Luo, Alexander T. Pearson | Meaning (existential), Computer science, Information retrieval, Detector, Interquartile range, Medical physics, +10 more | https://doi.org/10.1038/s41746-023-00819-6 |  |
| https://openalex.org/W3034850762 | Adversarial NLI: A New Benchmark for Natural Language Understanding | 2020 |  | preprint | 585 | yes | Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, Douwe Kiela | Benchmark (surveying), Computer science, Adversarial system, Variety (cybernetics), Artificial intelligence, Machine learning, +12 more | https://doi.org/10.18653/v1/2020.acl-main.441 | We introduce a new large-scale NLI benchmark dataset, collected via an iterative, adversarial human-and-model-in-the-loop procedure. We show that training models on this new dataset leads to state-of-the-art performance on a variety of popular NLI benchmarks, while posing a more difficult challenge with its new test set. Our analysis sheds light on the shortcomings of current state-of-the-art models, and shows that non-expert annotators are successful at finding their weaknesses. The data collection method can be applied in a never-ending learning scenario, becoming a moving target for NLU, rather than a static benchmark that will quickly saturate. |
| https://openalex.org/W2945127593 | Deeper Text Understanding for IR with Contextual Neural Language Modeling | 2019 |  | preprint | 403 | yes | Zhuyun Dai, Jamie Callan | Computer science, Leverage (statistics), Language model, Natural language processing, Artificial intelligence, Relevance (law), +6 more | https://doi.org/10.1145/3331184.3331303 | Neural networks provide new possibilities to automatically learn complex\nlanguage patterns and query-document relations. Neural IR models have achieved\npromising results in learning query-document relevance patterns, but few\nexplorations have been done on understanding the text content of a query or a\ndocument. This paper studies leveraging a recently-proposed contextual neural\nlanguage model, BERT, to provide deeper text understanding for IR. Experimental\nresults demonstrate that the contextual text representations from BERT are more\neffective than traditional word embeddings. Compared to bag-of-words retrieval\nmodels, the contextual language model can better leverage language structures,\nbringing large improvements on queries written in natural languages. Combining\nthe text understanding ability with search knowledge leads to an enhanced\npre-trained BERT model that can bene… |
| https://openalex.org/W4376256450 | Towards more effective encoders in pre-training for sequential recommendation | 2023 | World Wide Web | article | 5 | no | Ke Sun, Tieyun Qian, Ming Zhong, Xuhui Li | Computer science, Encoder, Transformer, Locality, Convolutional neural network, Artificial intelligence, +8 more | https://doi.org/10.1007/s11280-023-01163-1 |  |
| https://openalex.org/W3080997787 | GPT-GNN | 2020 |  | article | 423 | yes | Ziniu Hu, Yuxiao Dong, Kuansan Wang, Kai-Wei Chang, Yizhou Sun | Computer science, Graph, Machine learning, Artificial intelligence, Generative model, Generative grammar, +6 more | https://doi.org/10.1145/3394486.3403237 | Graph neural networks (GNNs) have been demonstrated to be powerful in modeling graph-structured data. However, training GNNs requires abundant task-specific labeled data, which is often arduously expensive to obtain. One effective way to reduce the labeling effort is to pre-train an expressive GNN model on unlabelled data with self-supervision and then transfer the learned model to downstream tasks with only a few labels. In this paper, we present the GPT-GNN framework to initialize GNNs by generative pre-training. GPT-GNN introduces a self-supervised attributed graph generation task to pre-train a GNN so that it can capture the structural and semantic properties of the graph. We factorize the likelihood of graph generation into two components: 1) attribute generation and 2) edge generation. By modeling both components, GPT-GNN captures the inherent dependency between node attributes an… |
| https://openalex.org/W3125679694 | Grey-box Extraction of Natural Language Models | 2021 | International Conference on Machine Learning | article | 8 | no | Santiago Zanella-Béguelin, Shruti Tople, Andrew Paverd, Boris Köpf | Computer science, Artificial intelligence, Exploit, Embedding, Encoder, Inference, +12 more | http://proceedings.mlr.press/v139/zanella-beguelin21a/zanella-beguelin21a.pdf | Model extraction attacks attempt to replicate a target machine learning model from predictions obtained by querying its inference API. Most existing attacks on Deep Neural Networks achieve this by supervised training of the copy using the victim's predictions. An emerging class of attacks exploit algebraic properties of DNNs to obtain high-fidelity copies using orders of magnitude fewer queries than the prior state-of-the-art. So far, such powerful attacks have been limited to networks with few hidden layers and ReLU activations. In this paper we present algebraic attacks on large-scale natural language models in a grey-box setting, targeting models with a pre-trained (public) encoder followed by a single (private) classification layer. Our key observation is that a small set of arbitrary embedding vectors is likely to form a basis of the classification layer's input space, which a grey… |
| https://openalex.org/W4283801824 | Shifting machine learning for healthcare from development to deployment and from models to data | 2022 | Nature Biomedical Engineering | review | 371 | yes | Angela Zhang, Lei Xing, James Zou, Joseph C. Wu | Software deployment, Computer science, Artificial intelligence, Health care, Machine learning, Generative grammar, +9 more | https://doi.org/10.1038/s41551-022-00898-y | In the past decade, the application of machine learning (ML) to healthcare has helped drive the automation of physician tasks as well as enhancements in clinical capabilities and access to care. This progress has emphasized that, from model development to model deployment, data play central roles. In this Review, we provide a data-centric view of the innovations and challenges that are defining ML for healthcare. We discuss deep generative models and federated learning as strategies to augment datasets for improved model performance, as well as the use of the more recent transformer models for handling larger datasets and enhancing the modelling of clinical text. We also discuss data-focused problems in the deployment of ML, emphasizing the need to efficiently deliver data to ML models for timely clinical predictions and to account for natural data shifts that can deteriorate model perf… |
| https://openalex.org/W3034328552 | Dice Loss for Data-imbalanced NLP Tasks | 2020 |  | article | 554 | yes | Xiaoya Li, Xiaofei Sun, Yuxian Meng, Junjun Liang, Fei Wu, Jiwei Li | Dice, Computer science, Artificial intelligence, Natural language processing, Machine learning, Task (project management), +12 more | https://doi.org/10.18653/v1/2020.acl-main.45 | Many NLP tasks such as tagging and machine reading comprehension are faced with the severe data imbalance issue: negative examples significantly outnumber positive examples, and the huge number of easy-negative examples overwhelms the training. The most commonly used cross entropy (CE) criteria is actually an accuracy-oriented objective, and thus creates a discrepancy between training and test: at training time, each training instance contributes equally to the objective function, while at test time F1 score concerns more about positive examples. In this paper, we propose to use dice loss in replacement of the standard cross-entropy objective for data-imbalanced NLP tasks. Dice loss is based on the Sørensen--Dice coefficient or Tversky index, which attaches similar importance to false positives and false negatives, and is more immune to the data-imbalance issue. To further alleviate the… |
| https://openalex.org/W3101449015 | BERT-ATTACK: Adversarial Attack Against BERT Using BERT | 2020 |  | article | 505 | yes | Linyang Li, Ruotian Ma, Qipeng Guo, Xiangyang Xue, Xipeng Qiu | Computer science, Adversarial system, Language model, Code (set theory), Fluency, Artificial intelligence, +9 more | https://doi.org/10.18653/v1/2020.emnlp-main.500 | Adversarial attacks for discrete data (such as texts) have been proved significantly more challenging than continuous data (such as images) since it is difficult to generate adversarial samples with gradient-based methods. Current successful attack methods for texts usually adopt heuristic replacement strategies on the character or word level, which remains challenging to find the optimal solution in the massive space of possible combinations of replacements while preserving semantic consistency and language fluency. In this paper, we propose BERT-Attack, a high-quality and effective method to generate adversarial samples using pre-trained masked language models exemplified by BERT. We turn BERT against its fine-tuned models and other deep neural models in downstream tasks so that we can successfully mislead the target models to predict incorrectly. Our method outperforms state-of-the-a… |
| https://openalex.org/W4387487230 | Comparative Analysis of LSTM, GRU and Transformer Models for German to English Language Translation | 2023 |  | article | 2 | no | Premanand Ghadekar, Neel Malwatkar, Nikhil Sontakke, Nirvisha Soni | Machine translation, Computer science, Transformer, Recurrent neural network, Artificial intelligence, Natural language processing, +17 more | http://dx.doi.org/10.1109/asiancon58793.2023.10270018 | Natural Language Processing (NLP) encompasses a broad range of techniques and methodologies for processing and understanding human language. One of the most important NLP applications that has experienced significant advancements and has gained immense importance over the years is Neural Machine Translation. Research on German-to-English language machine translation has remained a prominent area of research within the field of Natural Language Processing and Deep. This paper presents an in-depth analysis of three significant models that are used for Neural Machine Translation namely Recurrent Neural Network with Long Short-Term Memory, Recurrent Neural Network with Gated Recurrent Unit, and the Transformer. For the implementation of each model, a large data corpus of 221,534 sentence pairs is used. Two evaluation metrics are employed to assess the performance of models i.e., the BLEU Sc… |
| https://openalex.org/W4205201623 | A Literature Survey of Recent Advances in Chatbots | 2022 | Information | article | 387 | yes | Guendalina Caldarini, Sardar Jaf, Kenneth McGarry | Chatbot, Computer science, Conversation, Applications of artificial intelligence, Data science, Order (exchange), +6 more | https://doi.org/10.3390/info13010041 | Chatbots are intelligent conversational computer systems designed to mimic human conversation to enable automated online guidance and support. The increased benefits of chatbots led to their wide adoption by many industries in order to provide virtual assistance to customers. Chatbots utilise methods and algorithms from two Artificial Intelligence domains: Natural Language Processing and Machine Learning. However, there are many challenges and limitations in their application. In this survey we review recent advances on chatbots, where Artificial Intelligence and Natural Language processing are used. We highlight the main challenges and limitations of current work and make recommendations for future research investigation. |
| https://openalex.org/W2919290281 | Massively Multilingual Neural Machine Translation | 2019 |  | article | 403 | yes | Roee Aharoni, Melvin Johnson, Orhan Fırat | Computer science, Machine translation, Massively parallel, Volume (thermodynamics), Computational linguistics, Artificial intelligence, +13 more | https://doi.org/10.18653/v1/n19-1388 | Roee Aharoni, Melvin Johnson, Orhan Firat. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 2019. |
| https://openalex.org/W3038033387 | Multilingual Universal Sentence Encoder for Semantic Retrieval | 2020 |  | article | 403 | yes | Yinfei Yang, Daniel Cer, Amin Ahmad, Mandy Guo, Jax Law, Noah Constant, Gustavo Hernández Ábrego, Steve Yuan, +4 more | Sentence, Encoder, Computer science, Natural language processing, Linguistics, Constant (computer programming), +6 more | https://doi.org/10.18653/v1/2020.acl-demos.12 | Yinfei Yang, Daniel Cer, Amin Ahmad, Mandy Guo, Jax Law, Noah Constant, Gustavo Hernandez Abrego, Steve Yuan, Chris Tar, Yun-hsuan Sung, Brian Strope, Ray Kurzweil. Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations. 2020. |
| https://openalex.org/W4312799098 | Analog-memory-based 14nm Hardware Accelerator for Dense Deep Neural Networks including Transformers | 2022 | 2022 IEEE International Symposium on Circuits and Systems (ISCAS) | article | 6 | no | Atsuya Okazaki, Pritish Narayanan, Stefano Ambrogio, Kohji Hosokawa, Hsinyu Tsai, Akiyo Nomura, Takeo Yasuda, Charles Mackin, +8 more | Computer science, MNIST database, Resistive random-access memory, Computer hardware, CMOS, Deep learning, +8 more | https://doi.org/10.1109/iscas48785.2022.9937292 | Analog non-volatile memory (NVM)-based accelerators for deep neural networks perform high-throughput and energy-efficient multiply-accumulate (MAC) operations (e.g., high TeraOPS/W) by taking advantage of massively parallelized analog MAC operations, implemented with Ohm's law and Kirchhoff's current law on array-matrices of resistive devices. While the wide-integer and floating-point operations offered by conventional digital CMOS computing are much more suitable than analog computing for conventional applications that require high accuracy and true reproducibility, deep neural networks can still provide competitive end-to-end results even with modest (e.g., 4-bit) precision in synaptic operations. In this paper, we describe a 14-nm inference chip, comprising multiple 512$\times$ 512 arrays of Phase Change Memory (PCM) devices, which can deliver software-equivalent inference accuracy f… |
| https://openalex.org/W4407576760 | Transformer-based heart language model with electrocardiogram annotations | 2025 | Scientific Reports | article | 4 | yes | Stojancho Tudjarski, Marjan Gušev, Evangelos Kanoulas | Computer science, Transformer, Natural language processing, Artificial intelligence, Electrical engineering, Engineering, +1 more | https://doi.org/10.1038/s41598-024-84270-x |  |
| https://openalex.org/W3115903740 | SemEval-2020 Task 12: Multilingual Offensive Language Identification in Social Media (OffensEval 2020) | 2020 |  | article | 378 | yes | Marcos Zampieri, Preslav Nakov, Sara Rosenthal, Pepa Atanasova, Georgi Karadzhov, Hamdy Mubarak, Leon Derczynski, Zeses Pitenis, +1 more | SemEval, Offensive, Computer science, Task (project management), Identification (biology), Social media, +11 more | https://doi.org/10.18653/v1/2020.semeval-1.188 | We present the results and the main findings of SemEval-2020 Task 12 on Multilingual Offensive Language Identification in Social Media (OffensEval-2020). The task included three subtasks corresponding to the hierarchical taxonomy of the OLID schema from OffensEval-2019, and it was offered in five languages: Arabic, Danish, English, Greek, and Turkish. OffensEval-2020 was one of the most popular tasks at SemEval-2020, attracting a large number of participants across all subtasks and languages: a total of 528 teams signed up to participate in the task, 145 teams submitted official runs on the test data, and 70 teams submitted system description papers. |
| https://openalex.org/W4389885061 | How Artificial Intelligence Is Shaping Medical Imaging Technology: A Survey of Innovations and Applications | 2023 | Bioengineering | article | 412 | yes | Luís Coelho | Artificial intelligence, Medical imaging, Applications of artificial intelligence, Health care, Convolutional neural network, Computer science, +10 more | https://doi.org/10.3390/bioengineering10121435 | The integration of artificial intelligence (AI) into medical imaging has guided in an era of transformation in healthcare. This literature review explores the latest innovations and applications of AI in the field, highlighting its profound impact on medical diagnosis and patient care. The innovation segment explores cutting-edge developments in AI, such as deep learning algorithms, convolutional neural networks, and generative adversarial networks, which have significantly improved the accuracy and efficiency of medical image analysis. These innovations have enabled rapid and accurate detection of abnormalities, from identifying tumors during radiological examinations to detecting early signs of eye disease in retinal images. The article also highlights various applications of AI in medical imaging, including radiology, pathology, cardiology, and more. AI-based diagnostic tools not onl… |
| https://openalex.org/W3158818505 | An Attention-Based Deep Learning Approach for Sleep Stage Classification With Single-Channel EEG | 2021 | IEEE Transactions on Neural Systems and Rehabilitation Engineering | article | 584 | yes | Emadeldeen Eldele, Zhenghua Chen, Chengyu Liu, Min Wu, Chee Keong Kwoh, Xiaoli Li, Cuntai Guan | Electroencephalography, Sleep (system call), Channel (broadcasting), Computer science, Sleep Stages, Stage (stratigraphy), +11 more | https://doi.org/10.1109/tnsre.2021.3076234 | Automatic sleep stage mymargin classification is of great importance to measure sleep quality. In this paper, we propose a novel attention-based deep learning architecture called AttnSleep to classify sleep stages using single channel EEG signals. This architecture starts with the feature extraction module based on multi-resolution convolutional neural network (MRCNN) and adaptive feature recalibration (AFR). The MRCNN can extract low and high frequency features and the AFR is able to improve the quality of the extracted features by modeling the inter-dependencies between the features. The second module is the temporal context encoder (TCE) that leverages a multi-head attention mechanism to capture the temporal dependencies among the extracted features. Particularly, the multi-head attention deploys causal convolutions to model the temporal relations in the input features. We evaluate t… |
| https://openalex.org/W3039695075 | Language-agnostic BERT Sentence Embedding | 2022 | Proceedings of the 60th Annual Meeting of the Association for Computational Lin… | article | 443 | yes | Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan, Wei Wang | Computer science, Natural language processing, Sentence, Artificial intelligence, Embedding, Margin (machine learning), +11 more | https://doi.org/10.18653/v1/2022.acl-long.62 | While BERT is an effective method for learning monolingual sentence embeddings for semantic similarity and embedding based transfer learning BERT based cross-lingual sentence embeddings have yet to be explored. We systematically investigate methods for learning multilingual sentence embeddings by combining the best methods for learning monolingual and cross-lingual representations including: masked language modeling (MLM), translation language modeling (TLM), dual encoder translation ranking, and additive margin softmax. We show that introducing a pre-trained multilingual language model dramatically reduces the amount of parallel training data required to achieve good performance by 80%. Composing the best of these methods produces a model that achieves 83.7% bi-text retrieval accuracy over 112 languages on Tatoeba, well above the 65.5% achieved by LASER, while still performing competit… |
| https://openalex.org/W2944715081 | Densifying Assumed-Sparse Tensors | 2019 | Lecture notes in computer science | book-chapter | 3 | no | Derya Çavdar, Valeriu Codreanu, Can Karakus, John A. Lockman, Damian Podareanu, Vikram A. Saletore, Alexander Sergeev, Don D. Smith, +6 more | Computer science, Machine translation, Transformer, Scaling, Artificial neural network, Network topology, +10 more | https://doi.org/10.1007/978-3-030-20656-7_2 |  |
| https://openalex.org/W4402137445 | A Literature Review on Emotional Intelligence of Large Language Models (LLMs) | 2024 | International Journal of Advanced Research in Computer Science | review | 2 | yes | Pinaki Raj, Pinaki Raj | Computer science, Natural language processing, Artificial intelligence | https://doi.org/10.26483/ijarcs.v15i4.7111 | Large Language Models(LLMs) are artificial intelligence models that use deep neural networks to perform Natural Language Processing (NLP) tasks. These tasks include interaction between humans and computers, enabling computers to interpret and generate human languages in a meaningful manner. Large Language models are called "large" because of the architecture’s size and the huge sets of training text data. With the emergence of transformer-based LLMs, the game of NLPs has reached another level. This is due to their ability to handle long-range text dependencies in parallel. The growing prevalence of transformer-based LLMs in human lives has necessitated evaluating the scope of the Emotional Intelligence(EI) of LLMs. This paper will discuss the need for emotional intelligence in transformer-based LLMs and the various existing studies that have evaluated this aspect. The potential challeng… |
| https://openalex.org/W4286669150 | High-resolution <i>de novo</i> structure prediction from primary sequence | 2022 |  | preprint | 379 | yes | Ruidong Wu, Fan Ding, Rui Wang, Rui Shen, Xiwen Zhang, Shitong Luo, Chenpeng Su, Zuofan Wu, +4 more | Computer science, Computational biology, Protein structure prediction, Protein structure, Sequence (biology), Protein family, +14 more | https://doi.org/10.1101/2022.07.21.500999 | Abstract Recent breakthroughs have used deep learning to exploit evolutionary information in multiple sequence alignments (MSAs) to accurately predict protein structures. However, MSAs of homologous proteins are not always available, such as with orphan proteins or fast-evolving proteins like antibodies, and a protein typically folds in a natural setting from its primary amino acid sequence into its three-dimensional structure, suggesting that evolutionary information and MSAs should not be necessary to predict a protein’s folded form. Here, we introduce OmegaFold, the first computational method to successfully predict high-resolution protein structure from a single primary sequence alone. Using a new combination of a protein language model that allows us to make predictions from single sequences and a geometry-inspired transformer model trained on protein structures, OmegaFold outperfo… |
| https://openalex.org/W4398228950 | What does artificial intelligence mean in rheumatology? | 2024 | Archives of Rheumatology | article | 10 | yes | Kunal Chandwar, Durga Prasanna Misra | Artificial intelligence, Machine learning, Deep learning, Computer science, Applications of artificial intelligence, Artificial neural network, +1 more | https://doi.org/10.46497/archrheumatol.2024.10664 | Intelligence is the ability of humans to learn from experiences to ascribe conscious weights and unconscious biases to modulate their outputs from given inputs. Transferring this ability to computers is artificial intelligence (AI). The ability of computers to understand data in an intelligent manner is machine learning. When such learning is with images and videos, which involves deeper layers of artificial neural networks, it is described as deep learning. Large language models are the latest development in AI which incorporate self-learning into deep learning through transformers. AI in Rheumatology has immense potential to revolutionize healthcare and research. Machine learning could aid clinical diagnosis and decision-making, and deep learning could extend this to analyze images of radiology or positron emission tomography scans or histopathology images to aid a clinician’s diagnos… |
| https://openalex.org/W3172335055 | QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering | 2021 |  | article | 454 | yes | Michihiro Yasunaga, Hongyu Ren, Antoine Bosselut, Percy Liang, Jure Leskovec | Question answering, Computer science, Knowledge graph, Natural language processing, Artificial intelligence, Conceptual graph, +2 more | https://doi.org/10.18653/v1/2021.naacl-main.45 | Michihiro Yasunaga, Hongyu Ren, Antoine Bosselut, Percy Liang, Jure Leskovec. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021. |
| https://openalex.org/W3034689979 | A Transformer-based Approach for Source Code Summarization | 2020 |  | preprint | 354 | yes | Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, Kai-Wei Chang | Automatic summarization, Computer science, Source code, Pairwise comparison, Code (set theory), Encoding (memory), +11 more | https://doi.org/10.18653/v1/2020.acl-main.449 | Generating a readable summary that describes the functionality of a program is known as source code summarization. In this task, learning code representation by modeling the pairwise relationship between code tokens to capture their long-range dependencies is crucial. To learn code representation for summarization, we explore the Transformer model that uses a self-attention mechanism and has shown to be effective in capturing long-range dependencies. In this work, we show that despite the approach is simple, it outperforms the state-of-the-art techniques by a significant margin. We perform extensive analysis and ablation studies that reveal several important findings, e.g., the absolute encoding of source code tokens’ position hinders, while relative encoding significantly improves the summarization performance. We have made our code publicly available to facilitate future research. |
| https://openalex.org/W3210957772 | Transformer-Based Deep Neural Language Modeling for Construct-Specific Automatic Item Generation | 2021 |  | preprint | 4 | yes | Björn Erik Hommel, Franz-Josef M. Wollang, Veronika Kotova, Hannes Zacher, Stefan C. Schmukle | Computer science, Artificial intelligence, Construct (python library), Transformer, Cognition, Natural language processing, +9 more | https://doi.org/10.31234/osf.io/qfvpe | Algorithmic automatic item generation can be used to obtain large quantities of cognitive items in the domains of knowledge and aptitude testing. However, conventional item models used by template-based automatic item generation techniques are not ideal for the creation of items for non-cognitive constructs. Progress in this area has been made recently by employing long short-term memory recurrent neural networks to produce word sequences that syntactically resemble items typically found in personality questionnaires. To date, such items have been produced unconditionally, without the possibility of selectively targeting personality domains. In this article, we offer a brief synopsis on past developments in natural language processing and explain why the automatic generation of construct-specific items has become attainable only due to recent technological progress. We propose that pre-… |
| https://openalex.org/W3092557781 | Overview of the Transformer-based Models for NLP Tasks | 2020 | Annals of Computer Science and Information Systems | article | 352 | yes | Anthony Gillioz, Jacky Casas, Elena Mugellini, Omar Abou Khaled | Transformer, Architecture, Computer science, Artificial intelligence, Encoder, Margin (machine learning), +10 more | https://doi.org/10.15439/2020f20 | In 2017, Vaswani et al. proposed a new neural network architecture named Transformer. That modern architecture quickly revolutionized the natural language processing world. Models like GPT and BERT relying on this Transformer architecture have fully outperformed the previous state-of-the-art networks. It surpassed the earlier approaches by such a wide margin that all the recent cutting edge models seem to rely on these Transformer-based architectures. In this paper, we provide an overview and explanations of the latest models. We cover the auto-regressive models such as GPT, GPT-2 and XLNET, as well as the auto-encoder architecture such as BERT and a lot of post-BERT models like RoBERTa, ALBERT, ERNIE 1.0/2.0. |
| https://openalex.org/W4366817968 | Generative AI at Work | 2023 |  | report | 729 | yes | Erik Brynjolfsson, Danielle Li, Lindsey Raymond | Generative grammar, Work (physics), Computer science, Artificial intelligence, Engineering, Mechanical engineering | https://doi.org/10.3386/w31161 | We study the staggered introduction of a generative AI-based conversational assistant using data from 5,179 customer support agents.Access to the tool increases productivity, as measured by issues resolved per hour, by 14 percent on average, with the greatest impact on novice and lowskilled workers, and minimal impact on experienced and highly skilled workers.We provide suggestive evidence that the AI model disseminates the potentially tacit knowledge of more able workers and helps newer workers move down the experience curve.In addition, we show that AI assistance improves customer sentiment, reduces requests for managerial intervention, and improves employee retention. |
| https://openalex.org/W3035050380 | Extractive Summarization as Text Matching | 2020 |  | preprint | 406 | yes | Ming Zhong, Pengfei Liu, Yiran Chen, Danqing Wang, Xipeng Qiu, Xuanjing Huang | Automatic summarization, Computer science, Matching (statistics), Natural language processing, Artificial intelligence, Sentence, +7 more | https://doi.org/10.18653/v1/2020.acl-main.552 | This paper creates a paradigm shift with regard to the way we build neural extractive summarization systems. Instead of following the commonly used framework of extracting sentences individually and modeling the relationship between sentences, we formulate the extractive summarization task as a semantic text matching problem, in which a source document and candidate summaries will be (extracted from the original text) matched in a semantic space. Notably, this paradigm shift to semantic matching framework is well-grounded in our comprehensive analysis of the inherent gap between sentence-level and summary-level extractors based on the property of the dataset. Besides, even instantiating the framework with a simple form of a matching model, we have driven the state-of-the-art extractive result on CNN/DailyMail to a new level (44.41 in ROUGE-1). Experiments on the other five datasets also… |
| https://openalex.org/W2962785754 | HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization | 2019 |  | preprint | 357 | yes | Xingxing Zhang, Furu Wei, Ming Zhou | Automatic summarization, Computer science, Transformer, Encoder, Artificial intelligence, Sentence, +9 more | https://doi.org/10.18653/v1/p19-1499 | Neural extractive summarization models usually employ a hierarchical encoder for document encoding and they are trained using sentence-level labels, which are created heuristically using rule-based methods. Training the hierarchical encoder with these inaccurate labels is challenging. Inspired by the recent work on pre-training transformer sentence encoders (Devlin et al., 2018), we propose Hibert (as shorthand for HIerachical Bidirectional Encoder Representations from Transformers) for document encoding and a method to pre-train it using unlabeled data. We apply the pre-trained Hibert to our summarization model and it outperforms its randomly initialized counterpart by 1.25 ROUGE on the CNN/Dailymail dataset and by 2.0 ROUGE on a version of New York Times dataset. We also achieve the state-of-the-art performance on these two datasets. |
| https://openalex.org/W4409261463 | Foundations of Deep Learning and Large Language Models in Cybersecurity | 2025 | Advances in computational intelligence and robotics book series | book-chapter | 4 | no | Hewa Majeed Zangana, Marwan Omar, Jamal N. Al-Karaki | Computer science, Computer security, Cognitive science, Artificial intelligence, Natural language processing, Psychology | https://doi.org/10.4018/979-8-3373-3296-3.ch001 | The integration of deep learning (DL) and large language models (LLMs) has significantly advanced the field of cybersecurity, offering innovative approaches to threat detection, anomaly identification, and secure communication. Deep learning techniques, such as neural networks and reinforcement learning, have demonstrated robust capabilities in detecting previously unknown threats by learning patterns from vast amounts of cybersecurity data. Similarly, LLMs, particularly transformers, have revolutionized natural language processing tasks, enabling effective vulnerability analysis, malware classification, and phishing detection. This chapter explores the foundational concepts of deep learning and LLMs, highlighting their applications and challenges within the cybersecurity landscape. Additionally, it discusses the synergy between these technologies, focusing on how they complement tradit… |
| https://openalex.org/W4385340665 | Assemble the shallow or integrate a deep? Toward a lightweight solution for glyph-aware Chinese text classification | 2023 | PLoS ONE | article | 3 | yes | Jingrui Hou, Ping Wang | Glyph (data visualization), Computer science, Artificial intelligence, Artificial neural network, Chinese characters, Natural language processing, +7 more | http://dx.doi.org/10.1371/journal.pone.0289204 | As hieroglyphic languages, such as Chinese, differ from alphabetic languages, researchers have always been interested in using internal glyph features to enhance semantic representation. However, the models used in such studies are becoming increasingly computationally expensive, even for simple tasks like text classification. In this paper, we aim to balance model performance and computation cost in glyph-aware Chinese text classification tasks. To address this issue, we propose a lightweight ensemble learning method for glyph-aware Chinese text classification (LEGACT) that consists of typical shallow networks as base learners and machine learning classifiers as meta-learners. Through model design and a series of experiments, we demonstrate that an ensemble approach integrating shallow neural networks can achieve comparable results even when compared to large-scale transformer models.… |
| https://openalex.org/W4225856847 | Novel AI to avert the mental health crisis in COVID-19: Novel application of GPT2 in Cognitive Behaviour Therapy | 2022 | Research Square (Research Square) | preprint | 5 | yes | ­ Anonymous | Mental health, Computer science, Artificial intelligence, Scalability, Cognition, Transfer of learning, +4 more | https://doi.org/10.21203/rs.3.rs-382748/v3 | Abstract The effect of the COVID-19 pandemic on mental health is substantial. The World Health Organization has called for action to avert an impending mental health crisis. To respond to this call, this paper contributes a novel application of Deep Learning in Natural Language Generation (NLG) to seed healthy thoughts for mental health therapy. For the 1 st time in literature, a transfer learning capable large neural network with more than 100 million parameters for a NLG based mental health therapy application is proposed &amp; demonstrated. This AI is designed to address scalable impact for millions of families with a timely health intervention in a privacy-safe approach. To the best of our knowledge, this is the first research paper to apply GPT2 (Generative Pretrained Transformer) for Cognitive Behavior therapy (CBT). Further, the paper demonstrates the proposed neural network arch… |
| https://openalex.org/W4293147636 | Transmorph: a transformer based morphological disambiguator for Turkish | 2022 | TURKISH JOURNAL OF ELECTRICAL ENGINEERING & COMPUTER SCIENCES | article | 3 | no | HİLAL ÖZER, EMİN ERKAN KORKMAZ | Agglutinative language, Computer science, Lexical analysis, Natural language processing, Artificial intelligence, Transformer, +10 more | https://doi.org/10.55730/1300-0632.3912 | The agglutinative nature of the Turkish language has a complex morphological structure, and there are generally more than one parse for a given word. Before further processing, morphological disambiguation is required to determine the correct morphological analysis of a word. Morphological disambiguation is one of the first and crucial steps in natural language processing since its success determines later analyses. In our proposed morphological disambiguation method, we used a transformer-based sequence-to-sequence neural network architecture. Transformers are commonly used in various NLP tasks, and they produce state-of-the-art results in machine translation. However, to the best of our knowledge, transformer-based encoder-decoders have not been studied in morphological disambiguation. In this study, in addition to character level tokenization, three input subword representations are… |
| https://openalex.org/W4394828356 | GPT (Generative Pre-Trained Transformer)— A Comprehensive Review on Enabling Technologies, Potential Applications, Emerging Challenges, and Future Directions | 2024 | IEEE Access | review | 449 | yes | Gokul Yenduri, M. Ramalingam, G. Chemmalar Selvi, Y. Supriya, Gautam Srivastava, Praveen Kumar Reddy Maddikunta, G. Deepti Raj, Rutvij H. Jhaveri, +4 more | Computer science, Transformer, Architecture, Natural language understanding, Emerging technologies, Natural language, +9 more | https://doi.org/10.1109/access.2024.3389497 | The Generative Pre-trained Transformer (GPT) represents a notable breakthrough in the domain of natural language processing, which is propelling us toward the development of machines that can understand and communicate using language in a manner that closely resembles that of humans. GPT is based on the transformer architecture, a deep neural network designed for natural language processing tasks. Due to their impressive performance on natural language processing tasks and ability to effectively converse, GPT have gained significant popularity among researchers and industrial communities, making them one of the most widely used and effective models in natural language processing and related fields, which motivated to conduct this review. This review provides a detailed overview of the GPT, including its architecture, working process, training procedures, enabling technologies, and its i… |
| https://openalex.org/W3035375600 | Named Entity Recognition as Dependency Parsing | 2020 |  | preprint | 428 | yes | Juntao Yu, Bernd Bohnet, Massimo Poesio | Computer science, Named-entity recognition, Natural language processing, Parsing, Dependency (UML), Artificial intelligence, +11 more | https://doi.org/10.18653/v1/2020.acl-main.577 | Named Entity Recognition (NER) is a fundamental task in Natural Language Processing, concerned with identifying spans of text expressing references to entities. NER research is often focused on flat entities only (flat NER), ignoring the fact that entity references can be nested, as in [Bank of [China]] (Finkel and Manning, 2009). In this paper, we use ideas from graph-based dependency parsing to provide our model a global view on the input via a biaffine model (Dozat and Manning, 2017). The biaffine model scores pairs of start and end tokens in a sentence which we use to explore all spans, so that the model is able to predict named entities accurately. We show that the model works well for both nested and flat NER through evaluation on 8 corpora and achieving SoTA performance on all of them, with accuracy gains of up to 2.2 percentage points. |
| https://openalex.org/W3202406646 | Swin Transformer: Hierarchical Vision Transformer using Shifted Windows | 2021 | arXiv (Cornell University) | preprint | 352 | yes | Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo | Transformer, Computer science, Segmentation, Computation, Artificial intelligence, Computer vision, +4 more | http://arxiv.org/abs/2103.14030 | This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with \textbf{S}hifted \textbf{win}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with… |
| https://openalex.org/W2970482702 | PubMedQA: A Dataset for Biomedical Research Question Answering | 2019 |  | article | 502 | yes | Qiao Jin, Bhuwan Dhingra, Zheng­ping Liu, William Cohen, Xinghua Lu | Question answering, Computer science, Natural language processing, Natural (archaeology), Natural language, Joint (building), +9 more | https://doi.org/10.18653/v1/d19-1259 | Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, Xinghua Lu. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019. |
| https://openalex.org/W4206395230 | Concept recognition as a machine translation problem | 2021 | BMC Bioinformatics | article | 7 | yes | Mayla R. Boguslav, Negacy Hailu, Michael Bada, William A. Baumgartner, Lawrence Hunter | Computer science, Machine translation, Machine learning, Artificial intelligence, Natural language processing, Robustness (evolution), +3 more | https://doi.org/10.1186/s12859-021-04141-4 | Abstract Background Automated assignment of specific ontology concepts to mentions in text is a critical task in biomedical natural language processing, and the subject of many open shared tasks. Although the current state of the art involves the use of neural network language models as a post-processing step, the very large number of ontology classes to be recognized and the limited amount of gold-standard training data has impeded the creation of end-to-end systems based entirely on machine learning. Recently, Hailu et al. recast the concept recognition problem as a type of machine translation and demonstrated that sequence-to-sequence machine learning models have the potential to outperform multi-class classification approaches. Methods We systematically characterize the factors that contribute to the accuracy and efficiency of several approaches to sequence-to-sequence machine learn… |
| https://openalex.org/W4400912296 | Integrated Method of Deep learning and Large Language Model in Speech Recognition | 2024 | Preprints.org | preprint | 2 | yes | Bo Guan, Jin Cao, Bingjie Huang, Zhuoyue Wang, Xingqi Wang, Zixiang Wang | Computer science, Speech recognition, Language model, Word error rate, Artificial intelligence, Cache language model, +16 more | https://doi.org/10.20944/preprints202407.1520.v1 | This research aims to explore the integration method of deep learning and large language models in speech recognition to improve the system&amp;rsquo;s recognition accuracy and ability to handle complex contexts. Deep neural network (DNN), convolutional neural network (CNN), long short-term memory network (LSTM) and Transformer-based large language model are used to build an integrated acoustic and language model framework. Experiments on TIMIT, LibriSpeech and Common Voice datasets show that the ensemble model shows significant improvements in both word error rate (WER) and real-time factor (RTF) compared to traditional models. Especially in terms of adaptability to multiple languages and accent changes, the model shows superior performance. The conclusion shows that through technology integration, the performance of the speech recognition system in complex environments can be effectiv… |
| https://openalex.org/W4328114198 | Two sequence- and two structure-based ML models have learned different aspects of protein biochemistry | 2023 |  | preprint | 4 | yes | Anastasiya V. Kulikova, Daniel J. Diaz, Tianlong Chen, T. Jeffrey Cole, Andrew D. Ellington, Claus O. Wilke | Leverage (statistics), Convolutional neural network, Protein structure prediction, Artificial intelligence, Computer science, Machine learning, +5 more | http://dx.doi.org/10.1101/2023.03.20.533508 | ABSTRACT Deep learning models are seeing increased use as methods to predict mutational effects or allowed mutations in proteins. The models commonly used for these purposes include large language models (LLMs) and 3D Convolutional Neural Networks (CNNs). These two model types have very different architectures and are commonly trained on different representations of proteins. LLMs make use of the transformer architecture and are trained purely on protein sequences whereas 3D CNNs are trained on voxelized representations of local protein structure. While comparable overall prediction accuracies have been reported for both types of models, it is not known to what extent these models make comparable specific predictions and/or generalize protein biochemistry in similar ways. Here, we perform a systematic comparison of two LLMs and two structure-based models (CNNs) and show that the differe… |
| https://openalex.org/W3158304688 | Dynamic Graph Convolutional Recurrent Network for Traffic Prediction: Benchmark and Solution | 2022 | ACM Transactions on Knowledge Discovery from Data | article | 454 | yes | Fuxian Li, Jie Feng, Huan Yan, Guangyin Jin, Fan Yang, Funing Sun, Depeng Jin, Yong Li | Computer science, Benchmark (surveying), Leverage (statistics), Graph, Data mining, Dynamic network analysis, +12 more | https://doi.org/10.1145/3532611 | Traffic prediction is the cornerstone of intelligent transportation system. Accurate traffic forecasting is essential for the applications of smart cities, i.e., intelligent traffic management and urban planning. Although various methods are proposed for spatio-temporal modeling, they ignore the dynamic characteristics of correlations among locations on road network. Meanwhile, most Recurrent Neural Network based works are not efficient enough due to their recurrent operations. Additionally, there is a severe lack of fair comparison among different methods on the same datasets. To address the above challenges, in this article, we propose a novel traffic prediction framework, named Dynamic Graph Convolutional Recurrent Network (DGCRN). In DGCRN, hyper-networks are designed to leverage and extract dynamic characteristics from node attributes, while the parameters of dynamic filters are ge… |
| https://openalex.org/W4224227775 | Communication-efficient federated learning via knowledge distillation | 2022 | Nature Communications | article | 484 | yes | Chuhan Wu, Fangzhao Wu, Lingjuan Lyu, Yongfeng Huang, Xing Xie | Federated learning, Computer science, Personalization, Raw data, Distillation, Models of communication, +12 more | https://doi.org/10.1038/s41467-022-29763-x | Abstract Federated learning is a privacy-preserving machine learning technique to train intelligent models from decentralized data, which enables exploiting private data by communicating local model updates in each iteration of model learning rather than the raw data. However, model updates can be extremely large if they contain numerous parameters, and many rounds of communication are needed for model training. The huge communication cost in federated learning leads to heavy overheads on clients and high environmental burdens. Here, we present a federated learning method named FedKD that is both communication-efficient and effective, based on adaptive mutual knowledge distillation and dynamic gradient compression techniques. FedKD is validated on three different scenarios that need privacy protection, showing that it maximally can reduce 94.89% of communication cost and achieve competi… |
| https://openalex.org/W3035231859 | TaBERT: Pretraining for Joint Understanding of Textual and Tabular Data | 2020 |  | article | 390 | yes | Pengcheng Yin, Graham Neubig, Wen-tau Yih, Sebastian Riedel | Computer science, Natural language processing, Parsing, Artificial intelligence, Benchmark (surveying), Feature (linguistics), +11 more | https://doi.org/10.18653/v1/2020.acl-main.745 | Recent years have witnessed the burgeoning of pretrained language models (LMs) for text-based natural language (NL) understanding tasks. Such models are typically trained on free-form NL text, hence may not be suitable for tasks like semantic parsing over structured data, which require reasoning over both free-form NL questions and structured tabular data (e.g., database tables). In this paper we present TaBERT, a pretrained LM that jointly learns representations for NL sentences and (semi-)structured tables. TaBERT is trained on a large corpus of 26 million tables and their English contexts. In experiments, neural semantic parsers using TaBERT as feature representation layers achieve new best results on the challenging weakly-supervised semantic parsing benchmark WikiTableQuestions, while performing competitively on the text-to-SQL dataset Spider. |
| https://openalex.org/W3111180042 | A review of on-device fully neural end-to-end automatic speech recognition algorithms | 2020 | arXiv (Cornell University) | review | 2 | yes | Chanwoo Kim, Dhananjaya Gowda, Dongsoo Lee, Jiyeon Kim, Ankur Kumar, Sung-Soo Kim, Abhinav Garg, Changwoo Han | Computer science, Speech recognition, Artificial neural network, Recurrent neural network, Language model, End-to-end principle, +12 more | http://arxiv.org/abs/2012.07974 | In this paper, we review various end-to-end automatic speech recognition algorithms and their optimization techniques for on-device applications. Conventional speech recognition systems comprise a large number of discrete components such as an acoustic model, a language model, a pronunciation model, a text-normalizer, an inverse-text normalizer, a decoder based on a Weighted Finite State Transducer (WFST), and so on. To obtain sufficiently high speech recognition accuracy with such conventional speech recognition systems, a very large language model (up to 100 GB) is usually needed. Hence, the corresponding WFST size becomes enormous, which prohibits their on-device implementation. Recently, fully neural network end-to-end speech recognition algorithms have been proposed. Examples include speech recognition systems based on Connectionist Temporal Classification (CTC), Recurrent Neural N… |
| https://openalex.org/W4308804398 | Morphologically Motivated Input Variations and Data Augmentation in Turkish-English Neural Machine Translation | 2022 | ACM Transactions on Asian and Low-Resource Language Information Processing | article | 2 | yes | Zeynep Yi̇rmi̇beşoğlu, Tunga Güngör | Machine translation, Computer science, Transformer, Natural language processing, Turkish, Artificial intelligence, +9 more | https://doi.org/10.1145/3571073 | Success of neural networks in natural language processing has paved the way for neural machine translation (NMT), which rapidly became the mainstream approach in machine translation. Significant improvement in translation performance has been achieved with breakthroughs such as encoder-decoder networks, attention mechanism, and Transformer architecture. However, the necessity of large amounts of parallel data for training an NMT system and rare words in translation corpora are issues yet to be overcome. In this article, we approach NMT of the low-resource Turkish-English language pair. We employ state-of-the-art NMT architectures and data augmentation methods that exploit monolingual corpora. We point out the importance of input representation for the morphologically rich Turkish language and make a comprehensive analysis of linguistically and non-linguistically motivated input segmenta… |
| https://openalex.org/W4390678101 | Attention is all you need: utilizing attention in AI-enabled drug discovery | 2023 | Briefings in Bioinformatics | article | 338 | yes | Yang Zhang, Caiqi Liu, Mujiexin Liu, Tianyuan Liu, Hao Lin, Huang Cheng-bing, Ning Lin | Pace, Interpretability, Drug discovery, Computer science, Data science, Resource (disambiguation), +19 more | https://doi.org/10.1093/bib/bbad467 | Abstract Recently, attention mechanism and derived models have gained significant traction in drug development due to their outstanding performance and interpretability in handling complex data structures. This review offers an in-depth exploration of the principles underlying attention-based models and their advantages in drug discovery. We further elaborate on their applications in various aspects of drug development, from molecular screening and target binding to property prediction and molecule generation. Finally, we discuss the current challenges faced in the application of attention mechanisms and Artificial Intelligence technologies, including data quality, model interpretability and computational resource constraints, along with future directions for research. Given the accelerating pace of technological advancement, we believe that attention-based models will have an increasin… |
| https://openalex.org/W4211137602 | Language Model-Based Paired Variational Autoencoders for Robotic Language Learning | 2022 |  | preprint | 4 | yes | Ozan Özdemir | Computer science, Autoencoder, Artificial intelligence, Language model, Transformer, Encoder, +10 more | https://doi.org/10.36227/techrxiv.18532166.v1 | Human infants learn language while interacting with their environment in which their caregivers may describe the objects and actions they perform. Similar to human infants, artificial agents can learn language while interacting with their environment. In this work, first, we present a neural model that bidirectionally binds robot actions and their language descriptions in a simple object manipulation scenario. Building on our previous Paired Variational Autoencoders (PVAE) model, we demonstrate the superiority of the variational autoencoder over standard autoencoders by experimenting with cubes of different colours, and by enabling the production of alternative vocabularies. Additional experiments show that the model's channel-separated visual feature extraction module can cope with objects of different shapes. Next, we introduce PVAE-BERT, which equips the model with a pretrained large… |
| https://openalex.org/W4405042341 | ClinVec: Unified Embeddings of Clinical Codes Enable Knowledge-Grounded AI in Medicine | 2024 |  | preprint | 8 | yes | Ruth Johnson, Uri Gottlieb, Galit Shaham, Lihi Eisen, Jacob Waxman, Stav Devons-Sberro, Curtis R. Ginder, Peter Hong, +6 more | Vocabulary, Generalizability theory, Computer science, Health care, Artificial intelligence, Benchmarking, +18 more | https://doi.org/10.1101/2024.12.03.24318322 | Integrating structured clinical knowledge into artificial intelligence (AI) models remains a major challenge. Medical codes primarily reflect administrative workflows rather than clinical reason ing, limiting AI models’ ability to capture true clinical relationships and undermining their gen eralizability. To address this, we introduce ClinGraph, a clinical knowledge graph that integrates eight EHR-based vocabularies, and ClinVec, a set of 153,166 clinical code embeddings derived from ClinGraph using a graph transformer neural network. ClinVec provides a machine-readable representation of clinical knowledge that captures semantic relationships among diagnoses, med ications, laboratory tests, and procedures. Panels of clinicians from multiple institutions evalu ated the embeddings across 96 diseases and more than 3,000 clinical codes, confirming their alignment with expert knowledge. In… |
| https://openalex.org/W4220863497 | Quo vadis artificial intelligence? | 2022 | Discover Artificial Intelligence | article | 474 | yes | Yuchen Jiang, Xiang Li, Hao Luo, Shen Yin, Okyay Kaynak | Prosperity, Witness, Artificial intelligence, Applications of artificial intelligence, Computer science, Engineering ethics, +3 more | https://doi.org/10.1007/s44163-022-00022-8 |  |
| https://openalex.org/W4379087156 | What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization | 2023 | arXiv (Cornell University) | preprint | 4 | yes | Yufeng Zhang, Fengzhuo Zhang, Zhuoran Yang, Zhaoran Wang | Computer science, Regret, Estimator, Bayesian probability, Generalization, Generalization error, +8 more | http://arxiv.org/abs/2305.19420 | In this paper, we conduct a comprehensive study of In-Context Learning (ICL) by addressing several open questions: (a) What type of ICL estimator is learned by large language models? (b) What is a proper performance metric for ICL and what is the error rate? (c) How does the transformer architecture enable ICL? To answer these questions, we adopt a Bayesian view and formulate ICL as a problem of predicting the response corresponding to the current covariate, given a number of examples drawn from a latent variable model. To answer (a), we show that, without updating the neural network parameters, ICL implicitly implements the Bayesian model averaging algorithm, which is proven to be approximately parameterized by the attention mechanism. For (b), we analyze the ICL performance from an online learning perspective and establish a $\mathcal{O}(1/T)$ regret bound for perfectly pretrained ICL… |
| https://openalex.org/W3203711169 | VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text Understanding | 2021 | Proceedings of the 2021 Conference on Empirical Methods in Natural Language Pro… | article | 368 | yes | Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko, Armen Aghajanyan, Florian Metze, Luke Zettlemoyer, Christoph Feichtenhofer | Shot (pellet), Computer science, Natural language processing, Zero (linguistics), Artificial intelligence, Linguistics, +3 more | https://doi.org/10.18653/v1/2021.emnlp-main.544 | Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko, Armen Aghajanyan, Florian Metze, Luke Zettlemoyer, Christoph Feichtenhofer. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021. |
| https://openalex.org/W3165739088 | Distant context aware text generation from abstract meaning representation | 2021 | Applied Intelligence | article | 6 | no | Sen Yang, Dawei Feng, Yang Liu, Dongsheng Li | Computer science, ENCODE, Encoder, Language model, Theoretical computer science, Artificial intelligence, +12 more | https://doi.org/10.1007/s10489-021-02431-1 |  |
| https://openalex.org/W4413232341 | Autonomous Healthcare Diagnostics : A Multi-Modal AI Framework Using AWS SageMaker, Lambda, and Deep Learning Orchestration for Real-Time Medical Image Analysis | 2023 | International Journal of Scientific Research in Computer Science Engineering an… | article | 5 | yes | Praveen Kumar Reddy Gujjala | Orchestration, Modal, Artificial intelligence, Computer science, Deep learning, Computer vision, +9 more | https://doi.org/10.32628/cseit23564527 | The exponential growth in medical imaging data coupled with radiologist shortages has created critical bottlenecks in healthcare diagnostics, particularly in emergency and rural settings where immediate expert analysis is unavailable. This paper presents a novel autonomous healthcare diagnostics framework leveraging AWS cloud infrastructure, combining Amazon SageMaker for distributed model training, AWS Lambda for serverless inference orchestration, and Amazon Bedrock for large language model integration. Our multi-modal approach integrates Vision Transformers, Convolutional Neural Networks, and Graph Neural Networks within a unified architecture that processes CT scans, MRIs, X-rays, and clinical metadata simultaneously. The system employs advanced feature fusion techniques using attention mechanisms, federated learning across multiple hospital networks, and real-time model adaptation… |
| https://openalex.org/W4403131845 | Deep Learning Approaches To Image Classification: Exploring The Future Of Visual Data Analysis | 2022 |  | article | 4 | yes | Manikanth Sarisa, Venkata Nagesh Boddapati, Gagan Kumar Patra, Chandrababu Kuraku, Siddharth Konkimalla | Computer science, Artificial intelligence, Image (mathematics), Pattern recognition (psychology), Deep learning, Data science | https://doi.org/10.53555/kuey.v28i4.7863 | Over the past decade, deep learning has emerged as a revolutionary technology for the analysis of visual data, particularly images. This master's thesis focuses on deep learning approaches to image classification, which is a key task in many applications using visual data analysis. A state-of-the-art deep learning model, namely the Vision Transformer (ViT), is explored for image classification. ViT is trained using transfer-learning techniques on a new dataset of over 350,000 photographs of European buildings in eight cities, obtained across two separate flights from a drone-mounted camera. Initial results demonstrate that models pre-trained on large datasets such as JFT-300M can achieve performance competitively with the fine-tuning of models trained from scratch on smaller datasets and that ViT outperforms convolutional neural networks for drone-captured images. Further, the prospects… |
| https://openalex.org/W4224315052 | ET-BERT: A Contextualized Datagram Representation with Pre-training Transformers for Encrypted Traffic Classification | 2022 | Proceedings of the ACM Web Conference 2022 | article | 363 | yes | Xinjie Lin, Gang Xiong, Gaopeng Gou, Zhen Li, Junzheng Shi, Jing Yu | Computer science, Encryption, Traffic classification, Encoder, Artificial intelligence, Machine learning, +4 more | https://doi.org/10.1145/3485447.3512217 | Encrypted traffic classification requires discriminative and robust traffic\nrepresentation captured from content-invisible and imbalanced traffic data for\naccurate classification, which is challenging but indispensable to achieve\nnetwork security and network management. The major limitation of existing\nsolutions is that they highly rely on the deep features, which are overly\ndependent on data size and hard to generalize on unseen data. How to leverage\nthe open-domain unlabeled traffic data to learn representation with strong\ngeneralization ability remains a key challenge. In this paper,we propose a new\ntraffic representation model called Encrypted Traffic Bidirectional Encoder\nRepresentations from Transformer (ET-BERT), which pre-trains deep\ncontextualized datagram-level representation from large-scale unlabeled data.\nThe pre-trained model can be fine-tuned on a small number… |
| https://openalex.org/W4365504037 | A survey on deep learning tools dealing with data scarcity: definitions, challenges, solutions, tips, and applications | 2023 | Journal Of Big Data | article | 667 | yes | Laith Alzubaidi, Jinshuai Bai, Aiman Al-Sabaawi, José Santamaría, A. S. Albahri, Bashar Sami Nayyef Al-dabbagh, Mohammed A. Fadhel, Mohamed Manoufali, +10 more | Computer science, Deep learning, Artificial intelligence, Machine learning, Generalization, Scarcity, +8 more | https://doi.org/10.1186/s40537-023-00727-2 |  |
| https://openalex.org/W3137125108 | Interpretable machine learning: Fundamental principles and 10 grand challenges | 2022 | Statistics Surveys | article | 782 | yes | Cynthia Rudin, Chaofan Chen, Zhi Chen, Haiyang Huang, Lesia Semenova, Chudi Zhong | Interpretability, Artificial intelligence, Machine learning, Computer science, Artificial neural network, Troubleshooting, +5 more | https://doi.org/10.1214/21-ss133 | Interpretability in machine learning (ML) is crucial for high stakes decisions and troubleshooting. In this work, we provide fundamental principles for interpretable ML, and dispel common misunderstandings that dilute the importance of this crucial topic. We also identify 10 technical challenge areas in interpretable machine learning and provide history and background on each problem. Some of these problems are classically important, and some are recent problems that have arisen in the last few years. These problems are: (1) Optimizing sparse logical models such as decision trees; (2) Optimization of scoring systems; (3) Placing constraints into generalized additive models to encourage sparsity and better interpretability; (4) Modern case-based reasoning, including neural networks and matching for causal inference; (5) Complete supervised disentanglement of neural networks; (6) Complete… |
| https://openalex.org/W3167136668 | A Frustratingly Easy Approach for Entity and Relation Extraction | 2021 |  | article | 423 | yes | Zexuan Zhong, Danqi Chen | Relationship extraction, Computer science, Relation (database), Inference, Context (archaeology), Artificial intelligence, +16 more | https://doi.org/10.18653/v1/2021.naacl-main.5 | End-to-end relation extraction aims to identify named entities and extract relations between them. Most recent work models these two subtasks jointly, either by casting them in one structured prediction framework, or performing multi-task learning through shared representations. In this work, we present a simple pipelined approach for entity and relation extraction, and establish the new state-of-the-art on standard benchmarks (ACE04, ACE05 and SciERC), obtaining a 1.7%-2.8% absolute improvement in relation F1 over previous joint models with the same pre-trained encoders. Our approach essentially builds on two independent encoders and merely uses the entity model to construct the input for the relation model. Through a series of careful examinations, we validate the importance of learning distinct contextual representations for entities and relations, fusing entity information early in… |
| https://openalex.org/W4392749751 | A text classification network model combining machine learning and deep learning | 2024 | International Journal of Sensor Networks | article | 5 | no | Hao Chen, Haifei Zhang, Yuwei Yang, Long He | Computer science, Artificial intelligence, Deep learning, Machine learning, Natural language processing | https://doi.org/10.1504/ijsnet.2024.137333 | Text classification is significant in natural language processing tasks, which can deal with a large amount of data scientifically. However, for text feature extraction, it is not easy to simultaneously consider the characteristics of short and long texts. Moreover, it does not reflect the importance of words in the text, resulting in unsatisfactory text classification results. Therefore, this paper proposes a machine learning and deep learning model. Specifically, text features are extracted by joint training, and then an attention mechanism is introduced to classify short texts and long texts. Firstly, the pre-processed data is subjected to term frequency-inverse document frequency, text convolutional neural networks and rotary transformer models for joint extraction of text features. Subsequently, the attention mechanism is introduced for the weight distribution problem after model f… |
| https://openalex.org/W4378572016 | KanSan: Kannada-Sanskrit Parallel Corpus Construction for Machine Translation | 2023 | Communications in computer and information science | book-chapter | 2 | no | Asha Hegde, Hosahalli Lakshmaiah Shashirekha | Telugu, Sanskrit, Machine translation, Computer science, Natural language processing, Artificial intelligence, +8 more | https://doi.org/10.1007/978-3-031-33231-9_1 |  |
| https://openalex.org/W3155807546 | Retrieval Augmentation Reduces Hallucination in Conversation | 2021 |  | preprint | 405 | yes | Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, Jason Weston | Conversation, Computer science, Context (archaeology), Task (project management), Domain (mathematical analysis), Natural language processing, +15 more | https://doi.org/10.18653/v1/2021.findings-emnlp.320 | Despite showing increasingly human-like conversational abilities, state-of-the-art dialogue models often suffer from factual incorrectness and hallucination of knowledge (Roller et al., 2020). In this work we explore the use of neural-retrieval-in-the-loop architectures - recently shown to be effective in open-domain QA (Lewis et al., 2020b; Izacard and Grave, 2020) - for knowledge-grounded dialogue, a task that is arguably more challenging as it requires querying based on complex multi-turn dialogue context and generating conversationally coherent responses. We study various types of architectures with multiple components - retrievers, rankers, and encoder-decoders - with the goal of maximizing knowledgeability while retaining conversational ability. We demonstrate that our best models obtain state-of-the-art performance on two knowledge-grounded conversational tasks. The models exhibi… |
| https://openalex.org/W3214603987 | Generative Pre-Training from Molecules | 2021 |  | preprint | 2 | yes | Sanjar Adilov | Transformer, Computer science, Notation, Artificial intelligence, Generative grammar, Natural language processing, +9 more | https://doi.org/10.33774/chemrxiv-2021-5fwjd | SMILES is a line notation for entering and representing molecules. Being inherently a language construct, it allows estimating molecular data in a self-supervised fashion by employing machine learning methods for natural language processing (NLP). The recent success of attention-based neural networks in NLP has made large-corpora transformer pretraining a de facto standard for learning representations and transferring knowledge to downstream tasks. In this work, we attempt to adapt transformer capabilities to a large SMILES corpus by constructing a GPT-2-like language model. We experimentally show that a pretrained causal transformer captures general knowledge that can be successfully transferred to such downstream tasks as focused molecule generation and single-/multi-output molecular-property prediction. For each task, we freeze model parameters and attach trainable lightweight networ… |
| https://openalex.org/W3173783447 | DeCLUTR: Deep Contrastive Learning for Unsupervised Textual Representations | 2021 |  | article | 354 | yes | John Giorgi, Osvald Nitski, Bo Wang, Gary D. Bader | Computer science, Natural language processing, Artificial intelligence, Linguistics, Computational linguistics, Volume (thermodynamics), +11 more | https://doi.org/10.18653/v1/2021.acl-long.72 | John Giorgi, Osvald Nitski, Bo Wang, Gary Bader. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021. |
| https://openalex.org/W4396786706 | AtomGPT: Atomistic Generative Pre-trained Transformer for Forward and Inverse Materials Design | 2024 | arXiv (Cornell University) | preprint | 2 | yes | Kamal Choudhary | Generative grammar, Transformer, Inverse, Computer science, Artificial intelligence, Mathematics, +4 more | http://arxiv.org/abs/2405.03680 | Large language models (LLMs) such as generative pretrained transformers (GPTs) have shown potential for various commercial applications, but their applicability for materials design remains underexplored. In this article, we introduce AtomGPT, a model specifically developed for materials design based on transformer architectures, to demonstrate the capability for both atomistic property prediction and structure generation. We show that a combination of chemical and structural text descriptions can efficiently predict material properties with accuracy comparable to graph neural network models, including formation energies, electronic bandgaps from two different methods and superconducting transition temperatures. Furthermore, we demonstrate that AtomGPT can generate atomic structures for tasks such as designing new superconductors, with the predictions validated through density functiona… |
| https://openalex.org/W2970785793 | MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance | 2019 |  | article | 462 | yes | Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M. Meyer, Steffen Eger | Computer science, Natural language processing, Artificial intelligence, Joint (building), Natural (archaeology), Natural language, +4 more | https://doi.org/10.18653/v1/d19-1053 | A robust evaluation metric has a profound impact on the development of text generation systems. A desirable metric compares system output against references based on their semantics rather than surface forms. In this paper we investigate strategies to encode system and reference texts to devise a metric that shows a high correlation with human judgment of text quality. We validate our new metric, namely MoverScore, on a number of text generation tasks including summarization, machine translation, image captioning, and data-to-text generation, where the outputs are produced by a variety of neural and non-neural systems. Our findings suggest that metrics combining contextualized representations with a distance measure perform the best. Such metrics also demonstrate strong generalization capability across tasks. For ease-of-use we make our metrics available as web service. |
| https://openalex.org/W3100439847 | ProphetNet: Predicting Future N-gram for Sequence-to-SequencePre-training | 2020 |  | article | 336 | yes | Weizhen Qi, Yu Yan, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei Zhang, Ming Zhou | Computer science, Overfitting, Automatic summarization, Sequence (biology), Artificial intelligence, n-gram, +12 more | https://doi.org/10.18653/v1/2020.findings-emnlp.217 | This paper presents a new sequence-to-sequence pre-training model called ProphetNet, which introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. Instead of optimizing one-step-ahead prediction in the traditional sequence-to-sequence model, the ProphetNet is optimized by n-step ahead prediction that predicts the next n tokens simultaneously based on previous context tokens at each time step. The future n-gram prediction explicitly encourages the model to plan for the future tokens and prevent overfitting on strong local correlations. We pre-train ProphetNet using a base scale dataset (16GB) and a large-scale dataset (160GB), respectively. Then we conduct experiments on CNN/DailyMail, Gigaword, and SQuAD 1.1 benchmarks for abstractive summarization and question generation tasks. Experimental results show that Prophe… |
| https://openalex.org/W4387831890 | Vision Transformer-based recognition tasks: a critical review | 2023 | Journal of Image and Graphics | review | 5 | yes | Lijuan Zhou, Mao Jianing | Computer science, Transformer, Artificial intelligence, Segmentation, Facial recognition system, Face detection, +9 more | https://doi.org/10.11834/jig.220895 | Transformer模型在自然语言处理领域取得了很好的效果,同时因其能够更好地连接视觉和语言,也激发了计算机视觉界的极大兴趣。本文总结了视觉Transformer处理多种识别任务的百余种代表性方法,并对比分析了不同任务内的模型表现,在此基础上总结了每类任务模型的优点、不足以及面临的挑战。根据识别粒度的不同,分别着眼于诸如图像分类、视频分类的基于全局识别的方法,以及目标检测、视觉分割的基于局部识别的方法。考虑到现有方法在3种具体识别任务的广泛流行,总结了在人脸识别、动作识别和姿态估计中的方法。同时,也总结了可用于多种视觉任务或领域无关的通用方法的研究现状。基于Transformer的模型实现了许多端到端的方法,并不断追求准确率与计算成本的平衡。全局识别任务下的Transformer模型对补丁序列切分和标记特征表示进行了探索,局部识别任务下的Transformer模型因能够更好地捕获全局信息而取得了较好的表现。在人脸识别和动作识别方面,注意力机制减少了特征表示的误差,可以处理丰富多样的特征。Transformer可以解决姿态估计中特征错位的问题,有利于改善基于回归的方法性能,还减少了三维估计时深度映射所产生的歧义。大量探索表明视觉Transformer在识别任务中的有效性,并且在特征表示或网络结构等方面的改进有利于提升性能。;Due to its ability to model long-distance dependencies,self-attention mechanism for adaptive computing,scalability for large models and big data,and better connection between vision and language,Transformer model is beneficial for natural language processing and computer vision apparently. To melt Transformer into vision tasks… |
| https://openalex.org/W4284692572 | Adversarial attacks and defenses in natural language processing | 2022 |  | dissertation | 4 | yes | Xinshuai Dong | Adversarial system, Computer science, Artificial intelligence, Robustness (evolution), Inference, Convolutional neural network, +7 more | https://doi.org/10.32657/10356/159029 | Deep neural networks (DNNs) are becoming increasingly successful in many fields. However, DNNs are shown to be strikingly susceptible to adversarial examples. For instance, models pre-trained on very large corpora can still be easily fooled by word substitution attacks using only synonyms. This phenomenon has raised grand security challenges to modern machine learning systems, such as self-driving, spam filtering, and speech recognition, where DNNs are widely deployed. In this thesis, we first give a brief introduction of adversarial attacks and defenses. We focus on Natural Language Processing (NLP) and review some recent advances in attack algorithms and defense methods in Chapter 2. We also give a formalized definition of the research objective in this thesis, i.e., how to improve the adversarial robustness of NLP models. To this end, we propose novel and effective solutions to enhan… |
| https://openalex.org/W3188404242 | TransMed: Transformers Advance Multi-Modal Medical Image Classification | 2021 | Diagnostics | article | 396 | yes | Yin Dai, Yifan Gao, Fayu Liu | Computer science, Convolutional neural network, Artificial intelligence, Transformer, Locality, Pattern recognition (psychology), +14 more | https://doi.org/10.3390/diagnostics11081384 | Over the past decade, convolutional neural networks (CNN) have shown very competitive performance in medical image analysis tasks, such as disease classification, tumor segmentation, and lesion detection. CNN has great advantages in extracting local features of images. However, due to the locality of convolution operation, it cannot deal with long-range relationships well. Recently, transformers have been applied to computer vision and achieved remarkable success in large-scale datasets. Compared with natural images, multi-modal medical images have explicit and important long-range dependencies, and effective multi-modal fusion strategies can greatly improve the performance of deep models. This prompts us to study transformer-based structures and apply them to multi-modal medical images. Existing transformer-based network architectures require large-scale datasets to achieve better perf… |
| https://openalex.org/W4409166874 | Detection and classification of ChatGPT-generated content using deep transformer models | 2025 | Frontiers in Artificial Intelligence | article | 9 | yes | Lakshmi Babu Saheer, Lakshmi Babu Saheer, Kshipra Dhame, Gayathri Singaram | Generalizability theory, Artificial intelligence, Computer science, Deep learning, Machine learning, Transformer, +16 more | https://doi.org/10.3389/frai.2025.1458707 | Introduction The rapid advancement of AI, particularly artificial neural networks, has led to revolutionary breakthroughs and applications, such as text-generating tools and chatbots. However, this potent technology also introduces potential misuse and societal implications, including privacy violations, misinformation, and challenges to integrity and originality in academia. Several studies have attempted to distinguish and classify AI-generated textual content from human-authored work, but their performance remains questionable, particularly for AI models utilizing large language models like ChatGPT. Methods To address this issue, we compiled a dataset consisting of both human-written and AI-generated (ChatGPT) content. This dataset was then used to train and evaluate a range of machine learning and deep learning models under various training conditions. We assessed the efficacy of di… |
| https://openalex.org/W4400089152 | Advanced NLP Techniques for Sentiment Analysis and Text Summarization Using RNNs and Transformers | 2024 | International Journal for Research in Applied Science and Engineering Technology | article | 2 | yes | Kumar Pritam | Automatic summarization, Computer science, Artificial intelligence, Natural language processing, Recurrent neural network, Artificial neural network, +9 more | https://doi.org/10.22214/ijraset.2024.63358 | Abstract: This research focuses on leveraging artificial intelligence and neural network architectures to enhance the capability of machines in comprehending, interpreting, and summarizing text data in human languages. The study aims to improve natural language processing (NLP) tasks, specifically sentiment classification and text summarization. Key efforts include the development of neural network architectures such as Recurrent Neural Networks (RNNs) and Transformers to model linguistic contexts and sequences. The creation of annotated datasets for sentiment analysis and summarization was essential for training and evaluating these models. Additionally, transfer learning techniques were explored to pretrain language models on large corpora, enhancing their performance. The evaluation of neural network models utilized relevant NLP metrics like accuracy, ROC curve, and F1 score for sent… |
| https://openalex.org/W3034503922 | MIND: A Large-scale Dataset for News Recommendation | 2020 |  | article | 433 | yes | Fangzhao Wu, Ying Qiao, Jiun-Hung Chen, Chuhan Wu, Tao Qi, Jianxun Lian, Danyang Liu, Xing Xie, +3 more | Chen, Computational linguistics, Natural language processing, Scale (ratio), Computer science, Artificial intelligence, +8 more | https://doi.org/10.18653/v1/2020.acl-main.331 | Fangzhao Wu, Ying Qiao, Jiun-Hung Chen, Chuhan Wu, Tao Qi, Jianxun Lian, Danyang Liu, Xing Xie, Jianfeng Gao, Winnie Wu, Ming Zhou. Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 2020. |
| https://openalex.org/W4406170795 | Accurate predictions on small data with a tabular foundation model | 2025 | Nature | article | 361 | yes | Noah Hollmann, Samuel Müller, Lennart Purucker, Arjun Krishnakumar, Max Körfer, Shi Bin Hoo, Robin Tibor Schirrmeister, Frank Hutter | Computer science, Raw data, Artificial intelligence, Deep learning, Machine learning, Ensemble learning, +4 more | https://doi.org/10.1038/s41586-024-08328-6 |  |
| https://openalex.org/W4285102489 | Depth-Aware Vision-and-Language Navigation using Scene Query Attention Network | 2022 | 2022 International Conference on Robotics and Automation (ICRA) | article | 3 | no | Sinan Tan, Meng-Meng Ge, Di Guo, Huaping Liu, Fuchun Sun | Computer science, Artificial intelligence, Transformer, RGB color model, Robot, Task (project management), +14 more | https://doi.org/10.1109/icra46639.2022.9811921 | Vision-and-language navigation (VLN) has been an important task in the field of Robotics and Computer Vision. However, most existing vision-and-language navigation models only use features extracted from RGB observation as input, while robots can utilize depth sensors in the real world. Existing research has also shown that simply adding a depth stream to neural models could only provide a marginal improvement to the performance of the VLN task. Therefore, in our work, we develop a novel method for the VLN task using semantic map observations built from RGB-D input. We use vision-pretraining to efficiently encode the semantic map with CNN and scene query attention network by answering queries about semantic information of specific regions of a scene. The proposed method could be used with a simple model and does not require large-scale vision-language transformer pretraining, bringing a… |
| https://openalex.org/W4389519254 | G-Eval: NLG Evaluation using Gpt-4 with Better Human Alignment | 2023 |  | article | 447 | yes | Yang Liu, Dan Iter, Xu Yi‐chong, Shuohang Wang, Ruochen Xu, Chenguang Zhu | Automatic summarization, Natural language generation, Computer science, Artificial intelligence, Natural language processing, Margin (machine learning), +10 more | https://doi.org/10.18653/v1/2023.emnlp-main.153 | The quality of texts generated by natural language generation (NLG) systems is hard to measure automatically. Conventional reference-based metrics, such as BLEU and ROUGE, have been shown to have relatively low correlation with human judgments, especially for tasks that require creativity and diversity. Recent studies suggest using large language models (LLMs) as reference-free metrics for NLG evaluation, which have the benefit of being applicable to new tasks that lack human references. However, these LLM-based evaluators still have lower human correspondence than medium-size neural evaluators. In this work, we present G-Eval, a framework of using large language models with chain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of NLG outputs. We experiment with two generation tasks, text summarization and dialogue generation. We show that G-Eval with GPT-4 as the b… |
| https://openalex.org/W3133458480 | MSA Transformer | 2021 |  | preprint | 327 | yes | Roshan Rao, Jason Liu, Robert Verkuil, Joshua Meier, John Canny, Pieter Abbeel, Tom Sercu, Alexander Rives | Computer science, Language model, Inference, Margin (machine learning), Transformer, Artificial intelligence, +13 more | https://doi.org/10.1101/2021.02.12.430858 | Abstract Unsupervised protein language models trained across millions of diverse sequences learn structure and function of proteins. Protein language models studied to date have been trained to perform inference from individual sequences. The longstanding approach in computational biology has been to make inferences from a family of evo lutionarily related sequences by fitting a model to each family independently. In this work we combine the two paradigms. We introduce a protein language model which takes as input a set of sequences in the form of a multiple sequence alignment. The model interleaves row and column attention across the input sequences and is trained with a variant of the masked language modeling objective across many protein families. The performance of the model surpasses current state-of-the-art unsupervised structure learning methods by a wide margin, with far greater… |
| https://openalex.org/W3020873385 | Survey on categorical data for neural networks | 2020 | Journal Of Big Data | article | 551 | yes | John Hancock, Taghi M. Khoshgoftaar | Categorical variable, Computer science, Artificial neural network, Leverage (statistics), Artificial intelligence, Machine learning, +11 more | https://doi.org/10.1186/s40537-020-00305-w | Abstract This survey investigates current techniques for representing qualitative data for use as input to neural networks. Techniques for using qualitative data in neural networks are well known. However, researchers continue to discover new variations or entirely new methods for working with categorical data in neural networks. Our primary contribution is to cover these representation techniques in a single work. Practitioners working with big data often have a need to encode categorical values in their datasets in order to leverage machine learning algorithms. Moreover, the size of data sets we consider as big data may cause one to reject some encoding techniques as impractical, due to their running time complexity. Neural networks take vectors of real numbers as inputs. One must use a technique to map qualitative values to numerical values before using them as input to a neural netw… |
| https://openalex.org/W4307936861 | Leveraging Pre-trained Models for Failure Analysis Triplets Generation | 2022 | arXiv (Cornell University) | preprint | 2 | yes | Kenneth Ezukwoke, Anis Hoayek, Mireille Batton-Hubert, Xavier Boucher, Pascal Gounet, Jérôme Adrian | Transformer, Computer science, Generative grammar, Artificial intelligence, Language model, Automatic summarization, +7 more | http://arxiv.org/abs/2210.17497 | Pre-trained Language Models recently gained traction in the Natural Language Processing (NLP) domain for text summarization, generation and question-answering tasks. This stems from the innovation introduced in Transformer models and their overwhelming performance compared with Recurrent Neural Network Models (Long Short Term Memory (LSTM)). In this paper, we leverage the attention mechanism of pre-trained causal language models such as Transformer model for the downstream task of generating Failure Analysis Triplets (FATs) - a sequence of steps for analyzing defected components in the semiconductor industry. We compare different transformer models for this generative task and observe that Generative Pre-trained Transformer 2 (GPT2) outperformed other transformer model for the failure analysis triplet generation (FATG) task. In particular, we observe that GPT2 (trained on 1.5B parameter… |
| https://openalex.org/W2970200208 | Benchmarking Zero-shot Text Classification: Datasets, Evaluation and Entailment Approach | 2019 |  | article | 475 | yes | Wenpeng Yin, Jamaal Hay, Dan Roth | Benchmarking, Computer science, Zero (linguistics), Textual entailment, Artificial intelligence, Natural language processing, +6 more | https://doi.org/10.18653/v1/d19-1404 | Wenpeng Yin, Jamaal Hay, Dan Roth. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019. |
| https://openalex.org/W3201870057 | Training Spiking Neural Networks Using Lessons From Deep Learning | 2023 | Proceedings of the IEEE | article | 572 | yes | Jason K. Eshraghian, Max Ward, Emre Neftci, Xinxin Wang, Gregor Lenz, Girish Dwivedi, Mohammed Bennamoun, Doo Seok Jeong, +1 more | Computer science, Python (programming language), Artificial intelligence, Deep learning, Spiking neural network, Backpropagation, +4 more | https://doi.org/10.1109/jproc.2023.3308088 | The brain is the perfect place to look for inspiration to develop more efficient neural networks. The inner workings of our synapses and neurons provide a glimpse at what the future of deep learning might look like. This article serves as a tutorial and perspective showing how to apply the lessons learned from several decades of research in deep learning, gradient descent, backpropagation, and neuroscience to biologically plausible spiking neural networks (SNNs). We also explore the delicate interplay between encoding data as spikes and the learning process; the challenges and solutions of applying gradient-based learning to SNNs; the subtle link between temporal backpropagation and spike timing-dependent plasticity; and how deep learning might move toward biologically plausible online learning. Some ideas are well accepted and commonly used among the neuromorphic engineering community,… |
| https://openalex.org/W3158287159 | Sector classification for crowd-based software requirements | 2021 |  | article | 6 | no | Kushagra Bhatia, Arpit Sharma | Computer science, Software requirements, Software, Requirements engineering, User requirements document, Machine learning, +6 more | https://doi.org/10.1145/3412841.3442005 | Requirements engineering (RE) is the process of defining, documenting, and maintaining software requirements. Crowd-based RE (CrowdRE) involves large scale user participation in requirements engineering tasks. It improves the quality of software requirements and helps in reducing the cost. Manual extraction of useful insights from a large body of unstructured, and noisy natural language data produced during CrowdRE is an expensive, error prone and time consuming task. Thus, automated techniques are required for processing the CrowdRE data. We focus on the problem of automatic classification of crowd-based software requirements into sectors. We propose three different approaches for sector classification of crowd-based software requirements. These approaches are based on supervised machine learning (ML) models, neural networks, and bidirectional encoder representations from transformers… |
| https://openalex.org/W3037572520 | Learning Relationships between Text, Audio, and Video via Deep Canonical Correlation for Multimodal Language Analysis | 2020 | Proceedings of the AAAI Conference on Artificial Intelligence | article | 353 | yes | Zhongkai Sun, Prathusha K Sarma, William A. Sethares, Yingyu Liang | Canonical correlation, Computer science, Utterance, Artificial intelligence, Benchmark (surveying), Natural language processing, +8 more | https://doi.org/10.1609/aaai.v34i05.6431 | Multimodal language analysis often considers relationships between features based on text and those based on acoustical and visual properties. Text features typically outperform non-text features in sentiment analysis or emotion recognition tasks in part because the text features are derived from advanced language models or word embeddings trained on massive data sources while audio and video features are human-engineered and comparatively underdeveloped. Given that the text, audio, and video are describing the same utterance in different ways, we hypothesize that the multimodal sentiment analysis and emotion recognition can be improved by learning (hidden) correlations between features extracted from the outer product of text and audio (we call this text-based audio) and analogous text-based video. This paper proposes a novel model, the Interaction Canonical Correlation Network (ICCN),… |
| https://openalex.org/W3010145447 | Predicting retrosynthetic pathways using transformer-based models and a hyper-graph exploration strategy | 2020 | Chemical Science | article | 424 | yes | Philippe Schwaller, Riccardo Petraglia, Valerio Zullo, Vishnu H Nair, Rico Andreas Haeuselmann, Riccardo Pisoni, Costas Bekas, Anna Iuliano, +1 more | Retrosynthetic analysis, Transformer, Computer science, Artificial intelligence, Graph, Machine learning, +8 more | https://doi.org/10.1039/c9sc05704h | We present an extension of our Molecular Transformer model combined with a hyper-graph exploration strategy for automatic retrosynthesis route planning without human intervention. |
| https://openalex.org/W4389335560 | Predicting Patients' Satisfaction With Mental Health Drug Treatment Using Their Reviews: Unified Interchangeable Model Fusion Approach | 2023 | JMIR Mental Health | article | 4 | yes | Yi Wang, Yide Yu, Yue Liu, Yan Ma, Patrick Cheong‐Iao Pang | Mental health, Psychology, Medicine, Artificial intelligence, Psychotherapist, Computer science | https://doi.org/10.2196/49894 | Background After the COVID-19 pandemic, the conflict between limited mental health care resources and the rapidly growing number of patients has become more pronounced. It is necessary for psychologists to borrow artificial intelligence (AI)–based methods to analyze patients’ satisfaction with drug treatment for those undergoing mental illness treatment. Objective Our goal was to construct highly accurate and transferable models for predicting the satisfaction of patients with mental illness with medication by analyzing their own experiences and comments related to medication intake. Methods We extracted 41,851 reviews in 20 categories of disorders related to mental illnesses from a large public data set of 161,297 reviews in 16,950 illness categories. To discover a more optimal structure of the natural language processing models, we proposed the Unified Interchangeable Model Fusion to… |
| https://openalex.org/W4296079469 | A Survey of Ensemble Learning: Concepts, Algorithms, Applications, and Prospects | 2022 | IEEE Access | article | 987 | yes | Ibomoiye Domor Mienye, Yanxia Sun | Boosting (machine learning), Gradient boosting, AdaBoost, Ensemble learning, Machine learning, Artificial intelligence, +5 more | https://doi.org/10.1109/access.2022.3207287 | Ensemble learning techniques have achieved state-of-the-art performance in diverse machine learning applications by combining the predictions from two or more base models. This paper presents a concise overview of ensemble learning, covering the three main ensemble methods: bagging, boosting, and stacking, their early development to the recent state-of-the-art algorithms. The study focuses on the widely used ensemble algorithms, including random forest, adaptive boosting (AdaBoost), gradient boosting, extreme gradient boosting (XGBoost), light gradient boosting machine (LightGBM), and categorical boosting (CatBoost). An attempt is made to concisely cover their mathematical and algorithmic representations, which is lacking in the existing literature and would be beneficial to machine learning researchers and practitioners. |
| https://openalex.org/W3016996576 | Robustly Pre-trained Neural Model for Direct Temporal Relation Extraction | 2020 | arXiv (Cornell University) | preprint | 5 | yes | Hong Guan, Jianfu Li, Hua Xu, Murthy Devarakonda | Computer science, Artificial intelligence, Personalization, Relationship extraction, Relation (database), Artificial neural network, +15 more | http://arxiv.org/abs/2004.06216 | Background: Identifying relationships between clinical events and temporal expressions is a key challenge in meaningfully analyzing clinical text for use in advanced AI applications. While previous studies exist, the state-of-the-art performance has significant room for improvement. Methods: We studied several variants of BERT (Bidirectional Encoder Representations using Transformers) some involving clinical domain customization and the others involving improved architecture and/or training strategies. We evaluated these methods using a direct temporal relations dataset which is a semantically focused subset of the 2012 i2b2 temporal relations challenge dataset. Results: Our results show that RoBERTa, which employs better pre-training strategies including using 10x larger corpus, has improved overall F measure by 0.0864 absolute score (on the 1.00 scale) and thus reducing the error rate… |
| https://openalex.org/W4383737134 | A Survey on Large Language Models: Applications, Challenges, Limitations, and Practical Usage | 2023 |  | preprint | 332 | yes | Muhammad Usman Hadi, qasem al tashi, Rizwan Qureshi, Abbas Shah, Amgad Muneer, Muhammad Irfan, Anas Zafar, Muhammad Bilal Shaikh, +3 more | Interpretability, Computer science, Artificial intelligence | https://doi.org/10.36227/techrxiv.23589741.v1 | &lt;p&gt;Within the vast expanse of computerized language processing, a revolutionary entity known as Large Language Models (LLMs) has emerged, wielding immense power in its capacity to comprehend intricate linguistic patterns and conjure coherent and contextually fitting responses. Large language models (LLMs) are a type of artificial intelligence (AI) that have emerged as powerful tools for a wide range of tasks, including natural language processing (NLP), machine translation, and question-answering. This survey paper provides a comprehensive overview of LLMs, including their history, architecture, training methods, applications, and challenges. The paper begins by discussing the fundamental concepts of generative AI and the architecture of generative pre- trained transformers (GPT). It then provides an overview of the history of LLMs, their evolution over time, and the different tra… |
| https://openalex.org/W3035740499 | Dependency Graph Enhanced Dual-transformer Structure for Aspect-based Sentiment Classification | 2020 |  | article | 323 | yes | Hao Tang, Donghong Ji, Chenliang Li, Qiji Zhou | Computer science, Artificial intelligence, Graph, Transformer, Dependency grammar, Sentence, +10 more | https://doi.org/10.18653/v1/2020.acl-main.588 | Aspect-based sentiment classification is a popular task aimed at identifying the corresponding emotion of a specific aspect. One sentence may contain various sentiments for different aspects. Many sophisticated methods such as attention mechanism and Convolutional Neural Networks (CNN) have been widely employed for handling this challenge. Recently, semantic dependency tree implemented by Graph Convolutional Networks (GCN) is introduced to describe the inner connection between aspects and the associated emotion words. But the improvement is limited due to the noise and instability of dependency trees. To this end, we propose a dependency graph enhanced dual-transformer network (named DGEDT) by jointly considering the flat representations learnt from Transformer and graph-based representations learnt from the corresponding dependency graph in an iterative interaction manner. Specifically… |
| https://openalex.org/W4362581834 | Revolutionizing education with AI: Exploring the transformative potential of ChatGPT | 2023 | Contemporary Educational Technology | article | 763 | yes | Tufan Adıgüzel, Mehmet Haldun Kaya, Fatih Kürşat Cansu | Transformative learning, Computer science, Artificial intelligence, Psychology, Pedagogy | https://doi.org/10.30935/cedtech/13152 | Artificial intelligence (AI) introduces new tools to the educational environment with the potential to transform conventional teaching and learning processes. This study offers a comprehensive overview of AI technologies, their potential applications in education, and the difficulties involved. Chatbots and related algorithms that can simulate human interactions and generate human-like text based on input from natural language are discussed. In addition to the advantages of cutting-edge chatbots like ChatGPT, their use in education raises important ethical and practical challenges. The authors aim to provide insightful information on how AI may be successfully incorporated into the educational setting to benefit teachers and students, while promoting responsible and ethical use. |
| https://openalex.org/W3102925419 | Event Extraction by Answering (Almost) Natural Questions | 2020 |  | article | 380 | yes | Xinya Du, Claire Cardie | Computer science, Event (particle physics), Preprocessor, Question answering, Argument (complex analysis), Artificial intelligence, +16 more | https://doi.org/10.18653/v1/2020.emnlp-main.49 | The problem of event extraction requires detecting the event trigger and extracting its corresponding arguments. Existing work in event argument extraction typically relies heavily on entity recognition as a preprocessing/concurrent step, causing the well-known problem of error propagation. To avoid this issue, we introduce a new paradigm for event extraction by formulating it as a question answering (QA) task that extracts the event arguments in an end-to-end manner. Empirical results demonstrate that our framework outperforms prior methods substantially; in addition, it is capable of extracting event arguments for roles not seen at training time (i.e., in a zero-shot learning setting). |
| https://openalex.org/W4389523751 | Automatic Transcription of Handwritten Old Occitan Language | 2023 |  | article | 2 | yes | Esteban Garces Arias, Vallari Pai, Matthias Schöffel, Christian Heumann, Matthias Aenmacher | Computer science, Transformer, Encoder, Transcription (linguistics), Natural language processing, Artificial intelligence, +14 more | http://dx.doi.org/10.18653/v1/2023.emnlp-main.953 | While existing neural network-based approaches have shown promising results in Handwritten Text Recognition (HTR) for high-resource languages and standardized/machine-written text, their application to low-resource languages often presents challenges, resulting in reduced effectiveness. In this paper, we propose an innovative HTR approach that leverages the Transformer architecture for recognizing handwritten Old Occitan language. Given the limited availability of data, which comprises only word pairs of graphical variants and lemmas, we develop and rely on elaborate data augmentation techniques for both text and image data. Our model combines a custom-trained Swin image encoder with a BERT text decoder, which we pre-train using a large-scale augmented synthetic data set and fine-tune on the small human-labeled data set. Experimental results reveal that our approach surpasses the perfor… |
| https://openalex.org/W3105232955 | HERO: Hierarchical Encoder for Video+Language Omni-representation Pre-training | 2020 |  | article | 378 | yes | Linjie Li, Yen‐Chun Chen, Yu Cheng, Zhe Gan, Licheng Yu, Jingjing Liu | Computer science, Artificial intelligence, Encoder, Transformer, Language model, Video tracking, +11 more | https://doi.org/10.18653/v1/2020.emnlp-main.161 | We present HERO, a novel framework for large-scale video+language omni-representation learning. HERO encodes multimodal inputs in a hierarchical structure, where local context of a video frame is captured by a Cross-modal Transformer via multimodal fusion, and global video context is captured by a Temporal Transformer. In addition to standard Masked Language Modeling (MLM) and Masked Frame Modeling (MFM) objectives, we design two new pre-training tasks: (i) Video-Subtitle Matching (VSM), where the model predicts both global and local temporal alignment; and (ii) Frame Order Modeling (FOM), where the model predicts the right order of shuffled video frames. HERO is jointly trained on HowTo100M and large-scale TV datasets to gain deep understanding of complex social dynamics with multi-character interactions. Comprehensive experiments demonstrate that HERO achieves new state of the art on… |
| https://openalex.org/W4400949264 | AI models collapse when trained on recursively generated data | 2024 | Nature | article | 410 | yes | Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Nicolas Papernot, Ross Anderson, Yarin Gal | Generative grammar, Generative model, Intuition, Computer science, Variety (cybernetics), The Internet, +5 more | https://doi.org/10.1038/s41586-024-07566-y |  |
| https://openalex.org/W7106296053 | Energy-Efficient NLP with Spiking Neural Networks: A Comprehensive Review of Opportunities and Challenges | 2025 | Zenodo (CERN European Organization for Nuclear Research) | preprint | 4 | yes | Alam, Mohammad Zahangir | Neuromorphic engineering, Computer science, Scalability, Spiking neural network, Artificial intelligence, Language model, +13 more | https://doi.org/10.5281/zenodo.17663364 | ABSTRACT—The rapid expansion of natural language processing (NLP) and the widespread adoption of large language models—such as BERT, GPT, LLaMa, and DeepSeek—have intensified global concerns about energy consumption and computational sustainability. Data centers now use approximately 1.5% of global electricity, and AI workloads are projected to contribute an additional 85–134 TWh annually by 2027. Training a single large transformer model can emit hundreds of tons of CO₂-equivalent, raising critical questions about the long-term scalability of current AI paradigms. Spiking Neural Networks (SNNs) offer a biologically inspired pathway toward energy-efficient NLP, utilizing event-driven processing and temporal coding to approach the extraordinary efficiency of the human brain's 20-watt operation. This paper comprehensively surveys opportunities and challenges in integrating SNNs with moder… |
| https://openalex.org/W4390493477 | The Batch Primary Components Transformer and Auto-Plasticity Learning Linear Units Architecture: Synthetic Image Generation Case | 2023 |  | article | 3 | no | Stanislav Selitskiy, Chihiro Inoue, Vitaly Schetinin, Livija Jakaite | MNIST database, Computer science, Transformer, Artificial intelligence, Curse of dimensionality, Dimensionality reduction, +9 more | http://dx.doi.org/10.1109/snams60348.2023.10375471 | Context tokenizing, which is popular in Large Language and Foundation Models (LLM, FM), leads to their excessive dimensionality inflation. Traditional Transformer models strive to reduce intractable excessive dimensionality at the among-token attention level, while we propose additional between-dimensions attention mechanism for dimensionality reduction. A novel Transformer-based architecture is presented, which aims at the individual dimension attention and, by doing so, performs the implicit relevant primary components' feature selection in artificial neural networks (ANN). As an additional mechanism allowing adaptive plasticity learning in ANN, a neuron-specific Learning Rectified Linear Unit layer is proposed for further feature selection via weight decay. The performance of the presented layers is tested on the encoder-decoder architecture applied for the synthetic image generation… |
| https://openalex.org/W4400190916 | Automated Commit Intelligence by Pre-training | 2024 | ACM Transactions on Software Engineering and Methodology | article | 5 | no | Shangqing Liu, Yanzhou Li, Xiaofei Xie, Wei Ma, Guozhu Meng, Yang Liu | Computer science, Commit, Artificial intelligence, Machine learning, Task (project management), Software development, +10 more | https://doi.org/10.1145/3674731 | GitHub commits, which record the code changes with natural language messages for description, play a critical role in software developers’ comprehension of software evolution. Due to their importance in software development, several learning-based works are conducted for GitHub commits, such as commit message generation and security patch identification. However, most existing works focus on customizing specialized neural networks for different tasks. Inspired by the superiority of code pre-trained models, which has confirmed their effectiveness across different downstream tasks, to promote the development of open-source software community, we first collect a large-scale commit benchmark including over 7.99 million commits across 7 programming languages. Based on this benchmark, we present CommitBART, a pre-trained encoder-decoder Transformer model for GitHub commits. The model is pre-t… |
| https://openalex.org/W3121263745 | Deep Learning applications for COVID-19 | 2021 | Journal Of Big Data | article | 343 | yes | Connor Shorten, Taghi M. Khoshgoftaar, Borko Furht | Deep learning, Artificial intelligence, Computer science, Interpretability, Repurposing, Data science, +5 more | https://doi.org/10.1186/s40537-020-00392-9 |  |
| https://openalex.org/W4402039302 | Exploring the potential of DistilBERT architecture for automatic essay scoring task | 2024 | Indonesian Journal of Electrical Engineering and Computer Science | article | 3 | yes | Soumia Ikiss, Najima Daoudi, Manar Abourezq, Mostafa Bellafkih | Task (project management), Architecture, Computer science, Computer architecture, Artificial intelligence, Human–computer interaction, +6 more | https://doi.org/10.11591/ijeecs.v36.i2.pp1234-1241 | &lt;span lang="EN-US"&gt;Automatic assessment of writing essays, or the process of using computers to evaluate and assign grades to written text, is very needed in the education system as an alternative to reduce human burden and time consumption, especially for large-scale tests. This task has received more attention in the last few years, being one of the major uses for natural language processing (NLP). Traditional automatic scoring systems typically rely on handcrafted features, whereas recent studies have used deep neural networks. Since the advent of transformers, pre-trained language models have performed well in many downstream tasks. We utilize the Kaggle benchmarking automated student assessment prize dataset to fine-tune the pre-trained DistilBERT in three different scenarios, and we compare results with the existing neural network-based approaches to achieve improved perform… |
| https://openalex.org/W4389332432 | A Comparative Study of Intent Classification Performance in Truncated Consumer Communication using GPT-Neo and GPT-2 | 2023 |  | article | 3 | no | Chanda Hirway, Enda Fallon, Paul Connolly, K. T. Flanagan, Deepak Yadav | Automatic summarization, Computer science, Artificial intelligence, Transformer, Natural language processing, Machine translation, +6 more | https://doi.org/10.1109/icetci58599.2023.10331337 | This study presents a comparative analysis of intent classification performance using two widely used language models, GPT-Neo and GPT-2, in the context of truncated consumer communications. Generative Pre-trained Transformer (GPT) is a machine learning technique that has revolutionized the field of natural language processing (NLP). GPT uses a transformer-based neural network architecture that is pre-trained on large volumes of data to generate highly accurate and versatile NLP models capable of performing various tasks, such as language translation, question-answering, and text summarization. GPT's ability to generate natural language responses that closely resemble those of humans has greatly enhanced the potential of language processing in the future. The data used in this study was provided by Circana. This data is an essential resource as it includes real world consumer purchases,… |
| https://openalex.org/W4360949780 | Small data machine learning in materials science | 2023 | npj Computational Materials | article | 580 | yes | Pengcheng Xu, Xiaobo Ji, Minjie Li, Wencong Lu | Computer science, Machine learning, Workflow, Artificial intelligence, Small data, Active learning (machine learning), +2 more | https://doi.org/10.1038/s41524-023-01000-z |  |
| https://openalex.org/W4312059229 | Bidirectional Recurrent Nets for ECG Signal Compression | 2022 | Journal of Computer Science Research | article | 4 | yes | Eman AL-Saidi, Khalil El Hindi | Computer science, Artificial intelligence, Convolutional neural network, Encoder, Data compression, Pattern recognition (psychology), +9 more | https://doi.org/10.30564/jcsr.v4i4.5204 | Electrocardiogram (ECG) is a commonly used tool in biological diagnosis of heart diseases. ECG allows the representation of electrical signals which cause heart muscles to contract and relax. Recently, accurate deep learning methods have been developed to overcome manual diagnosis in terms of time and effort. However, most of current automatic medical diagnosis use long electrocardiogram (ECG) signals to inspect different types of heart arrhythmia. Therefore, ECG signal files tend to require large storage to store and may cause significant overhead when exchanged over a computer network. This raises the need to come up with effective compression methods for ECG signals. In this work, the authors investigate using BERT (Bidirectional Encoder Representations from Transformers) model, which is a bidirectional neural network that was originally designed for natural language. The authors eva… |
| https://openalex.org/W3100107515 | Document Ranking with a Pretrained Sequence-to-Sequence Model | 2020 |  | article | 380 | yes | Rodrigo Nogueira, Zhiying Jiang, Ronak Pradeep, Jimmy Lin | Computer science, Ranking (information retrieval), Sequence (biology), Encoder, Relevance (law), Artificial intelligence, +15 more | https://doi.org/10.18653/v1/2020.findings-emnlp.63 | This work proposes the use of a pretrained sequence-to-sequence model for document ranking. Our approach is fundamentally different from a commonly adopted classification-based formulation based on encoder-only pretrained transformer architectures such as BERT. We show how a sequence-to-sequence model can be trained to generate relevance labels as “target tokens”, and how the underlying logits of these target tokens can be interpreted as relevance probabilities for ranking. Experimental results on the MS MARCO passage ranking task show that our ranking approach is superior to strong encoder-only models. On three other document retrieval test collections, we demonstrate a zero-shot transfer-based approach that outperforms previous state-of-the-art models requiring in-domain cross-validation. Furthermore, we find that our approach significantly outperforms an encoder-only architecture in… |
| https://openalex.org/W2965066169 | Show, Attend and Read: A Simple and Strong Baseline for Irregular Text Recognition | 2019 | Proceedings of the AAAI Conference on Artificial Intelligence | article | 430 | yes | Hui Li, Peng Wang, Chunhua Shen, Guyu Zhang | Computer science, Baseline (sea), Word (group theory), Encoder, Distortion (music), Code (set theory), +24 more | https://doi.org/10.1609/aaai.v33i01.33018610 | Recognizing irregular text in natural scene images is challenging due to the large variance in text appearance, such as curvature, orientation and distortion. Most existing approaches rely heavily on sophisticated model designs and/or extra fine-grained annotations, which, to some extent, increase the difficulty in algorithm implementation and data collection. In this work, we propose an easy-to-implement strong baseline for irregular scene text recognition, using offthe-shelf neural network components and only word-level annotations. It is composed of a 31-layer ResNet, an LSTMbased encoder-decoder framework and a 2-dimensional attention module. Despite its simplicity, the proposed method is robust. It achieves state-of-the-art performance on irregular text recognition benchmarks and comparable results on regular text datasets. The code will be released. |
| https://openalex.org/W4382202677 | TrOCR: Transformer-Based Optical Character Recognition with Pre-trained Models | 2023 | Proceedings of the AAAI Conference on Artificial Intelligence | article | 303 | yes | Minghao Li, Tengchao Lv, Jingye Chen, Lei Cui, Yijuan Lu, Dinei Florêncio, Cha Zhang, Zhoujun Li, +1 more | Transformer, Computer science, AKA, Artificial intelligence, Language model, Optical character recognition, +9 more | https://doi.org/10.1609/aaai.v37i11.26538 | Text recognition is a long-standing research problem for document digitalization. Existing approaches are usually built based on CNN for image understanding and RNN for char-level text generation. In addition, another language model is usually needed to improve the overall accuracy as a post-processing step. In this paper, we propose an end-to-end text recognition approach with pre-trained image Transformer and text Transformer models, namely TrOCR, which leverages the Transformer architecture for both image understanding and wordpiece-level text generation. The TrOCR model is simple but effective, and can be pre-trained with large-scale synthetic data and fine-tuned with human-labeled datasets. Experiments show that the TrOCR model outperforms the current state-of-the-art models on the printed, handwritten and scene text recognition tasks. The TrOCR models and code are publicly availab… |
| https://openalex.org/W3196650609 | Mapping single-cell data to reference atlases by transfer learning | 2021 | Nature Biotechnology | article | 579 | yes | Mohammad Lotfollahi, Mohsen Naghipourfar, Malte D. Luecken, M. Javad Khajavi, Maren Büttner, Marco Wagenstetter, Žiga Avsec, Adam Gayoso, +5 more | Computer science, Transfer of learning, Raw data, Imputation (statistics), Reference data, Data sharing, +10 more | https://doi.org/10.1038/s41587-021-01001-7 | Abstract Large single-cell atlases are now routinely generated to serve as references for analysis of smaller-scale studies. Yet learning from reference data is complicated by batch effects between datasets, limited availability of computational resources and sharing restrictions on raw data. Here we introduce a deep learning strategy for mapping query datasets on top of a reference called single-cell architectural surgery (scArches). scArches uses transfer learning and parameter optimization to enable efficient, decentralized, iterative reference building and contextualization of new datasets with existing references without sharing raw data. Using examples from mouse brain, pancreas, immune and whole-organism atlases, we show that scArches preserves biological state information while removing batch effects, despite using four orders of magnitude fewer parameters than de novo integrati… |
| https://openalex.org/W4403397986 | LLM-MRI Python module: a brain scanner for LLMs | 2024 |  | article | 2 | yes | Luiz Pereira da Costa, Mateus R. Figênio, André Santanchè, Luiz Gomes-Jr | Python (programming language), Scanner, Computer science, Programming language, Artificial intelligence | https://doi.org/10.5753/sbbd_estendido.2024.243136 | LLMs (Large Language Models) have demonstrated human-level language and knowledge acquisition skills in several tasks. However, despite the recent success and broad use, understanding how these skills are learned and encoded inside the underlying neural network is still challenging. The goal of the LLM-MRI package is to simplify the study of activation patterns in any transformer-based LLM, similarly to how MRI (magnetic resonance imaging) simplifies with biological brains. The package, written for the Python language, allows the mapping of neural regions using a parameterized reduction of the model's dimensionality. Neural regions can be visualized according to the forward-pass activations stimulated by a set of documents. Similarly, the package enables the creation of graph models representing the interlayer network of connections stimulated by a set of documents. These features allow… |
| https://openalex.org/W4404040610 | Enhancing Arabic Sentiment Analysis of Consumer Reviews: Machine Learning and Deep Learning Methods Based on NLP | 2024 | Algorithms | article | 4 | yes | Hani Almaqtari, Feng Zeng, Ammar Mohammed | Artificial intelligence, Arabic, Computer science, Natural language processing, Sentiment analysis, Deep learning, +3 more | https://doi.org/10.3390/a17110495 | Sentiment analysis utilizes Natural Language Processing (NLP) techniques to extract opinions from text, which is critical for businesses looking to refine strategies and better understand customer feedback. Understanding people’s sentiments about products through emotional tone analysis is paramount. However, analyzing sentiment in Arabic and its dialects poses challenges due to the language’s intricate morphology, right-to-left script, and nuanced emotional expressions. To address this, this study introduces the Arb-MCNN-Bi Model, which integrates the strengths of the transformer-based AraBERT (Arabic Bidirectional Encoder Representations from Transformers) model with a Multi-channel Convolutional Neural Network (MCNN) and a Bidirectional Gated Recurrent Unit (BiGRU) for Arabic sentiment analysis. AraBERT, designed specifically for Arabic, captures rich contextual information through w… |
| https://openalex.org/W3111174583 | Transformer protein language models are unsupervised structure learners | 2020 |  | preprint | 327 | yes | Roshan Rao, Joshua Meier, Tom Sercu, Sergey Ovchinnikov, Alexander Rives | Transformer, Unsupervised learning, Computer science, Artificial intelligence, Language model, Machine learning, +10 more | https://doi.org/10.1101/2020.12.15.422761 | A bstract Unsupervised contact prediction is central to uncovering physical, structural, and functional constraints for protein structure determination and design. For decades, the predominant approach has been to infer evolutionary constraints from a set of related sequences. In the past year, protein language models have emerged as a potential alternative, but performance has fallen short of state-of-the-art approaches in bioinformatics. In this paper we demonstrate that Transformer attention maps learn contacts from the unsupervised language modeling objective. We find the highest capacity models that have been trained to date already outperform a state-of-the-art unsupervised contact prediction pipeline, suggesting these pipelines can be replaced with a single forward pass of an end-to-end model. 1 |
| https://openalex.org/W4401889742 | Recurrent Neural Networks: A Comprehensive Review of Architectures, Variants, and Applications | 2024 | Information | review | 330 | yes | Ibomoiye Domor Mienye, Theo G. Swart, George Obaido | Recurrent neural network, Computer science, Artificial intelligence, Transformer, Deep learning, Long short term memory, +5 more | https://doi.org/10.3390/info15090517 | Recurrent neural networks (RNNs) have significantly advanced the field of machine learning (ML) by enabling the effective processing of sequential data. This paper provides a comprehensive review of RNNs and their applications, highlighting advancements in architectures, such as long short-term memory (LSTM) networks, gated recurrent units (GRUs), bidirectional LSTM (BiLSTM), echo state networks (ESNs), peephole LSTM, and stacked LSTM. The study examines the application of RNNs to different domains, including natural language processing (NLP), speech recognition, time series forecasting, autonomous vehicles, and anomaly detection. Additionally, the study discusses recent innovations, such as the integration of attention mechanisms and the development of hybrid models that combine RNNs with convolutional neural networks (CNNs) and transformer architectures. This review aims to provide ML… |
| https://openalex.org/W4404088824 | Deep Learning and Machine Learning -- Object Detection and Semantic Segmentation: From Theory to Applications | 2024 | arXiv (Cornell University) | preprint | 3 | yes | Jin‐Tao Ren, Ziqian Bi, Qian Niu, Junyu Liu, Benji Peng, Sen Zhang, Xiaoyong Pan, Jinlang Wang, +11 more | Artificial intelligence, Segmentation, Computer science, Deep learning, Machine learning, Object detection, +3 more | http://arxiv.org/abs/2410.15584 | An in-depth exploration of object detection and semantic segmentation is provided, combining theoretical foundations with practical applications. State-of-the-art advancements in machine learning and deep learning are reviewed, focusing on convolutional neural networks (CNNs), YOLO architectures, and transformer-based approaches such as DETR. The integration of artificial intelligence (AI) techniques and large language models for enhancing object detection in complex environments is examined. Additionally, a comprehensive analysis of big data processing is presented, with emphasis on model optimization and performance evaluation metrics. By bridging the gap between traditional methods and modern deep learning frameworks, valuable insights are offered for researchers, data scientists, and engineers aiming to apply AI-driven methodologies to large-scale object detection tasks. |
| https://openalex.org/W4313413766 | Robust Data Augmentation for Neural Machine Translation through EVALNET | 2022 | Mathematics | article | 4 | yes | Yo-Han Park, Yong‐Seok Choi, Seung Yun, Sanghun Kim, Kong-Joo Lee | Computer science, Machine translation, Sentence, Artificial intelligence, Artificial neural network, Transformer, +11 more | https://doi.org/10.3390/math11010123 | Since building Neural Machine Translation (NMT) systems requires a large parallel corpus, various data augmentation techniques have been adopted, especially for low-resource languages. In order to achieve the best performance through data augmentation, the NMT systems should be able to evaluate the quality of augmented data. Several studies have addressed data weighting techniques to assess data quality. The basic idea of data weighting adopted in previous studies is the loss value that a system calculates when learning from training data. The weight derived from the loss value of the data, through simple heuristic rules or neural models, can adjust the loss used in the next step of the learning process. In this study, we propose EvalNet, a data evaluation network, to assess parallel data of NMT. EvalNet exploits a loss value, a cross-attention map, and a semantic similarity between par… |
| https://openalex.org/W4385567149 | Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? | 2022 |  | article | 582 | yes | Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Michael Lewis, Hannaneh Hajishirzi, Luke Zettlemoyer | Computer science, Inference, Task (project management), Context (archaeology), Key (lock), Artificial intelligence, +15 more | https://doi.org/10.18653/v1/2022.emnlp-main.759 | Large language models (LMs) are able to in-context learn—perform a new task via inference alone by conditioning on a few input-label pairs (demonstrations) and making predictions for new inputs. However, there has been little understanding of how the model learns and which aspects of the demonstrations contribute to end task performance. In this paper, we show that ground truth demonstrations are in fact not required—randomly replacing labels in the demonstrations barely hurts performance on a range of classification and multi-choce tasks, consistently over 12 different models including GPT-3. Instead, we find that other aspects of the demonstrations are the key drivers of endtask performance, including the fact that they provide a few examples of (1) the label space, (2) the distribution of the input text, and (3) the overall format of the sequence. Together, our analysis provides a ne… |
| https://openalex.org/W4380353786 | Customizing General-Purpose Foundation Models for Medical Report Generation | 2023 | arXiv (Cornell University) | preprint | 3 | yes | Bang Yang, Asif Raza, Yuexian Zou, Tong Zhang | Computer science, Deep learning, Artificial intelligence, Transformer, Machine learning, Encoder, +6 more | http://arxiv.org/abs/2306.05642 | Medical caption prediction which can be regarded as a task of medical report generation (MRG), requires the automatic generation of coherent and accurate captions for the given medical images. However, the scarcity of labelled medical image-report pairs presents great challenges in the development of deep and large-scale neural networks capable of harnessing the potential artificial general intelligence power like large language models (LLMs). In this work, we propose customizing off-the-shelf general-purpose large-scale pre-trained models, i.e., foundation models (FMs), in computer vision and natural language processing with a specific focus on medical report generation. Specifically, following BLIP-2, a state-of-the-art vision-language pre-training approach, we introduce our encoder-decoder-based MRG model. This model utilizes a lightweight query Transformer to connect two FMs: the gi… |
| https://openalex.org/W4213025374 | Transformer Network for Remaining Useful Life Prediction of Lithium-Ion Batteries | 2022 | IEEE Access | article | 327 | yes | Daoquan Chen, Weicong Hong, Xiuze Zhou | Transformer, Computer science, Battery capacity, Noise reduction, Encoder, Artificial neural network, +11 more | https://doi.org/10.1109/access.2022.3151975 | Accurately predicting the Remaining Useful Life (RUL) of a Li-ion battery plays an important role in managing the health and estimating the state of a battery. With the rapid development of electric vehicles, there is an increasing need to develop and improve the techniques for predicting RUL. To predict RUL, we designed a Transformer-based neural network. First, battery capacity data is always full of noise, especially during battery charge/discharge regeneration. To alleviate this problem, we applied a Denoising Auto-Encoder (DAE) to process raw data. Then, to capture temporal information and learn useful features, a reconstructed sequence was fed into a Transformer network. Finally, to bridge denoising and prediction tasks, we combined these two tasks into a unified framework. Results of extensive experiments conducted on two data sets and a comparison with some existing methods show… |
| https://openalex.org/W4387870893 | Phydi: Initializing Parameterized Hypercomplex Neural Networks As Identity Functions | 2023 | IRIS Research product catalog (Sapienza University of Rome) | article | 3 | yes | Matteo Mancanelli, Eleonora Grassucci, Aurelio Uncini, Danilo Comminiello | Hypercomplex number, Parameterized complexity, Initialization, Computer science, Artificial neural network, Convergence (economics), +14 more | https://hdl.handle.net/11573/1693477 | Neural models based on hypercomplex algebra systems are growing and prolificating for a plethora of applications, ranging from computer vision to natural language processing. Hand in hand with their adoption, parameterized hypercomplex neural networks (PHNNs) are growing in size and no techniques have been adopted so far to control their convergence at a large scale. In this paper, we study PHNNs convergence and propose parameterized hypercomplex identity initialization (PHYDI), a method to improve their convergence at different scales, leading to more robust performance when the number of layers scales up, while also reaching the same performance with fewer iterations. We show the effectiveness of this approach in different benchmarks and with common PHNNs with ResNets- and Transformer-based architecture. The code is available at https://github.com/ispamm/PHYDI. |
| https://openalex.org/W4385567070 | Improved Evaluation of Automatic Source Code Summarisation | 2022 |  | article | 2 | yes | Jesse Phillips, David Bowes, Mahmoud El‐Haj, Tracy Hall | Computer science, Source code, Preprocessor, Natural language processing, Artificial intelligence, Language model, +17 more | http://dx.doi.org/10.18653/v1/2022.gem-1.28 | Source code summaries are a vital tool for the understanding and maintenance of source code as they can be used to explain code in simple terms. However, source code with missing, incorrect, or outdated summaries is a common occurrence in production code. Automatic source code summarisation seeks to solve these issues by generating up-to-date summaries of source code methods. Recent work in automatically generating source code summaries uses neural networks for generating summaries; commonly Sequence-to-Sequence or Transformer models, pretrained on method-summary pairs. The most common method of evaluating the quality of these summaries is comparing the machine-generated summaries against human-written summaries. Summaries can be evaluated using n-gram-based translation metrics such as BLEU, METEOR, or ROUGE-L. However, these metrics alone can be unreliable and new Natural Language Gene… |
| https://openalex.org/W4394828156 | ChatGPT for Robotics: Design Principles and Model Abilities | 2024 | IEEE Access | article | 371 | yes | Sai Vemprala, Rogerio Bonatti, Arthur Bucker, Ashish Kapoor | Robotics, Artificial intelligence, Computer science, Dialog box, Human–computer interaction, Robot, +7 more | https://doi.org/10.1109/access.2024.3387941 | This paper presents an experimental study regarding the use of OpenAI&#x2019;s ChatGPT for robotics applications. We outline a strategy that combines design principles for prompt engineering and the creation of a high-level function library which allows ChatGPT to adapt to different robotics tasks, simulators, and form factors. We focus our evaluations on the effectiveness of different prompt engineering techniques and dialog strategies towards the execution of various types of robotics tasks. We explore ChatGPT&#x2019;s ability to use free-form dialog, parse XML tags, and to synthesize code, in addition to the use of task-specific prompting functions and closed-loop reasoning through dialogues. Our study encompasses a range of tasks within the robotics domain, from basic logical, geometrical, and mathematical reasoning all the way to complex domains such as aerial navigation, manipulat… |
| https://openalex.org/W4387912975 | A Comparison of Grammatical Error Correction Models in English Writing | 2023 |  | article | 3 | no | Kadir Eker, Meltem Kurt Pehli̇vanoğlu, Ayşe Gül Eker, Muhammad Abdan Syakura, N. Jeremi Duru | Computer science, Spelling, Natural language processing, Artificial intelligence, Error detection and correction, Task (project management), +16 more | https://doi.org/10.1109/ubmk59864.2023.10286642 | Grammatical error correction (GEC) is the task of correcting grammatical, spelling, and word usage errors in texts. Detecting and correcting these errors makes it difficult for the reader to understand the text and negatively affects communication. Editors and proofreaders can identify and correct grammatical errors while reading texts. However, this manual correction takes a long time. With the development of technology, artificial intelligence-supported tools have been developed that automatically correct errors. In artificial intelligence-supported systems, this task is usually performed using natural language processing methods. In this study, a traditional neural network architecture (Attention BiLSTM) and a transformer-based language model (T5) were trained. Then, the success of these trained models and Large Language Models (LLMs) (ChatGPT, Google Bard, LanguageTool) in English g… |
| https://openalex.org/W4211131435 | Dependency-aware Form Understanding | 2021 |  | article | 4 | no | Shaokun Zhang, Yuanchun Li, Weixiang Yan, Yao Guo, Xiangqun Chen | Computer science, Artificial intelligence, Embedding, Transformer, Dependency (UML), Machine learning, +12 more | https://doi.org/10.1109/issre52982.2021.00026 | Form understanding is an important task in many fields such as software testing, AI assistants, and improving accessibility. One key goal of understanding a complex set of forms is to identify the dependencies between form elements. However, it remains a challenge to capture the dependencies accurately due to the diversity of UI design patterns and the variety in development experiences. In this paper, we propose a deep-learning-based approach called DependEX, which integrates convolutional neural networks (CNNs) and transformers to help understand dependencies within forms. DependEX extracts semantic features from UI images using CNN-based models, captures contextual patterns using a multilayer transformer encoder module, and models dependencies between form elements using two embedding layers. We evaluate DependEX with a large-scale dataset from mobile Web applications. Experimental r… |
| https://openalex.org/W3094233850 | Complaint Identification in Social Media with Transformer Networks | 2020 | arXiv (Cornell University) | preprint | 2 | yes | Mali Jin, Νικόλαος Αλέτρας | Complaint, Computer science, Transformer, Social media, Artificial neural network, Sentiment analysis, +11 more | http://arxiv.org/abs/2010.10910 | Complaining is a speech act extensively used by humans to communicate a negative inconsistency between reality and expectations. Previous work on automatically identifying complaints in social media has focused on using feature-based and task-specific neural network models. Adapting state-of-the-art pre-trained neural language models and their combinations with other linguistic information from topics or sentiment for complaint prediction has yet to be explored. In this paper, we evaluate a battery of neural models underpinned by transformer networks which we subsequently combine with linguistic information. Experiments on a publicly available data set of complaints demonstrate that our models outperform previous state-of-the-art methods by a large margin achieving a macro F1 up to 87. |
| https://openalex.org/W3128776197 | Spatial-Spectral Transformer for Hyperspectral Image Classification | 2021 | Remote Sensing | article | 396 | yes | Xin He, Yushi Chen, Zhouhan Lin | Overfitting, Computer science, Hyperspectral imaging, Pattern recognition (psychology), Artificial intelligence, Convolutional neural network, +6 more | https://doi.org/10.3390/rs13030498 | Recently, a great many deep convolutional neural network (CNN)-based methods have been proposed for hyperspectral image (HSI) classification. Although the proposed CNN-based methods have the advantages of spatial feature extraction, they are difficult to handle the sequential data with and CNNs are not good at modeling the long-range dependencies. However, the spectra of HSI are a kind of sequential data, and HSI usually contains hundreds of bands. Therefore, it is difficult for CNNs to handle HSI processing well. On the other hand, the Transformer model, which is based on an attention mechanism, has proved its advantages in processing sequential data. To address the issue of capturing relationships of sequential spectra in HSI in a long distance, in this study, Transformer is investigated for HSI classification. Specifically, in this study, a new classification framework titled spatial… |
| https://openalex.org/W4220656870 | Open domain Conversational Model using transfer learning | 2022 | 2022 12th International Conference on Cloud Computing, Data Science &amp; Engin… | article | 2 | no | Mukund K Roy, Garima Aggarwal, Abhay Bansal, Deeksha Juneja | Computer science, Transformer, Fluency, Transfer of learning, Sentence, Natural language processing, +11 more | https://doi.org/10.1109/confluence52989.2022.9734155 | Conversational modeling is a complex task, comprising of natural language understanding and generating text as a response to the input given. In recent years, there has been the rise of a new set of conversational AI systems that are based on deep-learning neural networks architectures. With the advent of bigger and larger Transformer based Language Models trained on gigabytes of textual data and billions of parameters, the language generators are now able to generate answers, write articles and summarize texts with human-like fluency. In this paper, an open domain conversational agent that leverages transfer-learning based on OpenAI's GPT-2 transformer language model has been implemented and then fine-tunes it on the Reddit dataset to generate relevant and in-context responses. Experimental results and live talking with the system show that the sentence formation for the responses are… |
| https://openalex.org/W2923622379 | Competence-based Curriculum Learning for Neural Machine Translation | 2019 |  | article | 304 | yes | Emmanouil Antonios Platanios, Otilia Stretcu, Graham Neubig, Barnabás Póczos, Tom M. Mitchell | Computer science, Machine translation, Curriculum, Competence (human resources), Artificial intelligence, Cognitive science, +7 more | https://doi.org/10.18653/v1/n19-1119 | Emmanouil Antonios Platanios, Otilia Stretcu, Graham Neubig, Barnabas Poczos, Tom Mitchell. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 2019. |
| https://openalex.org/W4221166835 | Unified Structure Generation for Universal Information Extraction | 2022 | Proceedings of the 60th Annual Meeting of the Association for Computational Lin… | article | 362 | yes | Yaojie Lu, Q. Liu, Dai Dai, Xinyan Xiao, Hongyu Lin, Xianpei Han, Le Sun, Hua Wu | Computer science, Computational linguistics, Information extraction, Volume (thermodynamics), Natural language processing, Library science, +6 more | https://doi.org/10.18653/v1/2022.acl-long.395 | Yaojie Lu, Qing Liu, Dai Dai, Xinyan Xiao, Hongyu Lin, Xianpei Han, Le Sun, Hua Wu. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022. |
| https://openalex.org/W4387995257 | Codebook Features: Sparse and Discrete Interpretability for Neural Networks | 2023 | arXiv (Cornell University) | preprint | 2 | yes | Alex Tamkin, Mohammad Taufeeque, Noah D. Goodman | Codebook, Bottleneck, Interpretability, Computer science, Artificial neural network, Artificial intelligence, +6 more | http://arxiv.org/abs/2310.17230 | Understanding neural networks is challenging in part because of the dense, continuous nature of their hidden states. We explore whether we can train neural networks to have hidden states that are sparse, discrete, and more interpretable by quantizing their continuous features into what we call codebook features. Codebook features are produced by finetuning neural networks with vector quantization bottlenecks at each layer, producing a network whose hidden features are the sum of a small number of discrete vector codes chosen from a larger codebook. Surprisingly, we find that neural networks can operate under this extreme bottleneck with only modest degradation in performance. This sparse, discrete bottleneck also provides an intuitive way of controlling neural network behavior: first, find codes that activate when the desired behavior is present, then activate those same codes during ge… |
| https://openalex.org/W3113067059 | Skeleton-based action recognition via spatial and temporal transformer networks | 2021 | Computer Vision and Image Understanding | article | 356 | yes | Chiara Plizzari, Marco Cannici, Matteo Matteucci | Computer science, RGB color model, Artificial intelligence, Skeleton (computer programming), Pattern recognition (psychology), Spatial analysis, +9 more | https://doi.org/10.1016/j.cviu.2021.103219 |  |
| https://openalex.org/W4402813529 | Large‐language‐model empowered 3D dose prediction for intensity‐modulated radiotherapy | 2024 | Medical Physics | article | 9 | no | Zehao Dong, Yixin Chen, Hiram A. Gay, Yao Hao, Geoffrey D. Hugo, Pamela Samson, Tianyu Zhao | Computer science, Radiation treatment planning, Histogram, Radiation therapy, Artificial intelligence, Artificial neural network, +9 more | https://doi.org/10.1002/mp.17416 | Abstract Background Treatment planning is currently a patient specific, time‐consuming, and resource demanding task in radiotherapy. Dose‐volume histogram (DVH) prediction plays a critical role in automating this process. The geometric relationship between DVHs in radiotherapy plans and organs‐at‐risk (OAR) and planning target volume (PTV) has been well established. This study explores the potential of deep learning models for predicting DVHs using images and subsequent human intervention facilitated by a large‐language model (LLM) to enhance the planning quality. Method We propose a pipeline to convert unstructured images to a structured graph consisting of image‐patch nodes and dose nodes. A novel Dose Graph Neural Network (DoseGNN) model is developed for predicting DVHs from the structured graph. The proposed DoseGNN is enhanced with the LLM to encode massive knowledge from prescript… |
| https://openalex.org/W4385342469 | Convolutional Neural Networks: A Survey | 2023 | Computers | article | 475 | yes | Moez Krichen | Convolutional neural network, Computer science, Artificial intelligence, Deep learning, Preprocessor, Field (mathematics), +4 more | https://doi.org/10.3390/computers12080151 | Artificial intelligence (AI) has become a cornerstone of modern technology, revolutionizing industries from healthcare to finance. Convolutional neural networks (CNNs) are a subset of AI that have emerged as a powerful tool for various tasks including image recognition, speech recognition, natural language processing (NLP), and even in the field of genomics, where they have been utilized to classify DNA sequences. This paper provides a comprehensive overview of CNNs and their applications in image recognition tasks. It first introduces the fundamentals of CNNs, including the layers of CNNs, convolution operation (Conv_Op), Feat_Maps, activation functions (Activ_Func), and training methods. It then discusses several popular CNN architectures such as LeNet, AlexNet, VGG, ResNet, and InceptionNet, and compares their performance. It also examines when to use CNNs, their advantages and limit… |
| https://openalex.org/W4283392904 | Artificial intelligence and smart vision for building and construction 4.0: Machine and deep learning methods and applications | 2022 | Automation in Construction | article | 820 | yes | Shanaka Kristombu Baduge, Sadeep Thilakarathna, Jude Shalitha Perera, Mehrdad Arashpour, P. Sharafi, Bertrand Teodosio, Ankit Shringi, Priyan Mendis | Automation, Building automation, Systems engineering, Engineering, Artificial intelligence, Building information modeling, +10 more | https://doi.org/10.1016/j.autcon.2022.104440 | This article presents a state-of-the-art review of the applications of Artificial Intelligence (AI), Machine Learning (ML), and Deep Learning (DL) in building and construction industry 4.0 in the facets of architectural design and visualization; material design and optimization; structural design and analysis; offsite manufacturing and automation; construction management, progress monitoring, and safety; smart operation, building management and health monitoring; and durability, life cycle analysis, and circular economy. This paper presents a unique perspective on applications of AI/DL/ML in these domains for the complete building lifecycle, from conceptual stage, design stage, construction stage, operational and maintenance stage until the end of life. Furthermore, data collection strategies using smart vision and sensors, data cleaning methods (post-processing), data storage for devel… |
| https://openalex.org/W3126334425 | Adaptive Semiparametric Language Models | 2021 | arXiv (Cornell University) | preprint | 2 | yes | Dani Yogatama, Cyprien de Masson d’Autume, Lingpeng Kong | Computer science, Artificial intelligence, Language model, Parametric statistics, Transformer, Term (time), +7 more | http://arxiv.org/abs/2102.02557 | We present a language model that combines a large parametric neural network (i.e., a transformer) with a non-parametric episodic memory component in an integrated architecture. Our model uses extended short-term context by caching local hidden states -- similar to transformer-XL -- and global long-term memory by retrieving a set of nearest neighbor tokens at each timestep. We design a gating function to adaptively combine multiple information sources to make a prediction. This mechanism allows the model to use either local context, short-term memory, or long-term memory (or any combination of them) on an ad hoc basis depending on the context. Experiments on word-based and character-based language modeling datasets demonstrate the efficacy of our proposed method compared to strong baselines. |
| https://openalex.org/W4387698226 | CLIP-based Model for Effective and Explainable Apparent Personality Perception | 2023 |  | article | 6 | no | Peter Zhuowei Gan, Arcot Sowmya, Gelareh Mohammadi | Computer science, Artificial intelligence, Context (archaeology), Perception, Field (mathematics), Feature (linguistics), +9 more | https://doi.org/10.1145/3607865.3613178 | In the field of Apparent Personality Perception (APP), a central challenge involves drawing robust inferences from observable behaviour and appearance. Various existing methods attempt to accomplish this by deploying diverse neural network architectures, all aimed at extracting and integrating features from multimodal data sources such as text, audio, and video. Notwithstanding, these methods grapple with issues related to generalisability and explainability, which hamper their applicability and trustworthiness in real-world situations. Responding to this challenge, our paper presents a novel approach to APP that capitalizes on the unique strengths of CLIP (Contrastive Language-Image Pre-Training), a large-scale multimodal pre-training method. CLIP learns image and text representations via natural language supervision and demonstrates a remarkable ability to handle a range of vision-rel… |
| https://openalex.org/W4385572634 | Self-Instruct: Aligning Language Models with Self-Generated Instructions | 2023 |  | article | 492 | yes | Yi‐Zhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, Hannaneh Hajishirzi | Computer science, Computational linguistics, Programming language, Volume (thermodynamics), Natural language processing, Linguistics, +4 more | https://doi.org/10.18653/v1/2023.acl-long.754 | Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, Hannaneh Hajishirzi. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2023. |
| https://openalex.org/W2970925270 | Simple, Scalable Adaptation for Neural Machine Translation | 2019 |  | article | 301 | yes | Ankur Bapna, Orhan Fırat | Computer science, Simple (philosophy), Machine translation, Adaptation (eye), Scalability, Translation (biology), +10 more | https://doi.org/10.18653/v1/d19-1165 | Ankur Bapna, Orhan Firat. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019. |
| https://openalex.org/W4393170111 | Enhancing image captioning performance based on efficientnet B0 model and transformer encoder-decoder | 2024 | AIP conference proceedings | article | 2 | yes | Abhisht Joshi, Ahmed Alkhayyat, Harsh Gunwant, Abhay Tripathi, Moolchand Sharma | Closed captioning, Encoder, Computer science, Transformer, Image (mathematics), Artificial intelligence, +5 more | http://dx.doi.org/10.1063/5.0184395 | In recent years, improvements in natural language processing and computer vision have come together to provide automatic image caption generation. Image captioning is the process of creating a description for an image. Captioning an image needs the recognition of significant items, their properties, and their connections within the image. Additionally, it must create phrases that are syntactically and semantically accurate. Deep learning approaches can address the complexities and difficulties associated with image captions. This paper describes a joint model which is capable of automatically captioning images using EfficientNet-B0 and a transformer with multi-head attention. The model is an aggregation of an EfficientNet & Transformer single encoder and decoder. The encoder utilizes EfficientNet-B0, a convolutional neural network-based algorithm that generates a detailed input image, r… |
| https://openalex.org/W2998230451 | Semantics-Aware BERT for Language Understanding | 2020 | Proceedings of the AAAI Conference on Artificial Intelligence | article | 344 | yes | Zhuosheng Zhang, Yuwei Wu, Hai Zhao, Zuchao Li, Shuailiang Zhang, Xi Zhou, Xiang Zhou | Computer science, Natural language processing, Semantics (computer science), Natural language understanding, Artificial intelligence, Inference, +9 more | https://doi.org/10.1609/aaai.v34i05.6510 | The latest work on language representations carefully integrates contextualized features into language model training, which enables a series of success especially in various machine reading comprehension and natural language inference tasks. However, the existing language representation models including ELMo, GPT and BERT only exploit plain context-sensitive features such as character or word embeddings. They rarely consider incorporating structured semantic information which can provide rich semantics for language representation. To promote natural language understanding, we propose to incorporate explicit contextual semantics from pre-trained semantic role labeling, and introduce an improved language representation model, Semantics-aware BERT (SemBERT), which is capable of explicitly absorbing contextual semantics over a BERT backbone. SemBERT keeps the convenient usability of its BE… |
| https://openalex.org/W3099919888 | IndicNLPSuite: Monolingual Corpora, Evaluation Benchmarks and Pre-trained Multilingual Language Models for Indian Languages | 2020 |  | article | 376 | yes | Divyanshu Kakwani, Anoop Kunchukuttan, Satish Golla, Gokul Narayanan C, Avik Bhattacharyya, Mitesh M. Khapra, Pratyush Kumar | Computer science, Natural language processing, Artificial intelligence, Benchmark (surveying), Paraphrase, Headline, +7 more | https://doi.org/10.18653/v1/2020.findings-emnlp.445 | In this paper, we introduce NLP resources for 11 major Indian languages from two major language families. These resources include: (a) large-scale sentence-level monolingual corpora, (b) pre-trained word embeddings, (c) pre-trained language models, and (d) multiple NLU evaluation datasets (IndicGLUE benchmark). The monolingual corpora contains a total of 8.8 billion tokens across all 11 languages and Indian English, primarily sourced from news crawls. The word embeddings are based on FastText, hence suitable for handling morphological complexity of Indian languages. The pre-trained language models are based on the compact ALBERT model. Lastly, we compile the (IndicGLUE benchmark for Indian language NLU. To this end, we create datasets for the following tasks: Article Genre Classification, Headline Prediction, Wikipedia Section-Title Prediction, Cloze-style Multiple choice QA, Winograd N… |
| https://openalex.org/W3043424630 | Evaluation of Sentiment Analysis in Finance: From Lexicons to Transformers | 2020 | IEEE Access | article | 290 | yes | Kostadin Mishev, Ana Gjorgjevikj, Irena Vodenska, Lou Chitkushev, Dimitar Trajanov | Computer science, Sentiment analysis, Unavailability, Artificial intelligence, Sentence, Transformer, +16 more | https://doi.org/10.1109/access.2020.3009626 | Financial and economic news is continuously monitored by financial market participants. According to the efficient market hypothesis, all past information is reflected in stock prices and new information is instantaneously absorbed in determining future stock prices. Hence, prompt extraction of positive or negative sentiments from news is very important for investment decision-making by traders, portfolio managers and investors. Sentiment analysis models can provide an efficient method for extracting actionable signals from the news. However, financial sentiment analysis is challenging due to domain-specific language and unavailability of large labeled datasets. General sentiment analysis models are ineffective when applied to specific domains such as finance. To overcome these challenges, we design an evaluation platform which we use to assess the effectiveness and performance of vario… |
| https://openalex.org/W4406837844 | A comprehensive overview of LLM-based approaches for machine translation | 2025 | Indonesian Journal of Electrical Engineering and Computer Science | article | 2 | yes | Bhuvaneswari Kumar, Vadivel Murugesan | Translation (biology), Computer science, Machine translation, Artificial intelligence, Chemistry, Messenger RNA, +2 more | https://doi.org/10.11591/ijeecs.v38.i1.pp344-356 | Statistical machine translation (SMT) used parallel corpora and statistical models, to identify translation patterns and probabilities. Although this method had advantages, it had trouble with idiomatic expressions, context-specific subtleties, and intricate linguistic structures. The subsequent introduction of deep neural networks such as recurrent neural networks (RNNs), long short-term memory (LSTMs), transformers with attention mechanisms, and the emergence of large language model (LLM) frameworks has marked a paradigm shift in machine translation in recent years and has entirely replaced the traditional statistical approaches. The LLMs are able to capture complex language patterns, semantics, and context because they have been trained on enormous volumes of text data. Our study summarizes the most significant contributions in the literature related to LLM prompting, fine-tuning, re… |
| https://openalex.org/W4380319827 | Regulating ChatGPT and other Large Generative AI Models | 2023 |  | article | 377 | yes | Philipp Hacker, Andreas Engel, Marco Mauer | Transparency (behavior), Generative grammar, Computer science, Terminology, Generative model, Value (mathematics), +9 more | https://doi.org/10.1145/3593013.3594067 | Large generative AI models (LGAIMs), such as ChatGPT, GPT-4 or Stable Diffusion, are rapidly transforming the way we communicate, illustrate, and create. However, AI regulation, in the EU and beyond, has primarily focused on conventional AI models, not LGAIMs. This paper will situate these new generative models in the current debate on trustworthy AI regulation, and ask how the law can be tailored to their capabilities. After laying technical foundations, the legal part of the paper proceeds in four steps, covering (1) direct regulation, (2) data protection, (3) content moderation, and (4) policy proposals. It suggests a novel terminology to capture the AI value chain in LGAIM settings by differentiating between LGAIM developers, deployers, professional and non-professional users, as well as recipients of LGAIM output. We tailor regulatory duties to these different actors along the valu… |
| https://openalex.org/W4408693799 | An interpretable deep geometric learning model to predict the effects of mutations on protein–protein interactions using large-scale protein language model | 2025 | Journal of Cheminformatics | article | 3 | yes | Caiya Zhang, Yanzhi Sun, Pingzhao Hu | Computer science, Scale (ratio), Artificial intelligence, Machine learning, Quantum mechanics, Physics | https://doi.org/10.1186/s13321-025-00979-5 | Protein-protein interactions (PPIs) are central to the mechanisms of signaling pathways and immune responses, which can help us understand disease etiology. Therefore, there is a significant need for efficient and rapid automated approaches to predict changes in PPIs. In recent years, there has been a significant increase in applying deep learning techniques to predict changes in binding affinity between the original protein complex and its mutant variants. Particularly, the adoption of graph neural networks (GNNs) has gained prominence for their ability to learn representations of protein-protein complexes. However, the conventional GNNs have mainly concentrated on capturing local features, often disregarding the interactions among distant elements that hold potential important information. In this study, we have developed a transformer-based graph neural network to extract features of… |
| https://openalex.org/W4310998817 | Group Generalized Mean Pooling for Vision Transformer | 2022 | arXiv (Cornell University) | preprint | 5 | yes | Byungsoo Ko, Han‐Gyu Kim, Byeongho Heo, Sangdoo Yun, Sanghyuk Chun, Geonmo Gu, Wonjae Kim | Pooling, Computer science, Transformer, Convolutional neural network, Artificial intelligence, Machine learning, +5 more | http://arxiv.org/abs/2212.04114 | Vision Transformer (ViT) extracts the final representation from either class token or an average of all patch tokens, following the architecture of Transformer in Natural Language Processing (NLP) or Convolutional Neural Networks (CNNs) in computer vision. However, studies for the best way of aggregating the patch tokens are still limited to average pooling, while widely-used pooling strategies, such as max and GeM pooling, can be considered. Despite their effectiveness, the existing pooling strategies do not consider the architecture of ViT and the channel-wise difference in the activation maps, aggregating the crucial and trivial channels with the same importance. In this paper, we present Group Generalized Mean (GGeM) pooling as a simple yet powerful pooling strategy for ViT. GGeM divides the channels into groups and computes GeM pooling with a shared pooling parameter per group. As… |
| https://openalex.org/W4285606469 | A benchmark dataset for Turkish data-to-text generation | 2022 | Computer Speech & Language | article | 3 | no | Şeniz Demir, Seza Oktem | Computer science, Turkish, Natural language processing, Sentence, Artificial intelligence, Benchmarking, +13 more | https://doi.org/10.1016/j.csl.2022.101433 |  |
| https://openalex.org/W4379769651 | Health system-scale language models are all-purpose prediction engines | 2023 | Nature | article | 396 | yes | Lavender Yao Jiang, Xujin Chris Liu, Nima Pour Nejatian, Mustafa Nasir-Moin, Duo Wang, Anas Z. Abidin, Kevin Eaton, Howard A. Riina, +20 more | Generalizability theory, Software deployment, Computer science, Machine learning, Predictive modelling, Artificial intelligence, +13 more | https://doi.org/10.1038/s41586-023-06160-y |  |
| https://openalex.org/W3202485818 | Stock Market Prediction with New Generation Deep Contextualized Word Representations and Deep Learning Models using User Sentiments | 2021 | 2021 International Conference on INnovations in Intelligent SysTems and Applica… | article | 3 | no | Derya Othan, Zeynep Hilal Kilimci | Computer science, Word2vec, Deep learning, Word embedding, Artificial intelligence, Embedding, +16 more | https://doi.org/10.1109/inista52262.2021.9548419 | Stocks give significant clues to the investors, analysts, and researchers for the purpose of predicting the direction of stock market. In this work, we propose financial sentiment analysis to forecast the direction of Borsa Istanbul (BIST100) by investigating user comments from Twitter about high volume stocks in BIST100. For this purpose, Word2Vec, GloVe, and FastText as word embedding models, Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), and Long Short-Term Memory Networks (LSTMs) as deep learning algorithms, Bidirectional Encoder Representations from Transformers (BERT), Embeddings from Language Models (ELMo), and Universal Language Model Fine-tuning (ULMFiT) as new generation deep contextualized word embedding models are employed for the classification task. To the best of our knowledge, this is the first study to predict the direction of stock market BIST1… |
| https://openalex.org/W2946567085 | Adaptive Attention Span in Transformers | 2019 |  | article | 283 | yes | Sainbayar Sukhbaatar, Édouard Grave, Piotr Bojanowski, Armand Joulin | Computer science, Transformer, Memory footprint, Context model, Span (engineering), Artificial intelligence, +6 more | https://doi.org/10.18653/v1/p19-1032 | We propose a novel self-attention mechanism that can learn its optimal attention span. This allows us to extend significantly the maximum context size used in Transformer, while maintaining control over their memory footprint and computational time. We show the effectiveness of our approach on the task of character level language modeling, where we achieve state-of-the-art performances on text8 and enwiki8 by using a maximum context of 8k characters. |
| https://openalex.org/W3104748221 | Scalable Zero-shot Entity Linking with Dense Entity Retrieval | 2020 |  | article | 325 | yes | Ledell Wu, Fabio Petroni, Martin Josifoski, Sebastian Riedel, Luke Zettlemoyer | Computer science, Encoder, Scalability, Context (archaeology), Code (set theory), Information retrieval, +13 more | https://doi.org/10.18653/v1/2020.emnlp-main.519 | This paper introduces a conceptually simple, scalable, and highly effective BERT-based entity linking model, along with an extensive evaluation of its accuracy-speed trade-off. We present a two-stage zero-shot linking algorithm, where each entity is defined only by a short textual description. The first stage does retrieval in a dense space defined by a bi-encoder that independently embeds the mention context and the entity descriptions. Each candidate is then re-ranked with a cross-encoder, that concatenates the mention and entity text. Experiments demonstrate that this approach is state of the art on recent zero-shot benchmarks (6 point absolute gains) and also on more established non-zero-shot evaluations (e.g. TACKBP-2010), despite its relative simplicity (e.g. no explicit entity embeddings or manually engineered mention tables). We also show that bi-encoder linking is very fast wit… |
| https://openalex.org/W3011650341 | Rethinking Batch Normalization in Transformers | 2020 | arXiv (Cornell University) | article | 5 | yes | Sheng Shen, Zhewei Yao, Amir Gholami, Michael W. Mahoney, Kurt Keutzer | Normalization (sociology), Computer science, Transformer, Backpropagation, Artificial intelligence, Artificial neural network, +8 more | https://arxiv.org/abs/2003.07845v1 | The standard normalization method for neural network (NN) models used in Natural Language Processing (NLP) is layer normalization (LN). This is different than batch normalization (BN), which is widely-adopted in Computer Vision. The preferred use of LN in NLP is principally due to the empirical observation that a (naive/vanilla) use of BN leads to significant performance degradation for NLP tasks; however, a thorough understanding of the underlying reasons for this is not always evident. In this paper, we perform a systematic study of NLP transformer models to understand why BN has a poor performance, as compared to LN. We find that the statistics of NLP data across the batch dimension exhibit large fluctuations throughout training. This results in instability, if BN is naively implemented. To address this, we propose Power Normalization (PN), a novel normalization scheme that resolves… |
| https://openalex.org/W4225843081 | ScaLA: Accelerating Adaptation of Pre-Trained Transformer-Based Language Models via Efficient Large-Batch Adversarial Noise | 2022 | arXiv (Cornell University) | preprint | 2 | yes | Minjia Zhang, Niranjan Uma Naresh, Yuxiong He | Computer science, Transformer, Scala, Language model, Adversarial system, Speedup, +9 more | http://arxiv.org/abs/2201.12469 | In recent years, large pre-trained Transformer-based language models have led to dramatic improvements in many natural language understanding tasks. To train these models with increasing sizes, many neural network practitioners attempt to increase the batch sizes in order to leverage multiple GPUs to improve training speed. However, increasing the batch size often makes the optimization more difficult, leading to slow convergence or poor generalization that can require orders of magnitude more training time to achieve the same model quality. In this paper, we explore the steepness of the loss landscape of large-batch optimization for adapting pre-trained Transformer-based language models to domain-specific tasks and find that it tends to be highly complex and irregular, posing challenges to generalization on downstream tasks. To tackle this challenge, we propose ScaLA, a novel and effic… |
| https://openalex.org/W4318983406 | Building a knowledge graph to enable precision medicine | 2023 | Scientific Data | article | 364 | yes | Payal Chandak, Kexin Huang, Marinka Žitnik | Disease, Precision medicine, Computer science, Personalized medicine, Clinical phenotype, Data science, +12 more | https://doi.org/10.1038/s41597-023-01960-3 |  |
| https://openalex.org/W4386346036 | SpikeBERT: A Language Spikformer Learned from BERT with Knowledge Distillation | 2023 | arXiv (Cornell University) | preprint | 2 | yes | Changze Lv, Tianlong Li, Jianhan Xu, Chenxi Gu, Zixuan Ling, Cenyuan Zhang, Xiaoqing Zheng, Xuanjing Huang | Computer science, Transformer, Distillation, Artificial intelligence, Artificial neural network, Machine learning, +14 more | http://arxiv.org/abs/2308.15122 | Spiking neural networks (SNNs) offer a promising avenue to implement deep neural networks in a more energy-efficient way. However, the network architectures of existing SNNs for language tasks are still simplistic and relatively shallow, and deep architectures have not been fully explored, resulting in a significant performance gap compared to mainstream transformer-based networks such as BERT. To this end, we improve a recently-proposed spiking Transformer (i.e., Spikformer) to make it possible to process language tasks and propose a two-stage knowledge distillation method for training it, which combines pre-training by distilling knowledge from BERT with a large collection of unlabelled texts and fine-tuning with task-specific instances via knowledge distillation again from the BERT fine-tuned on the same training examples. Through extensive experimentation, we show that the models tr… |
| https://openalex.org/W3017243633 | Sample-Efficient Deep Learning for COVID-19 Diagnosis Based on CT Scans | 2020 |  | preprint | 449 | yes | Xuehai He, Xingyi Yang, Shanghang Zhang, Jinyu Zhao, Yichen Zhang, Eric P. Xing, Pengtao Xie | Overfitting, Coronavirus disease 2019 (COVID-19), Inefficiency, Artificial intelligence, Computer science, Transfer of learning, +20 more | https://doi.org/10.1101/2020.04.13.20063941 | Abstract Coronavirus disease 2019 (COVID-19) has infected more than 1.3 million individuals all over the world and caused more than 106,000 deaths. One major hurdle in controlling the spreading of this disease is the inefficiency and shortage of medical tests. There have been increasing efforts on developing deep learning methods to diagnose COVID-19 based on CT scans. However, these works are difficult to reproduce and adopt since the CT data used in their studies are not publicly available. Besides, these works require a large number of CTs to train accurate diagnosis models, which are difficult to obtain. In this paper, we aim to address these two problems. We build a publicly-available dataset containing hundreds of CT scans positive for COVID-19 and develop sample-efficient deep learning methods that can achieve high diagnosis accuracy of COVID-19 from CT scans even when the number… |
| https://openalex.org/W4390711247 | Network pharmacology: towards the artificial intelligence-based precision traditional Chinese medicine | 2023 | Briefings in Bioinformatics | article | 475 | yes | Peng Zhang, Dingfan Zhang, Wuai Zhou, Lan Wang, Boyang Wang, Tingyu Zhang, Shao Li | Perspective (graphical), Computer science, Artificial intelligence, Traditional Chinese medicine, Mechanism (biology), Data science, +5 more | https://doi.org/10.1093/bib/bbad518 | Abstract Network pharmacology (NP) provides a new methodological perspective for understanding traditional medicine from a holistic perspective, giving rise to frontiers such as traditional Chinese medicine network pharmacology (TCM-NP). With the development of artificial intelligence (AI) technology, it is key for NP to develop network-based AI methods to reveal the treatment mechanism of complex diseases from massive omics data. In this review, focusing on the TCM-NP, we summarize involved AI methods into three categories: network relationship mining, network target positioning and network target navigating, and present the typical application of TCM-NP in uncovering biological basis and clinical value of Cold/Hot syndromes. Collectively, our review provides researchers with an innovative overview of the methodological progress of NP and its application in TCM from the AI perspective. |
| https://openalex.org/W3035038672 | DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference | 2020 |  | article | 278 | yes | Ji Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, Jimmy Lin | Inference, Computer science, Transformer, Language model, Redundancy (engineering), Artificial intelligence, +5 more | https://doi.org/10.18653/v1/2020.acl-main.204 | Large-scale pre-trained language models such as BERT have brought significant improvements to NLP applications. However, they are also notorious for being slow in inference, which makes them difficult to deploy in real-time applications. We propose a simple but effective method, DeeBERT, to accelerate BERT inference. Our approach allows samples to exit earlier without passing through the entire model. Experiments show that DeeBERT is able to save up to ~40% inference time with minimal degradation in model quality. Further analyses show different behaviors in the BERT transformer layers and also reveal their redundancy. Our work provides new ideas to efficiently apply deep transformer-based models to downstream tasks. Code is available at https://github.com/castorini/DeeBERT. |
| https://openalex.org/W4315977496 | A Survey of Graph Neural Networks for Recommender Systems: Challenges, Methods, and Directions | 2023 | ACM Transactions on Recommender Systems | article | 583 | yes | Chen Gao, Yu Zheng, Nian Li, Yinfeng Li, Yingrong Qin, Jinghua Piao, Yuhan Quan, Jianxin Chang, +3 more | Recommender system, Computer science, Artificial neural network, Graph, Artificial intelligence, Data science, +2 more | https://doi.org/10.1145/3568022 | Recommender system is one of the most important information services on today’s Internet. Recently, graph neural networks have become the new state-of-the-art approach to recommender systems. In this survey, we conduct a comprehensive review of the literature on graph neural network-based recommender systems. We first introduce the background and the history of the development of both recommender systems and graph neural networks. For recommender systems, in general, there are four aspects for categorizing existing works: stage, scenario, objective, and application. For graph neural networks, the existing methods consist of two categories: spectral models and spatial ones. We then discuss the motivation of applying graph neural networks into recommender systems, mainly consisting of the high-order connectivity, the structural property of data and the enhanced supervision signal. We then… |
| https://openalex.org/W4281749437 | Swin transformer for hyperspectral rare sub-pixel target detection | 2022 |  | article | 3 | no | Ludovic Girard, Vincent Roy, Thierry Eude, Philippe Giguère | Hyperspectral imaging, Transformer, Computer science, Architecture, Pixel, Convolutional neural network, +8 more | https://doi.org/10.1117/12.2617779 | Transformers are becoming the state-of-the-art in multiple Computer vision (CV) and Natural language processing (NLP) tasks. As for hyperspectral target detection, a Transformer architecture named SpectralFormer2 has been developed and demonstrated improved performance over the previously state-of-the-art Convolutional neural network (CNN) architecture on widely studied classification tasks. The SpectralFormer was adapted from a CV architecture, the Vision Transformer (ViT). Concurrently, still in CV, a hierarchical and multi-scale version of the ViT, named Shifted windows (Swin) Transformer, is gaining momentum and is already the stateof-the-art on multiple tasks. In this paper, we adapt the Swin Transformer for hyperspectral classification and rare sub-pixel target detection. We apply this new architecture to commonly studied classification benchmarks public datasets such as Pavia Uni… |
| https://openalex.org/W4367055910 | Self-supervised learning for medical image classification: a systematic review and implementation guidelines | 2023 | npj Digital Medicine | review | 345 | yes | Shih-Cheng Huang, Anuj Pareek, Malte Jensen, Matthew P. Lungren, Serena Yeung, Akshay Chaudhari | Artificial intelligence, Computer science, Machine learning, Deep learning, Supervised learning, Medical imaging, +6 more | https://doi.org/10.1038/s41746-023-00811-0 | Abstract Advancements in deep learning and computer vision provide promising solutions for medical image analysis, potentially improving healthcare and patient outcomes. However, the prevailing paradigm of training deep learning models requires large quantities of labeled training data, which is both time-consuming and cost-prohibitive to curate for medical images. Self-supervised learning has the potential to make significant contributions to the development of robust medical imaging models through its ability to learn useful insights from copious medical datasets without labels. In this review, we provide consistent descriptions of different self-supervised learning strategies and compose a systematic review of papers published between 2012 and 2022 on PubMed, Scopus, and ArXiv that applied self-supervised learning to medical imaging classification. We screened a total of 412 relevant… |
| https://openalex.org/W3034408878 | Pretrained Transformers Improve Out-of-Distribution Robustness | 2020 |  | preprint | 287 | yes | Dan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam Dziedzic, Rishabh Krishnan, Dawn Song | Robustness (evolution), Computer science, Transformer, Artificial intelligence, Machine learning, Engineering, +5 more | https://doi.org/10.18653/v1/2020.acl-main.244 | Although pretrained Transformers such as BERT achieve high accuracy on in-distribution examples, do they generalize to new distributions? We systematically measure out-of-distribution (OOD) generalization for seven NLP datasets by constructing a new robustness benchmark with realistic distribution shifts. We measure the generalization of previous models including bag-of-words models, ConvNets, and LSTMs, and we show that pretrained Transformers’ performance declines are substantially smaller. Pretrained transformers are also more effective at detecting anomalous or OOD examples, while many previous models are frequently worse than chance. We examine which factors affect robustness, finding that larger models are not necessarily more robust, distillation can be harmful, and more diverse pretraining data can enhance robustness. Finally, we show where future work can improve OOD robustness. |
| https://openalex.org/W4393342149 | A Novel Pretrained General-purpose Vision Language Model for the Vietnamese Language | 2024 | ACM Transactions on Asian and Low-Resource Language Information Processing | article | 2 | yes | Đinh Nhật Vũ, Quang Nhat Minh Pham, Giang Son Tran | Computer science, Closed captioning, Question answering, Artificial intelligence, Natural language processing, Vietnamese, +15 more | https://doi.org/10.1145/3654796 | Lying in the cross-section of computer vision and natural language processing, vision language models are capable of processing images and text at once. These models are helpful in various tasks: text generation from image and vice versa, image-text retrieval, or visual navigation. Besides building a model trained on a dataset for a task, people also study general-purpose models to utilize many datasets for multitasks. Their two primary applications are image captioning and visual question answering. For English, large datasets and foundation models are already abundant. However, for Vietnamese, they are still limited. To expand the language range, this work proposes a pretrained general-purpose image-text model named VisualRoBERTa. A dataset of 600k images with captions (translated MS COCO 2017 from English to Vietnamese) is introduced to pretrain VisualRoBERTa. The model’s architectur… |
| https://openalex.org/W4386557624 | Language Models for Novelty Detection in System Call Traces | 2023 | arXiv (Cornell University) | preprint | 3 | yes | Quentin Fournier, Daniel Aloise, Leandro R. Costa | Computer science, Novelty, Source code, Machine learning, Language model, Open source, +7 more | http://arxiv.org/abs/2309.02206 | Due to the complexity of modern computer systems, novel and unexpected behaviors frequently occur. Such deviations are either normal occurrences, such as software updates and new user activities, or abnormalities, such as misconfigurations, latency issues, intrusions, and software bugs. Regardless, novel behaviors are of great interest to developers, and there is a genuine need for efficient and effective methods to detect them. Nowadays, researchers consider system calls to be the most fine-grained and accurate source of information to investigate the behavior of computer systems. Accordingly, this paper introduces a novelty detection methodology that relies on a probability distribution over sequences of system calls, which can be seen as a language model. Language models estimate the likelihood of sequences, and since novelties deviate from previously observed behaviors by definition… |
| https://openalex.org/W4402614015 | A Comparative Study of Deep Learning Approaches for Arabic Language Processing | 2024 | Jordan Journal of Electrical Engineering | article | 2 | no | Mahmoud Mohamed, Khaled Alosman | Arabic, Natural language processing, Computer science, Artificial intelligence, Linguistics, Philosophy | https://doi.org/10.5455/jjee.204-1711016538 | Arabic is a difficult language for natural language processing (NLP) because of its complicated morphology, dialectal differences and the limited annotated resources. Although deep learning algorithms have reached state-of-the-art results in many NLP tasks, comprehensive comparative studies for Arabic remains scarce. This paper addresses this gap by systematically evaluating three prominent deep learning architectures - namely Recurrent Neural Networks (RNNs), Convolutional Neural Networks (CNNs) and Transformers - across five essential Arabic NLP tasks: i) mention of sentiment analysis, ii) named entity recognition, iii) machine translation, iv) text classification and v) dialect identification. We differ the performance of models trained from scratch with fine-tuned versions of AraBERT, a powerful Transformer-based model pre-trained on a large Arabic corpus. Our experiments employ the… |
| https://openalex.org/W4313069085 | Intelligent Learning Rate Distribution to Reduce Catastrophic Forgetting in Transformers | 2022 | Lecture notes in computer science | book-chapter | 2 | yes | Philip Kenneweg, Alexander Schulz, Sarah Schröder, Barbara Hammer | Forgetting, Computer science, Hyperparameter, Artificial intelligence, Transformer, Artificial neural network, +8 more | http://arxiv.org/abs/2404.01317 |  |
| https://openalex.org/W4385485020 | Software Module Classification for Commercial Bug Reports | 2023 |  | article | 4 | no | Ceyhun E. Öztürk, Eyüp Halit Yılmaz, Ömer Köksal, Aykut Koç | Computer science, Software regression, Support vector machine, Artificial intelligence, Machine learning, Software, +12 more | http://dx.doi.org/10.1109/icasspw59220.2023.10193706 | In this work, we curate and investigate a dataset named Turkish Software Report - Module Classification (TSRMC), consisting of commercial software bug reports of a company. Automated bug classification is required in large-scale software projects due to the vast amount of bugs. We analyze and report the statistical features and classification difficulty of the dataset. We use several methods from the text classification literature to assign each bug report of the TSRMC dataset a suitable software module. The utilized methods include traditional machine learning (ML) methods, such as support vector machine (SVM) and logistic regression; sequential deep learning (DL) models, such as gated recurrent unit (GRU) and convolutional neural networks (CNN); and Bidirectional Encoder Representations from Transformers (BERT)-based pre-trained language models (PLMs). Our work is one of the first eff… |
| https://openalex.org/W4386576685 | MTEB: Massive Text Embedding Benchmark | 2023 |  | article | 319 | yes | Niklas Muennighoff, Nouamane Tazi, Loïc Magne, Nils Reimers | Embedding, Benchmark (surveying), Benchmarking, Computer science, Similarity (geometry), Set (abstract data type), +19 more | https://doi.org/10.18653/v1/2023.eacl-main.148 | Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings todate. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficien… |
| https://openalex.org/W4385951836 | Chinese text classification by combining Chinese-BERTology-wwm and GCN | 2023 | PeerJ Computer Science | article | 9 | yes | Xue Xu, Yu Chang, Jianye An, Yongqiang Du | Computer science, Natural language processing, Artificial intelligence, Graph, Sentence, Theoretical computer science | https://doi.org/10.7717/peerj-cs.1544 | Text classification is an important and classic application in natural language processing (NLP). Recent studies have shown that graph neural networks (GNNs) are effective in tasks with rich structural relationships and serve as effective transductive learning approaches. Text representation learning methods based on large-scale pretraining can learn implicit but rich semantic information from text. However, few studies have comprehensively utilized the contextual semantic and structural information for Chinese text classification. Moreover, the existing GNN methods for text classification did not consider the applicability of their graph construction methods to long or short texts. In this work, we propose Chinese-BERTology-wwm-GCN, a framework that combines Chinese bidirectional encoder representations from transformers (BERT) series models with whole word masking (Chinese-BERTology-w… |
| https://openalex.org/W4324116354 | Artificial Intelligence Algorithms in Biomedical Application | 2023 |  | article | 4 | no | Yuehua Song | Biomedicine, Computer science, Artificial intelligence, Machine learning, Variety (cybernetics), Artificial neural network, +6 more | https://doi.org/10.1109/isbp57705.2023.10061317 | In recent years, the rapid development of artificial intelligence (AI) has accelerated the development of many social industries. In view of the demand for large data collection and effective medical data processing, AI has undoubtedly become an important part of biomedical research. Medical professionals can accurately diagnose and treat a variety of symptoms in patients with the help of AI algorithms. Modern AI technologies, such as traditional neural networks for structured data and natural language processing for unstructured data, can accurately analyze various medical data. The medical industry uses these AI learning techniques for disease diagnosis, drug discovery, and medical image analysis. Against this backdrop, this paper focuses on the application of AI algorithms in biomedicine and examined cases from biomedical research in addition to the introduction of machine learning,… |
| https://openalex.org/W4398765027 | Mechanics of a Drone-based System for Algal Bloom Detection Utilizing Deep Learning and LLMs | 2023 |  | article | 3 | no | Andrea Balcacer, Brendan Hannon, Yulia Kumar, Kuan Huang, Joseph Sarnoski, Shuting Liu, J. Jenny Li, Patricia Morreale | Drone, Algal bloom, Computer science, Bloom, Deep learning, Segmentation, +15 more | https://doi.org/10.1109/urtc60662.2023.10534955 | The emergence of harmful algal blooms (HABs) along the U.S. East Coast has necessitated the development of an innovative drone-based detection system, a response to the growing challenges posed by climate change and pollution. This multifaceted approach involves the collection of high-resolution images from drones and smartphone cameras, the detection and classification of algae using Transformer Neural Networks and Large Language Models (LLMs), and the application of a data segmentation approach for efficient data analysis. The system enhances traditional monitoring techniques by integrating unique custom data gathered from New Jersey's water bodies with public data collected by the state's Environmental Protection Department. It offers a more precise and efficient method for HAB detection. |
| https://openalex.org/W3156470785 | Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity | 2022 | Proceedings of the 60th Annual Meeting of the Association for Computational Lin… | article | 403 | yes | Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, Pontus Stenetorp | Computer science, Permutation (music), Set (abstract data type), Language model, Construct (python library), Generative grammar, +11 more | https://doi.org/10.18653/v1/2022.acl-long.556 | When primed with only a handful of training samples, very large, pretrained language models such as GPT-3 have shown competitive results when compared to fully-supervised, fine-tuned, large, pretrained language models. We demonstrate that the order in which the samples are provided can make the difference between near state-of-the-art and random guess performance: essentially some permutations are “fantastic” and some not. We analyse this phenomenon in detail, establishing that: it is present across model sizes (even for the largest current models), it is not related to a specific subset of samples, and that a given good permutation for one model is not transferable to another. While one could use a development set to determine which permutations are performant, this would deviate from the true few-shot setting as it requires additional annotated data. Instead, we use the generative nat… |
| https://openalex.org/W4361002760 | The Role of ChatGPT in Data Science: How AI-Assisted Conversational Interfaces Are Revolutionizing the Field | 2023 | Big Data and Cognitive Computing | article | 360 | yes | Hossein Hassani, Emmanuel Sirimal Silva | Computer science, Data science, Workflow, Field (mathematics), Artificial intelligence, Data pre-processing, +3 more | https://doi.org/10.3390/bdcc7020062 | ChatGPT, a conversational AI interface that utilizes natural language processing and machine learning algorithms, is taking the world by storm and is the buzzword across many sectors today. Given the likely impact of this model on data science, through this perspective article, we seek to provide an overview of the potential opportunities and challenges associated with using ChatGPT in data science, provide readers with a snapshot of its advantages, and stimulate interest in its use for data science projects. The paper discusses how ChatGPT can assist data scientists in automating various aspects of their workflow, including data cleaning and preprocessing, model training, and result interpretation. It also highlights how ChatGPT has the potential to provide new insights and improve decision-making processes by analyzing unstructured data. We then examine the advantages of ChatGPT’s arc… |
| https://openalex.org/W3158843543 | Form 10-Q Itemization | 2021 | arXiv (Cornell University) | preprint | 4 | yes | Yanci Zhang, Tianming Du, Yujie Sun, Lawrence Donohue, Rui Dai | Computer science, Convolutional neural network, Pipeline (software), Transformer, Information retrieval, Classifier (UML), +8 more | http://arxiv.org/abs/2104.11783 | The quarterly financial statement, or Form 10-Q, is one of the most frequently required filings for US public companies to disclose financial and other important business information. Due to the massive volume of 10-Q filings and the enormous variations in the reporting format, it has been a long-standing challenge to retrieve item-specific information from 10-Q filings that lack machine-readable hierarchy. This paper presents a solution for itemizing 10-Q files by complementing a rule-based algorithm with a Convolutional Neural Network (CNN) image classifier. This solution demonstrates a pipeline that can be generalized to a rapid data retrieval solution among a large volume of textual data using only typographic items. The extracted textual data can be used as unlabeled content-specific data to train transformer models (e.g., BERT) or fit into various field-focus natural language proc… |
| https://openalex.org/W2970862333 | Designing and Interpreting Probes with Control Tasks | 2019 |  | article | 397 | yes | John Hewitt, Percy Liang | Computer science, Joint (building), Natural language processing, Natural (archaeology), Control (management), Artificial intelligence, +4 more | https://doi.org/10.18653/v1/d19-1275 | John Hewitt, Percy Liang. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019. |
| https://openalex.org/W4380685454 | Re-Thinking Data Strategy and Integration for Artificial Intelligence: Concepts, Opportunities, and Challenges | 2023 | Applied Sciences | article | 534 | yes | Abdulaziz Aldoseri, Khalifa N. Al‐Khalifa, A.M.S. Hamouda | Interpretability, Big data, Computer science, Quality (philosophy), Artificial intelligence, Data quality, +8 more | https://doi.org/10.3390/app13127082 | The use of artificial intelligence (AI) is becoming more prevalent across industries such as healthcare, finance, and transportation. Artificial intelligence is based on the analysis of large datasets and requires a continuous supply of high-quality data. However, using data for AI is not without challenges. This paper comprehensively reviews and critically examines the challenges of using data for AI, including data quality, data volume, privacy and security, bias and fairness, interpretability and explainability, ethical concerns, and technical expertise and skills. This paper examines these challenges in detail and offers recommendations on how companies and organizations can address them. By understanding and addressing these challenges, organizations can harness the power of AI to make smarter decisions and gain competitive advantage in the digital age. It is expected, since this r… |
| https://openalex.org/W3104598375 | Vector-Vector-Matrix Architecture: A Novel Hardware-Aware Framework for Low-Latency Inference in NLP Applications | 2020 |  | article | 2 | yes | Matthew Khoury, Rumen Dangovski, Longwu Ou, Preslav Nakov, Yichen Shen, Jing Li | Computer science, FLOPS, Inference, Latency (audio), Machine translation, Artificial intelligence, +11 more | https://doi.org/10.18653/v1/2020.emnlp-main.640 | Deep neural networks have become the standard approach to building reliable Natural Language Processing (NLP) applications, ranging from Neural Machine Translation (NMT) to dialogue systems. However, improving accuracy by increasing the model size requires a large number of hardware computations, which can slow down NLP applications significantly at inference time. To address this issue, we propose a novel vector-vector-matrix architecture (VVMA), which greatly reduces the latency at inference time for NMT. This architecture takes advantage of specialized hardware that has low-latency vector-vector operations and higher-latency vector-matrix operations. It also reduces the number of parameters and FLOPs for virtually all models that rely on efficient matrix multipliers without significantly impacting accuracy. We present empirical results suggesting that our framework can reduce the lat… |
| https://openalex.org/W4321490654 | AtmoRep: Large Scale Representation Learning for Atmospheric Data | 2023 |  | preprint | 3 | yes | Christian Lessig, I. Luise, Martin G. Schultz | Initialization, Computer science, Artificial neural network, Representation (politics), Downscaling, Artificial intelligence, +15 more | https://doi.org/10.5194/egusphere-egu23-3117 | The AtmoRep project asks if one can train one neural network that represents and describes all atmospheric dynamics. AtmoRep&amp;#8217;s ambition is hence to demonstrate that the concept of large-scale representation learning, whose principle feasibility and potential was established by large language models such as GPT-3, is also applicable to scientific data and in particular to atmospheric dynamics. The project is enabled by the large amounts of atmospheric observations that have been made in the past as well as advances on neural network architectures and self-supervised learning that allow for effective training on petabytes of data. Eventually, we aim to train on all of the ERA5 reanalysis and, furthermore, fine tune on observational data such as satellite measurements to move beyond the limits of reanalyses.We will present the theoretical formulation of AtmoRep as an approximate… |
| https://openalex.org/W4402447115 | An overview of current issues in automatic text summarization of natural language using artificial intelligence methods | 2024 | Technology audit and production reserves | article | 5 | yes | Oleksii Kuznietsov, Gennadiy Kyselov | Automatic summarization, Computer science, Natural language processing, Current (fluid), Artificial intelligence, Natural language, +5 more | https://doi.org/10.15587/2706-5448.2024.309472 | The object of the research is the task of automatic abstracting of natural language texts. The importance of these tasks is determined by the existing problem of creating essays that would adequately reflect the content of the original text and highlight key information. This task requires the ability of models to deeply analyze the context of the text, which complicates the abstracting process. Results are presented that demonstrate the effectiveness of using generative models based on neural networks, text semantic analysis methods, and deep learning for automatic creation of abstracts. The use of models showed a high level of adequacy and informativeness of abstracts. GPT (Generative Pre-trained Transformer) generates text that looks like it was written by a human, which makes it useful for automatic essay generation. For example, the GPT model generates abbreviated summaries based o… |
| https://openalex.org/W3035229828 | A Joint Neural Model for Information Extraction with Global Features | 2020 |  | article | 372 | yes | Ying Lin, Heng Ji, Fei Huang, Lingfei Wu | Computer science, Sentence, Pairwise comparison, Artificial intelligence, Graph, Event (particle physics), +18 more | https://doi.org/10.18653/v1/2020.acl-main.713 | Most existing joint neural models for Information Extraction (IE) use local task-specific classifiers to predict labels for individual instances (e.g., trigger, relation) regardless of their interactions. For example, a victim of a die event is likely to be a victim of an attack event in the same sentence. In order to capture such cross-subtask and cross-instance inter-dependencies, we propose a joint neural framework, OneIE, that aims to extract the globally optimal IE result as a graph from an input sentence. OneIE performs end-to-end IE in four stages: (1) Encoding a given sentence as contextualized word representations; (2) Identifying entity mentions and event triggers as nodes; (3) Computing label scores for all nodes and their pairwise links using local classifiers; (4) Searching for the globally optimal graph with a beam decoder. At the decoding stage, we incorporate global feat… |
| https://openalex.org/W4388405233 | Gex'ez-English Bi-Directional Neural Machine Translation Using Transformer | 2023 |  | article | 2 | no | Sefineh Getachew, Yirga Yayeh Munaye | Machine translation, Computer science, Artificial intelligence, Transformer, Artificial neural network, Natural language processing, +8 more | https://doi.org/10.1109/ict4da59526.2023.10302254 | Machine translation is the technique of translating texts from one language to another without human intervention using artificial intelligence. Neural Machine Translation (NMT) is a method of machine translation that employs a large artificial neural network such as Transformers to forecast the possibility of a set of words, frequently in the form of complete sentences. There are several old Ge'ez scripts both in Ethiopia and elsewhere that require translation. Students and researchers are currently eager in learning about and being involved in the study of manuscripts written in Ge'ez. To increase Ge'ez users and solve the problems within the endangered Ge'ez language, it is important to develop Neural Machine Translation between English and Ge'ez languages. Therefore, the purpose of this paper is to propose the Ge'ez to English Bi-directional Neural Machine Translation using a deep l… |
| https://openalex.org/W2952984539 | Probing Neural Network Comprehension of Natural Language Arguments | 2019 |  | preprint | 378 | yes | Timothy Niven, Hung‐Yu Kao | Adversarial system, Argument (complex analysis), Comprehension, Computer science, Spurious relationship, Exploit, +18 more | https://doi.org/10.18653/v1/p19-1459 | We are surprised to find that BERT's peak performance of 77% on the Argument Reasoning Comprehension Task reaches just three points below the average untrained human baseline. However, we show that this result is entirely accounted for by exploitation of spurious statistical cues in the dataset. We analyze the nature of these cues and demonstrate that a range of models all exploit them. This analysis informs the construction of an adversarial dataset on which all models achieve random accuracy. Our adversarial dataset provides a more robust assessment of argument comprehension and should be adopted as the standard in future work. |
| https://openalex.org/W3116202926 | Could graph neural networks learn better molecular representation for drug discovery? A comparison study of descriptor-based and graph-based models | 2021 | Journal of Cheminformatics | article | 570 | yes | Dejun Jiang, Zhenhua Wu, Chang‐Yu Hsieh, Guangyong Chen, Ben Liao, Zhe Wang, Chao Shen, Dongsheng Cao, +2 more | Computer science, Machine learning, Graph, Support vector machine, Artificial intelligence, Property (philosophy), +8 more | https://doi.org/10.1186/s13321-020-00479-8 |  |
| https://openalex.org/W4388725043 | A study of generative large language model for medical research and healthcare | 2023 | npj Digital Medicine | article | 321 | yes | Peng Cheng, Xi Yang, Aokun Chen, Kaleb Smith, Nima PourNejatian, Anthony Costa, Cheryl Martin, Mona G. Flores, +11 more | Readability, Enthusiasm, Health care, Artificial intelligence, Test (biology), Relevance (law), +12 more | https://doi.org/10.1038/s41746-023-00958-w | Abstract There are enormous enthusiasm and concerns in applying large language models (LLMs) to healthcare. Yet current assumptions are based on general-purpose LLMs such as ChatGPT, which are not developed for medical use. This study develops a generative clinical LLM, GatorTronGPT, using 277 billion words of text including (1) 82 billion words of clinical text from 126 clinical departments and approximately 2 million patients at the University of Florida Health and (2) 195 billion words of diverse general English text. We train GatorTronGPT using a GPT-3 architecture with up to 20 billion parameters and evaluate its utility for biomedical natural language processing (NLP) and healthcare text generation. GatorTronGPT improves biomedical natural language processing. We apply GatorTronGPT to generate 20 billion words of synthetic text. Synthetic NLP models trained using synthetic text ge… |
| https://openalex.org/W2970431814 | Knowledge-Enriched Transformer for Emotion Detection in Textual Conversations | 2019 |  | article | 289 | yes | Peixiang Zhong, Di Wang, Chunyan Miao | Transformer, Computer science, Emotion detection, Natural language processing, Artificial intelligence, Emotion recognition, +3 more | https://doi.org/10.18653/v1/d19-1016 | Messages in human conversations inherently convey emotions. The task of detecting emotions in textual conversations leads to a wide range of applications such as opinion mining in social networks. However, enabling machines to analyze emotions in conversations is challenging, partly because humans often rely on the context and commonsense knowledge to express emotions. In this paper, we address these challenges by proposing a Knowledge-Enriched Transformer (KET), where contextual utterances are interpreted using hierarchical self-attention and external commonsense knowledge is dynamically leveraged using a context-aware affective graph attention mechanism. Experiments on multiple textual conversation datasets demonstrate that both context and commonsense knowledge are consistently beneficial to the emotion detection performance. In addition, the experimental results show that our KET mo… |
| https://openalex.org/W4385991043 | Language Independent Neuro-Symbolic Semantic Parsing for Form Understanding | 2023 | Lecture notes in computer science | book-chapter | 3 | no | Bhanu Prakash Voutharoja, Lizhen Qu, Fatemeh Shiri | Computer science, Parsing, Relation (database), Artificial intelligence, Inference, Theoretical computer science, +4 more | https://doi.org/10.1007/978-3-031-41679-8_8 |  |
| https://openalex.org/W2970295111 | Facebook FAIR’s WMT19 News Translation Task Submission | 2019 |  | article | 314 | yes | Nathan Ng, Kyra Yee, Alexei Baevski, Myle Ott, Michael Auli, Sergey Edunov | Computer science, Transformer, German, Natural language processing, Language model, Machine translation, +22 more | https://doi.org/10.18653/v1/w19-5333 | This paper describes Facebook FAIR’s submission to the WMT19 shared news translation task. We participate in four language directions, English <-> German and English <-> Russian in both directions. Following our submission from last year, our baseline systems are large BPE-based transformer models trained with the FAIRSEQ sequence modeling toolkit. This year we experiment with different bitext data filtering schemes, as well as with adding filtered back-translated data. We also ensemble and fine-tune our models on domain-specific data, then decode using noisy channel model reranking. Our system improves on our previous system’s performance by 4.5 BLEU points and achieves the best case-sensitive BLEU score for the translation direction English→Russian. |
| https://openalex.org/W4400997514 | Machine unlearning for generative AI | 2023 | Journal of AI, robotics & workplace automation. | article | 3 | no | Yashaswini Viswanath, Sudha Jamthe, Suresh Lokiah, Emanuele Bianchini | Generative grammar, Computer science, Artificial intelligence | https://doi.org/10.69554/kzrs2422 | This paper introduces a new field of AI research called machine unlearning and examines the challenges and approaches to extend machine unlearning to generative AI (GenAI). Machine unlearning is a model-driven approach to make an existing artificial intelligence (AI) model unlearn a set of data from its learning. Machine unlearning is becoming important for businesses to comply with privacy laws such as General Data Protection Regulation (GDPR) customer’s right to be forgotten, to manage security and to remove bias that AI models learn from their training data, as it is expensive to retrain and deploy the models without the bias or security or privacy compromising data. This paper presents the state of the art in machine unlearning approaches such as exact unlearning, approximate unlearning, zero-shot learning (ZSL) and fast and efficient unlearning. The paper highlights the challenges… |
| https://openalex.org/W4411000941 | Improving Narrative Coherence in Dense Video Captioning through Transformer and Large Language Models | 2025 | Journal of Innovative Image Processing | article | 2 | yes | Dvijesh Bhatt, Priyank Thakkar | Closed captioning, Transformer, Narrative, Computer science, Coherence (philosophical gambling strategy), Linguistics, +10 more | https://doi.org/10.36548/jiip.2025.2.005 | Dense video captioning aims to identify events within a video and generate natural language descriptions for each event. Most existing approaches adhere to a two-stage framework consisting of an event proposal module and a caption generation module. Previous methodologies have predominantly employed convolutional neural networks and sequential models to describe individual events in isolation. However, these methods limit the influence of neighboring events when generating captions for a specific segment, often resulting in descriptions that lack coherence with the broader storyline of the video. To address this limitation, we propose a captioning module that leverages both Transformer architecture and a Large Language Model (LLM). A convolutional and LSTM-based proposal module is used to detect and localize events within the video. An encoder-decoder-based Transformer model generates a… |
| https://openalex.org/W3035542229 | MixText: Linguistically-Informed Interpolation of Hidden Space for Semi-Supervised Text Classification | 2020 |  | preprint | 336 | yes | Jiaao Chen, Zichao Yang, Diyi Yang | Computer science, Leverage (statistics), Labeled data, Artificial intelligence, Semi-supervised learning, Interpolation (computer graphics), +15 more | https://doi.org/10.18653/v1/2020.acl-main.194 | This paper presents MixText, a semi-supervised learning method for text classification, which uses our newly designed data augmentation method called TMix. TMix creates a large amount of augmented training samples by interpolating text in hidden space. Moreover, we leverage recent advances in data augmentation to guess low-entropy labels for unlabeled data, hence making them as easy to use as labeled data. By mixing labeled, unlabeled and augmented data, MixText significantly outperformed current pre-trained and fined-tuned models and other state-of-the-art semi-supervised learning methods on several text classification benchmarks. The improvement is especially prominent when supervision is extremely limited. We have publicly released our code at https://github.com/GT-SALT/MixText. |
| https://openalex.org/W4411866951 | A Systematic Review of Pretrained Models in Automated Essay Scoring | 2025 | IEEE Access | review | 3 | yes | Ahmed M. ElMassry, Nazar Zaki, Negmeldin Alsheikh, Mohammed Mediani | Computer science, Artificial intelligence, Natural language processing, Information retrieval | https://doi.org/10.1109/access.2025.3584784 | Automated essay scoring (AES) uses artificial intelligence and machine learning methods to grade student essays and produce human-like scores. AES research has witnessed significant advancements over time by adopting diverse machine learning models. This evolution started with traditional techniques like regression and classification, and later advanced to deep learning models that leverage neural networks for enhancing scoring performance. This review focuses on the utilization of Transformer architectures, a sophisticated form of neural networks employing attention mechanisms, with an emphasis on pretrained models like BERT in AES research. The aim is to enhance the understanding of their applicability in advancing the AES research landscape. Additionally, this study selected and analyzed relevant papers from Scopus and Web of Science databases in the past five years, by adhering to t… |
| https://openalex.org/W2998184481 | Do Not Have Enough Data? Deep Learning to the Rescue! | 2020 | Proceedings of the AAAI Conference on Artificial Intelligence | article | 295 | yes | Ateret Anaby-Tavor, Boaz Carmeli, Esther Goldbraich, Amir Kantor, George Kour, Segev Shlomov, Naama Tepper, Naama Zwerdling | Computer science, Artificial intelligence, Classifier (UML), Machine learning, Labeled data, Task (project management), +8 more | https://doi.org/10.1609/aaai.v34i05.6233 | Based on recent advances in natural language modeling and those in text generation capabilities, we propose a novel data augmentation method for text classification tasks. We use a powerful pre-trained neural network model to artificially synthesize new labeled data for supervised learning. We mainly focus on cases with scarce labeled data. Our method, referred to as language-model-based data augmentation (LAMBADA), involves fine-tuning a state-of-the-art language generator to a specific task through an initial training phase on the existing (usually small) labeled data. Using the fine-tuned model and given a class label, new sentences for the class are generated. Our process then filters these new sentences by using a classifier trained on the original data. In a series of experiments, we show that LAMBADA improves classifiers' performance on a variety of datasets. Moreover, LAMBADA si… |
| https://openalex.org/W4388195809 | Identifying Signs and Symptoms of Urinary Tract Infection from Emergency Department Clinical Notes Using Large Language Models | 2023 |  | preprint | 3 | yes | Mark Iscoe, Vimig Socrates, Aidan Gilson, Ling Chi, Huan Li, Thomas Huang, Thomas Kearns, Rachelle Perkins, +2 more | Emergency department, Electronic health record, Medicine, Recall, Population, Artificial intelligence, +10 more | https://doi.org/10.1101/2023.10.20.23297156 | Abstract Objectives Symptom characterization is critical to urinary tract infection (UTI) diagnosis, but identification of symptoms from the electronic health record (EHR) is challenging, limiting large-scale research, public health surveillance, and EHR-based clinical decision support. We therefore developed and compared two natural language processing (NLP) models to identify UTI symptoms from unstructured emergency department (ED) notes. Methods The study population consisted of patients aged ≥ 18 who presented to the (ED) in a north-eastern United States health system between June 2013 and August 2021 and had a urinalysis performed. We annotated a random subset of 1,250 ED clinician notes from these visits for a list of 17 UTI symptoms. We then developed two task-specific large language models (LLMs) to perform the task of named entity recognition (NER): a convolutional neural netwo… |
| https://openalex.org/W4288031278 | Sentence-Level BERT and Multi-Task Learning of Age and Gender in Social\n Media | 2019 | arXiv (Cornell University) | preprint | 3 | yes | Muhammad Abdul-Mageed, Chiyu Zhang, Arun Kumar Rajendran, AbdelRahim Elmadany, Michael Przystupa, Lyle Ungar | Computer science, Artificial intelligence, Sentence, Encoder, Natural language processing, Task (project management), +15 more | http://arxiv.org/abs/1911.00637 | Social media currently provide a window on our lives, making it possible to\nlearn how people from different places, with different backgrounds, ages, and\ngenders use language. In this work we exploit a newly-created Arabic dataset\nwith ground truth age and gender labels to learn these attributes both\nindividually and in a multi-task setting at the sentence level. Our models are\nbased on variations of deep bidirectional neural networks. More specifically,\nwe build models with gated recurrent units and bidirectional encoder\nrepresentations from transformers (BERT). We show the utility of multi-task\nlearning (MTL) on the two tasks and identify task-specific attention as a\nsuperior choice in this context. We also find that a single-task BERT model\noutperform our best MTL models on the two tasks. We report tweet-level accuracy\nof 51.43% for the age task (three-way) and 65.30% on t… |
| https://openalex.org/W4385556979 | What Is Machine Learning, Artificial Neural Networks and Deep Learning?—Examples of Practical Applications in Medicine | 2023 | Diagnostics | review | 287 | yes | Jakub Kufel, Katarzyna Bargieł-Łączek, Szymon Kocot, Maciej Koźlik, Wiktoria Bartnikowska, Michał Janik, Łukasz Czogalik, Piotr Dudek, +6 more | Artificial intelligence, Artificial neural network, Machine learning, Computer science, Deep learning, Context (archaeology), +7 more | https://doi.org/10.3390/diagnostics13152582 | Machine learning (ML), artificial neural networks (ANNs), and deep learning (DL) are all topics that fall under the heading of artificial intelligence (AI) and have gained popularity in recent years. ML involves the application of algorithms to automate decision-making processes using models that have not been manually programmed but have been trained on data. ANNs that are a part of ML aim to simulate the structure and function of the human brain. DL, on the other hand, uses multiple layers of interconnected neurons. This enables the processing and analysis of large and complex databases. In medicine, these techniques are being introduced to improve the speed and efficiency of disease diagnosis and treatment. Each of the AI techniques presented in the paper is supported with an example of a possible medical application. Given the rapid development of technology, the use of AI in medici… |
| https://openalex.org/W3216313460 | Database Tuning using Natural Language Processing | 2021 | ACM SIGMOD Record | article | 2 | no | Immanuel Trummer | Computer science, Transformer, Artificial intelligence, Natural language processing, Transfer of learning, Language model, +14 more | https://doi.org/10.1145/3503780.3503788 | Introduction. We have seen significant advances in the state of the art in natural language processing (NLP) over the past few years [20]. These advances have been driven by new neural network architectures, in particular the Transformer model [19], as well as the successful application of transfer learning approaches to NLP [13]. Typically, training for specific NLP tasks starts from large language models that have been pre-trained on generic tasks (e.g., predicting obfuscated words in text [5]) for which large amounts of training data are available. Using such models as a starting point reduces task-specific training cost as well as the number of required training samples by orders of magnitude [7]. These advances motivate new use cases for NLP methods in the context of databases. |
| https://openalex.org/W4362667540 | On the use of AI-based tools like ChatGPT to support management research | 2023 | European Journal of Innovation Management | article | 272 | yes | Bastian Burger, Dominik K. Kanbach, Sascha Kraus, Matthias Breier, Vincenzo Corvello | Computer science, Objectivity (philosophy), Originality, Data science, Terminology, Artificial intelligence, +12 more | https://doi.org/10.1108/ejim-02-2023-0156 | Purpose The article discusses the current relevance of artificial intelligence (AI) in research and how AI improves various research methods. This article focuses on the practical case study of systematic literature reviews (SLRs) to provide a guideline for employing AI in the process. Design/methodology/approach Researchers no longer require technical skills to use AI in their research. The recent discussion about using Chat Generative Pre-trained Transformer (GPT), a chatbot by OpenAI, has reached the academic world and fueled heated debates about the future of academic research. Nevertheless, as the saying goes, AI will not replace our job; a human being using AI will. This editorial aims to provide an overview of the current state of using AI in research, highlighting recent trends and developments in the field. Findings The main result is guidelines for the use of AI in the scienti… |
| https://openalex.org/W3093891978 | Document-Level Relation Extraction with Adaptive Thresholding and Localized Context Pooling | 2021 | Proceedings of the AAAI Conference on Artificial Intelligence | article | 271 | yes | Wenxuan Zhou, Kevin Huang, Tengyu Ma, Jing Huang | Thresholding, Pooling, Computer science, Context (archaeology), Artificial intelligence, Benchmark (surveying), +15 more | https://doi.org/10.1609/aaai.v35i16.17717 | Document-level relation extraction (RE) poses new challenges compared to its sentence-level counterpart. One document commonly contains multiple entity pairs, and one entity pair occurs multiple times in the document associated with multiple possible relations. In this paper, we propose two novel techniques, adaptive thresholding and localized context pooling, to solve the multi-label and multi-entity problems. The adaptive thresholding replaces the global threshold for multi-label classification in the prior work with a learnable entities-dependent threshold. The localized context pooling directly transfers attention from pre-trained language models to locate relevant context that is useful to decide the relation. We experiment on three document-level RE benchmark datasets: DocRED, a recently released large-scale RE dataset, and two datasets CDRand GDA in the biomedical domain. Our ATL… |
| https://openalex.org/W4382241160 | Video Captioning using Sentence Vector-enabled Convolutional Framework with Short-Connected LSTM | 2023 | Multimedia Tools and Applications | article | 7 | no | Dinesh Naik, C. D. Jaidhar | Closed captioning, Computer science, Sentence, Encoder, Context (archaeology), Artificial intelligence, +14 more | https://doi.org/10.1007/s11042-023-15978-7 |  |
| https://openalex.org/W4404823208 | Discriminative, generative artificial intelligence, and foundation models in retina imaging | 2024 | Taiwan Journal of Ophthalmology | review | 3 | yes | Paisan Ruamviboonsuk, Niracha Arjkongharn, Nattaporn Vongsa, Pawin Pakaymaskul, Natsuda Kaothanthong | Medicine, Discriminative model, Foundation (evidence), Artificial intelligence, Generative grammar, Retina, +5 more | https://doi.org/10.4103/tjo.tjo-d-24-00064 | Abstract Recent advances of artificial intelligence (AI) in retinal imaging found its application in two major categories: discriminative and generative AI. For discriminative tasks, conventional convolutional neural networks (CNNs) are still major AI techniques. Vision transformers (ViT), inspired by the transformer architecture in natural language processing, has emerged as useful techniques for discriminating retinal images. ViT can attain excellent results when pretrained at sufficient scale and transferred to specific tasks with fewer images, compared to conventional CNN. Many studies found better performance of ViT, compared to CNN, for common tasks such as diabetic retinopathy screening on color fundus photographs (CFP) and segmentation of retinal fluid on optical coherence tomography (OCT) images. Generative Adversarial Network (GAN) is the main AI technique in generative AI in… |
| https://openalex.org/W3155475857 | Demystifying BERT: Implications for Accelerator Design | 2021 | arXiv (Cornell University) | preprint | 3 | yes | Suchita Pati, Shaizeen Aga, Nuwan Jayasena, Matthew D. Sinclair | Computer science, Computation, Artificial intelligence, Convolutional neural network, Encoder, Transformer, +16 more | http://arxiv.org/abs/2104.08335 | Transfer learning in natural language processing (NLP), as realized using models like BERT (Bi-directional Encoder Representation from Transformer), has significantly improved language representation with models that can tackle challenging language problems. Consequently, these applications are driving the requirements of future systems. Thus, we focus on BERT, one of the most popular NLP transfer learning algorithms, to identify how its algorithmic behavior can guide future accelerator design. To this end, we carefully profile BERT training and identify key algorithmic behaviors which are worthy of attention in accelerator design. We observe that while computations which manifest as matrix multiplication dominate BERT's overall runtime, as in many convolutional neural networks, memory-intensive computations also feature prominently. We characterize these computations, which have receiv… |
| https://openalex.org/W4225000412 | Accelerating materials discovery using artificial intelligence, high performance computing and robotics | 2022 | npj Computational Materials | article | 343 | yes | Edward O. Pyzer‐Knapp, Jed W. Pitera, Peter Staar, Seiji Takeda, Teodoro Laino, Daniel P. Sanders, James Sexton, John R. Smith, +1 more | Workflow, Computer science, Automation, Artificial intelligence, Robotics, Engineering, +3 more | https://doi.org/10.1038/s41524-022-00765-z | Abstract New tools enable new ways of working, and materials science is no exception. In materials discovery, traditional manual, serial, and human-intensive work is being augmented by automated, parallel, and iterative processes driven by Artificial Intelligence (AI), simulation and experimental automation. In this perspective, we describe how these new capabilities enable the acceleration and enrichment of each stage of the discovery cycle. We show, using the example of the development of a novel chemically amplified photoresist, how these technologies’ impacts are amplified when they are used in concert with each other as powerful, heterogeneous workflows. |
| https://openalex.org/W4383748896 | Energy Consumption Optimization of Swin Transformer Based on Local Aggregation and Group-Wise Transformation | 2023 |  | article | 2 | no | Yuhong Liu, Shengbo Chen, Lei Zhou, Peng Wang | Transformer, Computer science, Energy consumption, Artificial intelligence, Software deployment, Efficient energy use, +4 more | https://doi.org/10.1109/cvidl58838.2023.10167219 | The Transformer is a natural language processing model that uses attention mechanisms, originally introduced by Google in 2017. Due to its excellent feature extraction capabilities, researchers in the industry have explored the application of Transformer in computer vision tasks. Following the success of the Vision Transformer in 2020, the Swin Transformer was introduced, which outperforms traditional convolutional neural networks on various benchmarks in the vision field. As a result, Swin Transformer has become a popular backbone for practical production applications. While Swin Transformer excels in accuracy, its large parameter size and high energy consumption pose challenges for deployment at the edge. Addressing this issue is crucial for the wide-scale adoption of Swin Transformer. Current research on energy consumption for Transformer-based models is still in its early stages. To… |
| https://openalex.org/W4391836235 | Structured information extraction from scientific text with large language models | 2024 | Nature Communications | article | 450 | yes | John Dagdelen, Alexander Dunn, Sang‐Hoon Lee, Nicholas Walker, Andrew Rosen, Gerbrand Ceder, Kristin A. Persson, Anubhav Jain | Computer science, Relationship extraction, Information extraction, Task (project management), Information retrieval, JSON, +9 more | https://doi.org/10.1038/s41467-024-45563-x |  |
| https://openalex.org/W3047335959 | A comprehensive review of deep learning applications in hydrology and water resources | 2020 | Water Science & Technology | review | 518 | yes | Muhammed Sit, Bekir Zahit Demiray, Zhongrun Xiang, Gregory J. Ewing, Yusuf Sermet, İbrahim Demir | Water resources, Deep learning, Popularity, Computer science, Corporate governance, Data science, +13 more | https://doi.org/10.2166/wst.2020.369 | Abstract The global volume of digital data is expected to reach 175 zettabytes by 2025. The volume, variety and velocity of water-related data are increasing due to large-scale sensor networks and increased attention to topics such as disaster response, water resources management, and climate change. Combined with the growing availability of computational resources and popularity of deep learning, these data are transformed into actionable and practical knowledge, revolutionizing the water industry. In this article, a systematic review of literature is conducted to identify existing research that incorporates deep learning methods in the water sector, with regard to monitoring, management, governance and communication of water resources. The study provides a comprehensive review of state-of-the-art deep learning approaches used in the water industry for generation, prediction, enhanceme… |
| https://openalex.org/W3080295236 | Fake News Stance Detection Using Deep Learning Architecture (CNN-LSTM) | 2020 | IEEE Access | article | 312 | yes | Muhammad Umer, Zainab Imtiaz, Saleem Ullah, Arif Mehmood, Gyu Sang Choi, Byung-Won On | Computer science, Deep learning, Architecture, Artificial intelligence, Convolutional neural network, Machine learning, +3 more | https://doi.org/10.1109/access.2020.3019735 | Society and individuals are negatively influenced both politically and socially by the widespread increase of fake news either way generated by humans or machines. In the era of social networks, the quick rotation of news makes it challenging to evaluate its reliability promptly. Therefore, automated fake news detection tools have become a crucial requirement. To address the aforementioned issue, a hybrid Neural Network architecture, that combines the capabilities of CNN and LSTM, is used with two different dimensionality reduction approaches, Principle Component Analysis (PCA) and Chi-Square. This work proposed to employ the dimensionality reduction techniques to reduce the dimensionality of the feature vectors before passing them to the classifier. To develop the reasoning, this work acquired a dataset from the Fake News Challenges (FNC) website which has four types of stances: agree,… |
| https://openalex.org/W3082760180 | Transforming machine translation: a deep learning system reaches news translation quality comparable to human professionals | 2020 | Nature Communications | article | 290 | yes | Martin Popel, Markéta Tomková, Jakub Tomek, Łukasz Kaiser, Jakob Uszkoreit, Ondřej Bojar, Zdeněk Žabokrtský | Computer science, Translation (biology), Meaning (existential), Quality (philosophy), Machine translation, Artificial intelligence, +18 more | https://doi.org/10.1038/s41467-020-18073-9 |  |
| https://openalex.org/W3087156149 | Artificial intelligence in COVID-19 drug repurposing | 2020 | The Lancet Digital Health | review | 614 | yes | Yadi Zhou, Fei Wang, Jian Tang, Ruth Nussinov, Feixiong Cheng | Repurposing, Drug repositioning, Timeline, Coronavirus disease 2019 (COVID-19), Computer science, Big data, +15 more | https://doi.org/10.1016/s2589-7500(20)30192-8 |  |
| https://openalex.org/W4220838716 | Protein Language Model Performs Efficient Homology Detection | 2022 |  | preprint | 7 | yes | Mesih Kilinc, Kejue Jia, Robert L. Jernigan | Computer science, UniProt, Representation (politics), Artificial intelligence, Context (archaeology), Language model, +14 more | https://doi.org/10.1101/2022.03.10.483778 | Motivation There are now 225 million sequences in the UniProtKB database, as of January 2022 and 451 million protein sequences in the NCBI non-redundant database. This huge sequence data is ripe for analysis and can be extremely informative about biological function when analyzed with the appropriate methods. Evolutionary information such as the relationship among protein sequences is key to performing sequence analyses. Since sequence matching is one of the primary ways that annotations are found, higher-quality sequence matches yield a larger number of identified homologs. Thus, there is an essential need for a faster and more accurate homolog detection method to process the huge amount of rapidly growing biological sequences. Method Recently, we have seen major improvements in various predictive computational tasks such as structure prediction from the ever-improving artificial intel… |
| https://openalex.org/W4213072767 | RoBERTa-LSTM: A Hybrid Model for Sentiment Analysis With Transformer and Recurrent Neural Network | 2022 | IEEE Access | article | 286 | yes | Kian Long Tan, Chin Poo Lee, Kalaiarasi Sonai Muthu Anbananthen, Kian Ming Lim | Sentiment analysis, Computer science, Word embedding, Artificial intelligence, Transformer, Natural language processing, +11 more | https://doi.org/10.1109/access.2022.3152828 | Due to the rapid development of technology, social media has become more and more common in human daily life. Social media is a platform for people to express their feelings, feedback, and opinions. To understand the sentiment context of the text, sentiment analysis plays the role to determine whether the sentiment of the text is positive, negative, neutral or any other personal feeling. Sentiment analysis is prominent from the perspective of business or politics where it highly impacts the strategic decision making. The challenges of sentiment analysis are attributable to the lexical diversity, imbalanced dataset and long-distance dependencies of the texts. In view of this, a data augmentation technique with GloVe word embedding is leveraged to synthesize more lexically diverse samples by similar word vector replacements. The data augmentation also focuses on the oversampling of the mi… |
| https://openalex.org/W3183845133 | A Review of Graph Neural Networks and Their Applications in Power Systems | 2022 | Journal of Modern Power Systems and Clean Energy | review | 316 | yes | Wenlong Liao, Birgitte Bak‐Jensen, Jayakrishnan Radhakrishna Pillai, Yuelong Wang, Yusen Wang | Artificial neural network, Computer science, Graph, Power (physics), Artificial intelligence, Operations research, +4 more | https://doi.org/10.35833/mpce.2021.000058 | Deep neural networks have revolutionized many machine learning tasks in power systems, ranging from pattern recognition to signal processing. The data in these tasks are typically represented in Euclidean domains. Nevertheless, there is an increasing number of applications in power systems, where data are collected from non-Euclidean domains and represented as graph-structured data with high-dimensional features and interdependency among nodes. The complexity of graph-structured data has brought significant challenges to the existing deep neural networks defined in Euclidean domains. Recently, many publications generalizing deep neural networks for graph-structured data in power systems have emerged. In this paper, a comprehensive overview of graph neural networks (GNNs) in power systems is proposed. Specifically, several classical paradigms of GNN structures, e. g., graph convolutional… |
| https://openalex.org/W3176169354 | ARBERT &amp; MARBERT: Deep Bidirectional Transformers for Arabic | 2021 |  | article | 313 | yes | Muhammad Abdul-Mageed, AbdelRahim Elmadany, El Moatez Billah Nagoudi | Arabic, Transformer, Computer science, Natural language processing, Linguistics, Artificial intelligence, +5 more | https://doi.org/10.18653/v1/2021.acl-long.551 | Muhammad Abdul-Mageed, AbdelRahim Elmadany, El Moatez Billah Nagoudi. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021. |
| https://openalex.org/W2945102109 | Towards Complex Text-to-SQL in Cross-Domain Database with Intermediate Representation | 2019 |  | preprint | 363 | yes | Jiaqi Guo, Zecheng Zhan, Yan Gao, Yan Xiao, Jian–Guang Lou, Ting Liu, Dongmei Zhang | Computer science, SQL, Query by Example, Data definition language, Programming language, Schema (genetic algorithms), +9 more | https://doi.org/10.18653/v1/p19-1444 | We present a neural approach called IRNet for complex and cross-domain Text-to-SQL. IRNet aims to address two challenges: 1) the mismatch between intents expressed in natural language (NL) and the implementation details in SQL; 2) the challenge in predicting columns caused by the large number of out-of-domain words. Instead of end-to-end synthesizing a SQL query, IRNet decomposes the synthesis process into three phases. In the first phase, IRNet performs a schema linking over a question and a database schema. Then, IRNet adopts a grammar-based neural model to synthesize a SemQL query which is an intermediate representation that we design to bridge NL and SQL. Finally, IRNet deterministically infers a SQL query from the synthesized SemQL query with domain knowledge. On the challenging Text-to-SQL benchmark Spider, IRNet achieves 46.7% accuracy, obtaining 19.5% absolute improvement over p… |
| https://openalex.org/W3169629834 | Select, Extract and Generate: Neural Keyphrase Generation with Layer-wise Coverage Attention | 2020 | arXiv (Cornell University) | preprint | 2 | yes | Wasi Uddin Ahmad, Xiao Bai, Soomin Lee, Kai-Wei Chang | Computer science, Transformer, Artificial neural network, Generative grammar, Artificial intelligence, Generator (circuit theory), +14 more | http://arxiv.org/abs/2008.01739 | Natural language processing techniques have demonstrated promising results in keyphrase generation. However, one of the major challenges in \emph{neural} keyphrase generation is processing long documents using deep neural networks. Generally, documents are truncated before given as inputs to neural networks. Consequently, the models may miss essential points conveyed in the target document. To overcome this limitation, we propose \emph{SEG-Net}, a neural keyphrase generation model that is composed of two major components, (1) a selector that selects the salient sentences in a document and (2) an extractor-generator that jointly extracts and generates keyphrases from the selected sentences. SEG-Net uses Transformer, a self-attentive architecture, as the basic building block with a novel \emph{layer-wise} coverage attention to summarize most of the points discussed in the document. The ex… |
| https://openalex.org/W4297325261 | Generative Artificial Intelligence: Trends and Prospects | 2022 | Computer | article | 305 | yes | Mladjan Jovanovic, Mark Campbell | Computer science, Artificial intelligence, Generative grammar, Data science | https://doi.org/10.1109/mc.2022.3192720 | Generative artificial intelligence can make powerful artifacts when used at scale, but developing trust in these artifacts and controlling their creation are essential for user adoption. |
| https://openalex.org/W4286900267 | SSAST: Self-Supervised Audio Spectrogram Transformer | 2021 | arXiv (Cornell University) | preprint | 2 | yes | Yuan Gong, Cheng-I Lai, Yu-An Chung, James Glass | Spectrogram, Computer science, Discriminative model, Speech recognition, Transformer, Artificial intelligence, +8 more | http://arxiv.org/abs/2110.09784 | Recently, neural networks based purely on self-attention, such as the Vision Transformer (ViT), have been shown to outperform deep learning models constructed with convolutional neural networks (CNNs) on various vision tasks, thus extending the success of Transformers, which were originally developed for language processing, to the vision domain. A recent study showed that a similar methodology can also be applied to the audio domain. Specifically, the Audio Spectrogram Transformer (AST) achieves state-of-the-art results on various audio classification benchmarks. However, pure Transformer models tend to require more training data compared to CNNs, and the success of the AST relies on supervised pretraining that requires a large amount of labeled data and a complex training pipeline, thus limiting the practical usage of AST. This paper focuses on audio and speech classification, and aim… |
| https://openalex.org/W4410254428 | Genome language modeling (GLM): a beginner’s cheat sheet | 2025 | Biology Methods and Protocols | article | 4 | yes | Navya Tyagi, Naima Vahab, Sonika Tyagi | Computer science, Artificial intelligence, Genomics, Interoperability, Preprocessor, Data pre-processing, +6 more | https://doi.org/10.1093/biomethods/bpaf022 | Abstract Integrating genomics with diverse data modalities has the potential to revolutionize personalized medicine. However, this integration poses significant challenges due to the fundamental differences in data types and structures. The vast size of the genome necessitates transformation into a condensed representation containing key biomarkers and relevant features to ensure interoperability with other modalities. This commentary explores both conventional and state-of-the-art approaches to genome language modeling (GLM), with a focus on representing and extracting meaningful features from genomic sequences. We focus on the latest trends of applying language modeling techniques on genomics sequence data, treating it as a text modality. Effective feature extraction is essential in enabling machine learning models to effectively analyze large genomic datasets, particularly within mul… |
| https://openalex.org/W4225321635 | MSFT-YOLO: Improved YOLOv5 Based on Transformer for Detecting Defects of Steel Surface | 2022 | Sensors | article | 278 | yes | Zexuan Guo, Chensheng Wang, Guang Yang, Zeyuan Huang, Guo Li | Object detection, Computer science, Artificial intelligence, Detector, Field (mathematics), Transformer, +11 more | https://doi.org/10.3390/s22093467 | With the development of artificial intelligence technology and the popularity of intelligent production projects, intelligent inspection systems have gradually become a hot topic in the industrial field. As a fundamental problem in the field of computer vision, how to achieve object detection in the industry while taking into account the accuracy and real-time detection is an important challenge in the development of intelligent detection systems. The detection of defects on steel surfaces is an important application of object detection in the industry. Correct and fast detection of surface defects can greatly improve productivity and product quality. To this end, this paper introduces the MSFT-YOLO model, which is improved based on the one-stage detector. The MSFT-YOLO model is proposed for the industrial scenario in which the image background interference is great, the defect category… |
| https://openalex.org/W3138815606 | Membership Inference Attacks on Machine Learning: A Survey | 2022 | ACM Computing Surveys | review | 442 | yes | Hongsheng Hu, Zoran Salčić, Lichao Sun, Gillian Dobbie, Philip S. Yu, Xuyun Zhang | Computer science, Inference, Machine learning, Data science, Artificial intelligence, Point (geometry), +5 more | https://doi.org/10.1145/3523273 | Machine learning (ML) models have been widely applied to various applications, including image classification, text generation, audio recognition, and graph data analysis. However, recent studies have shown that ML models are vulnerable to membership inference attacks (MIAs), which aim to infer whether a data record was used to train a target model or not. MIAs on ML models can directly lead to a privacy breach. For example, via identifying the fact that a clinical record that has been used to train a model associated with a certain disease, an attacker can infer that the owner of the clinical record has the disease with a high chance. In recent years, MIAs have been shown to be effective on various ML models, e.g., classification models and generative models. Meanwhile, many defense methods have been proposed to mitigate MIAs. Although MIAs on ML models form a newly emerging and rapidl… |
| https://openalex.org/W3034353423 | Heterogeneous Graph Neural Networks for Extractive Document Summarization | 2020 |  | preprint | 273 | yes | Danqing Wang, Pengfei Liu, Yining Zheng, Xipeng Qiu, Xuanjing Huang | Automatic summarization, Computer science, Sentence, Graph, Granularity, Artificial intelligence, +6 more | https://doi.org/10.18653/v1/2020.acl-main.553 | As a crucial step in extractive document summarization, learning cross-sentence relations has been explored by a plethora of approaches. An intuitive way is to put them in the graph-based neural network, which has a more complex structure for capturing inter-sentence relationships. In this paper, we present a heterogeneous graph-based neural network for extractive summarization (HETERSUMGRAPH), which contains semantic nodes of different granularity levels apart from sentences. These additional nodes act as the intermediary between sentences and enrich the cross-sentence relations. Besides, our graph structure is flexible in natural extension from a single-document setting to multi-document via introducing document nodes. To our knowledge, we are the first one to introduce different types of nodes into graph-based neural networks for extractive document summarization and perform a compre… |
| https://openalex.org/W2985493027 | Probing Contextualized Sentence Representations with Visual Awareness | 2019 | arXiv (Cornell University) | preprint | 2 | yes | Zhuosheng Zhang, Rui Wang, Kehai Chen, Masao Utiyama, Eiichiro Sumita, Hai Zhao | Computer science, Sentence, Machine translation, Natural language processing, Artificial intelligence, Transformer, +9 more | http://arxiv.org/abs/1911.02971 | We present a universal framework to model contextualized sentence representations with visual awareness that is motivated to overcome the shortcomings of the multimodal parallel data with manual annotations. For each sentence, we first retrieve a diversity of images from a shared cross-modal embedding space, which is pre-trained on a large-scale of text-image pairs. Then, the texts and images are respectively encoded by transformer encoder and convolutional neural network. The two sequences of representations are further fused by a simple and effective attention layer. The architecture can be easily applied to text-only natural language processing tasks without manually annotating multimodal parallel corpora. We apply the proposed method on three tasks, including neural machine translation, natural language inference and sequence labeling and experimental results verify the effectivenes… |
| https://openalex.org/W4384027097 | Tools Do Not Create: Human Authorship in the Use of Generative Artificial Intelligence | 2023 | SSRN Electronic Journal | article | 12 | yes | Michael D. Murray | Generative grammar, Computer science, Artificial intelligence | https://doi.org/10.2139/ssrn.4501543 |  |
| https://openalex.org/W2952437275 | Exploring Pre-trained Language Models for Event Extraction and Generation | 2019 |  | article | 334 | yes | Sen Yang, Dawei Feng, Linbo Qiao, Zhigang Kan, Dongsheng Li | Computer science, Event (particle physics), Ranking (information retrieval), Task (project management), Artificial intelligence, Argument (complex analysis), +17 more | https://doi.org/10.18653/v1/p19-1522 | Traditional approaches to the task of ACE event extraction usually depend on manually annotated data, which is often laborious to create and limited in size. Therefore, in addition to the difficulty of event extraction itself, insufficient training data hinders the learning process as well. To promote event extraction, we first propose an event extraction model to overcome the roles overlap problem by separating the argument prediction in terms of roles. Moreover, to address the problem of insufficient training data, we propose a method to automatically generate labeled data by editing prototypes and screen out generated samples by ranking the quality. Experiments on the ACE2005 dataset demonstrate that our extraction model can surpass most existing extraction methods. Besides, incorporating our generation method exhibits further significant improvement. It obtains new state-of-the-art… |
| https://openalex.org/W2978707075 | Predicting materials properties without crystal structure: deep representation learning from stoichiometry | 2020 | Nature Communications | article | 333 | yes | Rhys E. A. Goodall, Alpha A. Lee | Computer science, Stoichiometry, Representation (politics), Block (permutation group theory), Key (lock), Graph, +12 more | https://doi.org/10.1038/s41467-020-19964-7 |  |
| https://openalex.org/W3156955375 | The Transformer Neural Network Architecture for Part-of-Speech Tagging | 2021 |  | article | 2 | no | Artem A. Maksutov, Vladimir I. Zamyatovskiy, Viacheslav O. Morozov, Sviatoslav O. Dmitriev | Computer science, Transformer, Natural language processing, Dependency grammar, Sentence, Artificial intelligence, +21 more | https://doi.org/10.1109/elconrus51938.2021.9396231 | Part-of-speech tagging (POS tagging) is one of the most important tasks in natural language processing. This process implies determining part of speech and assigning an appropriate tag for each word in given sentence. The resulting tag sequence can be used as is and as a part of more complicated tasks, such as dependency and constituency parsing. This task belongs to sequence-to-sequence tasks and multilayer bidirectional LSTM networks are commonly used for POS tagging. Such networks are rather slow in terms of training and processing large amounts of information due to sequential computation of each timestamp from the input sequence. This paper is focused on developing an accurate model for POS tagging that uses the original Transformer neural network architecture. |
| https://openalex.org/W3173688449 | Cross-modal Memory Networks for Radiology Report Generation | 2021 |  | article | 265 | yes | Zhihong Chen, Yaling Shen, Yan Song, Xiang Wan | Computer science, Modal, Volume (thermodynamics), Joint (building), Association (psychology), Chen, +15 more | https://doi.org/10.18653/v1/2021.acl-long.459 | Zhihong Chen, Yaling Shen, Yan Song, Xiang Wan. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021. |
| https://openalex.org/W4387339568 | ChatGPT Isn't Magic | 2023 | M/C Journal | article | 52 | yes | Tama Leaver, Suzanne Srdarov | Generative grammar, MAGIC (telescope), Realm, Variety (cybernetics), Computer science, Artificial intelligence, +6 more | https://doi.org/10.5204/mcj.3004 | Introduction Author Arthur C. Clarke famously argued that in science fiction literature “any sufficiently advanced technology is indistinguishable from magic” (Clarke). On 30 November 2022, technology company OpenAI publicly released their Large Language Model (LLM)-based chatbot ChatGPT (Chat Generative Pre-Trained Transformer), and instantly it was hailed as world-changing. Initial media stories about ChatGPT highlighted the speed with which it generated new material as evidence that this tool might be both genuinely creative and actually intelligent, in both exciting and disturbing ways. Indeed, ChatGPT is part of a larger pool of Generative Artificial Intelligence (AI) tools that can very quickly generate seemingly novel outputs in a variety of media formats based on text prompts written by users. Yet, claims that AI has become sentient, or has even reached a recognisable level of g… |
| https://openalex.org/W2912327653 | Survey on semantic segmentation using deep learning techniques | 2019 | Neurocomputing | article | 548 | yes | Fahad Lateef, Yassine Ruichek | Computer science, Artificial intelligence, Segmentation, Salient, Task (project management), Machine learning, +10 more | https://doi.org/10.1016/j.neucom.2019.02.003 |  |
| https://openalex.org/W3200987621 | Effects of Parameter Norm Growth During Transformer Training: Inductive Bias from Gradient Descent | 2021 | Proceedings of the 2021 Conference on Empirical Methods in Natural Language Pro… | preprint | 1 | yes | William Merrill, Vivek Ramanujan, Yoav Goldberg, Roy Schwartz, Noah A. Smith | Inductive bias, Transformer, Gradient descent, Computer science, Computation, Discretization, +14 more | https://doi.org/10.18653/v1/2021.emnlp-main.133 | The capacity of neural networks like the widely adopted transformer is known to be very high. Evidence is emerging that they learn successfully due to inductive bias in the training routine, typically a variant of gradient descent (GD). To better understand this bias, we study the tendency for transformer parameters to grow in magnitude ($\ell_2$ norm) during training, and its implications for the emergent representations within self attention layers. Empirically, we document norm growth in the training of transformer language models, including T5 during its pretraining. As the parameters grow in magnitude, we prove that the network approximates a discretized network with saturated activation functions. Such "saturated" networks are known to have a reduced capacity compared to the full network family that can be described in terms of formal languages and automata. Our results suggest sa… |
| https://openalex.org/W2971307358 | The Woman Worked as a Babysitter: On Biases in Language Generation | 2019 |  | article | 394 | yes | Emily Sheng, Kai-Wei Chang, Prem Natarajan, Nanyun Peng | Computer science, Joint (building), Natural language processing, Natural language, Natural (archaeology), Artificial intelligence, +6 more | https://doi.org/10.18653/v1/d19-1339 | Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, Nanyun Peng. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019. |
| https://openalex.org/W2949922292 | Entity-Relation Extraction as Multi-Turn Question Answering | 2019 |  | preprint | 344 | yes | Xiaoya Li, Fan Yin, Zijun Sun, Xiayu Li, Arianna Yuan, Duo Chai, Mingxin Zhou, Jiwei Li | Computer science, Question answering, Construct (python library), Task (project management), Dependency (UML), Context (archaeology), +17 more | https://doi.org/10.18653/v1/p19-1129 | In this paper, we propose a new paradigm for the task of entity-relation extraction. We cast the task as a multi-turn question answering problem, i.e., the extraction of entities and elations is transformed to the task of identifying answer spans from the context. This multi-turn QA formalization comes with several key advantages: firstly, the question query encodes important information for the entity/relation class we want to identify; secondly, QA provides a natural way of jointly modeling entity and relation; and thirdly, it allows us to exploit the well developed machine reading comprehension (MRC) models. Experiments on the ACE and the CoNLL04 corpora demonstrate that the proposed paradigm significantly outperforms previous best models. We are able to obtain the state-of-the-art results on all of the ACE04, ACE05 and CoNLL04 datasets, increasing the SOTA results on the three datas… |
| https://openalex.org/W4362519158 | Transformers in Remote Sensing: A Survey | 2023 | Remote Sensing | article | 313 | yes | Abdulaziz Amer Aleissaee, Amandeep Kumar, Rao Muhammad Anwer, Salman Khan, Hisham Cholakkal, Gui-Song Xia, Fahad Shahbaz Khan | Computer science, Remote sensing, Transformer, Hyperspectral imaging, Synthetic aperture radar, Artificial intelligence, +4 more | https://doi.org/10.3390/rs15071860 | Deep learning-based algorithms have seen a massive popularity in different areas of remote sensing image analysis over the past decade. Recently, transformer-based architectures, originally introduced in natural language processing, have pervaded computer vision field where the self-attention mechanism has been utilized as a replacement to the popular convolution operator for capturing long-range dependencies. Inspired by recent advances in computer vision, the remote sensing community has also witnessed an increased exploration of vision transformers for a diverse set of tasks. Although a number of surveys have focused on transformers in computer vision in general, to the best of our knowledge we are the first to present a systematic review of recent advances based on transformers in remote sensing. Our survey covers more than 60 recent transformer-based methods for different remote se… |
| https://openalex.org/W3105238007 | ETC: Encoding Long and Structured Inputs in Transformers | 2020 |  | article | 260 | yes | Joshua Ainslie, Santiago Ontañón, Chris Alberti, Vaclav Cvicek, Zachary Fisher, Philip Pham, Anirudh Ravula, Sumit Sanghai, +2 more | Transformer, Computer science, Encoding (memory), Natural language processing, Art history, Artificial intelligence, +4 more | https://doi.org/10.18653/v1/2020.emnlp-main.19 | Joshua Ainslie, Santiago Ontanon, Chris Alberti, Vaclav Cvicek, Zachary Fisher, Philip Pham, Anirudh Ravula, Sumit Sanghai, Qifan Wang, Li Yang. Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020. |
| https://openalex.org/W3020975691 | Explainable Deep Learning: A Field Guide for the Uninitiated | 2022 | Journal of Artificial Intelligence Research | article | 388 | yes | Gabriëlle Ras, Ning Xie, Marcel van Gerven, Derek Doran | Field (mathematics), Deep learning, Context (archaeology), Artificial intelligence, Computer science, Deep neural networks, +18 more | https://doi.org/10.1613/jair.1.13200 | Deep neural networks (DNNs) are an indispensable machine learning tool despite the difficulty of diagnosing what aspects of a model’s input drive its decisions. In countless real-world domains, from legislation and law enforcement to healthcare, such diagnosis is essential to ensure that DNN decisions are driven by aspects appropriate in the context of its use. The development of methods and studies enabling the explanation of a DNN’s decisions has thus blossomed into an active and broad area of research. The field’s complexity is exacerbated by competing definitions of what it means “to explain” the actions of a DNN and to evaluate an approach’s “ability to explain”. This article offers a field guide to explore the space of explainable deep learning for those in the AI/ML field who are uninitiated. The field guide: i) Introduces three simple dimensions defining the space of foundationa… |
| https://openalex.org/W4285065952 | A Review of Wavelet Analysis and Its Applications: Challenges and Opportunities | 2022 | IEEE Access | review | 437 | yes | Tiantian Guo, Tongpo Zhang, Eng Gee Lim, Miguel López‐Benítez, Fei Ma, Limin Yu | Wavelet, Wavelet transform, Lifting scheme, Computer science, Wavelet packet decomposition, Stationary wavelet transform, +7 more | https://doi.org/10.1109/access.2022.3179517 | As a general and rigid mathematical tool, wavelet theory has found many applications and is constantly developing. This article reviews the development history of wavelet theory, from the construction method to the discussion of wavelet properties. Then it focuses on the design and expansion of wavelet transform. The main models and algorithms of wavelet transform are discussed. The construction of rational wavelet transform (RWT) is provided by examples emphasizing the advantages of RWT over traditional wavelet transform through a review of the literature. The combination of wavelet theory and neural networks is one of the key points of the review. The review covers the evolution of Wavelet Neural Network (WNN), the system architecture and algorithm implementation. The review of the literature indicates the advantages and a clear trend of fast development in WNN that can be combined wi… |
| https://openalex.org/W3034849760 | CH-SIMS: A Chinese Multimodal Sentiment Analysis Dataset with Fine-grained Annotation of Modality | 2020 |  | article | 346 | yes | Wenmeng Yu, Hua Xu, Fanyang Meng, Yilin Zhu, Yixiao Ma, Jiele Wu, Jiyun Zou, Kai-Cheng Yang | Modalities, Computer science, Annotation, Sentiment analysis, Modality (human–computer interaction), Artificial intelligence, +8 more | https://doi.org/10.18653/v1/2020.acl-main.343 | Previous studies in multimodal sentiment analysis have used limited datasets, which only contain unified multimodal annotations. However, the unified annotations do not always reflect the independent sentiment of single modalities and limit the model to capture the difference between modalities. In this paper, we introduce a Chinese single- and multi-modal sentiment analysis dataset, CH-SIMS, which contains 2,281 refined video segments in the wild with both multimodal and independent unimodal annotations. It allows researchers to study the interaction between modalities or use independent unimodal annotations for unimodal sentiment analysis.Furthermore, we propose a multi-task learning framework based on late fusion as the baseline. Extensive experiments on the CH-SIMS show that our methods achieve state-of-the-art performance and learn more distinctive unimodal representations. The ful… |
| https://openalex.org/W4405329954 | Neural Machine Translation Model Using GRU with Hybrid Attention Mechanism for English to Kannada Language | 2024 | Journal of Wireless Mobile Networks Ubiquitous Computing and Dependable Applica… | article | 2 | yes | Gunti Spandan, Prasannavenkatesan Theerthagiri | Machine translation, Computer science, Artificial intelligence, Natural language processing, Transformer, Sentence, +6 more | https://doi.org/10.58346/jowua.2024.i4.017 | Neural machine translation is a machine translation system that uses artificial neural networks to identify nonlinear relationships between bilingual sentence pairs. The language English-Kannada pair has met with less attention in the field of Neural Machine Translation (NMT), mostly due to lack of parallel corpora and the complexity of Kannada's linguistic structure. This research proposes a novel NMT model based on Gated Recurrent Units (GRU) and a Hybrid attention mechanism designed specifically for Kannada's morphological complexity and compared with existing models. By adding language-specific preprocessing techniques and employing data augmentation tactics, our model outperforms previous LSTM, GRU and Bi-LSTM-based algorithms in terms of BLEU and METEOR scores. The proposed model with hybrid attention might be a useful substitute in low-resource settings because transformers and o… |
| https://openalex.org/W4385950788 | An exploration of the causal factors making an online course content popular &amp; engaging | 2023 | International Journal of Information Management Data Insights | article | 6 | yes | Divya Jatain, Vikram Sıngh, Naveen Dahiya | Popularity, Computer science, Word2vec, Sentiment analysis, Artificial intelligence, Convolutional neural network, +16 more | https://doi.org/10.1016/j.jjimei.2023.100194 | An education system consists of imparting knowledge, assessing the learners, collecting feedback, analysing it, and taking proper measures to improve it. All these measures ultimately improve learner satisfaction which gets reflected in the learner performance and makes the course popular. Due to the corona pandemic, educational institutes and universities have shifted their activities online. In such a scenario, it is challenging to identify a course's popularity and the factors that make it popular. In this work, the authors have performed aspect-based sentiment analysis of learner feedback to identify the essential aspects of creating a popular course. The traditional methods for analysing feedback for sentiment analysis require manual intervention and are tedious. The authors have proposed a framework that identifies learner reviews' aspect level sentiment polarity and selected the… |
| https://openalex.org/W4390906246 | A review of graph neural networks: concepts, architectures, techniques, challenges, datasets, applications, and future directions | 2024 | Journal Of Big Data | review | 386 | yes | Bharti Khemani, Shruti Patil, Ketan Kotecha, Sudeep Tanwar | Computer science, Python (programming language), Implementation, Graph, Theoretical computer science, Deep learning, +7 more | https://doi.org/10.1186/s40537-023-00876-4 |  |
| https://openalex.org/W3035642486 | Simplify the Usage of Lexicon in Chinese NER | 2020 |  | preprint | 280 | yes | Ruotian Ma, Minlong Peng, Qi Zhang, Zhongyu Wei, Xuanjing Huang | Computer science, Lexicon, Named-entity recognition, Natural language processing, Artificial intelligence, Benchmark (surveying), +19 more | https://doi.org/10.18653/v1/2020.acl-main.528 | Recently, many works have tried to augment the performance of Chinese named entity recognition (NER) using word lexicons. As a representative, Lattice-LSTM has achieved new benchmark results on several public Chinese NER datasets. However, Lattice-LSTM has a complex model architecture. This limits its application in many industrial areas where real-time NER responses are needed. In this work, we propose a simple but effective method for incorporating the word lexicon into the character representations. This method avoids designing a complicated sequence modeling architecture, and for any neural NER model, it requires only subtle adjustment of the character representation layer to introduce the lexicon information. Experimental studies on four benchmark Chinese NER datasets show that our method achieves an inference speed up to 6.15 times faster than those of state-of-the-art methods, al… |
| https://openalex.org/W3174377922 | Dual-level Collaborative Transformer for Image Captioning | 2021 | Proceedings of the AAAI Conference on Artificial Intelligence | article | 305 | yes | Yunpeng Luo, Jiayi Ji, Xiaoshuai Sun, Liujuan Cao, Yongjian Wu, Feiyue Huang, Chia‐Wen Lin, Rongrong Ji | Closed captioning, Computer science, Transformer, Locality, Grid, Graph, +15 more | https://doi.org/10.1609/aaai.v35i3.16328 | Descriptive region features extracted by object detection networks have played an important role in the recent advancements of image captioning. However, they are still criticized for the lack of contextual information and fine-grained details, which in contrast are the merits of traditional grid features. In this paper, we introduce a novel Dual-Level Collaborative Transformer (DLCT) network to realize the complementary advantages of the two features. Concretely, in DLCT, these two features are first processed by a novel Dual-way Self Attenion (DWSA) to mine their intrinsic properties, where a Comprehensive Relation Attention component is also introduced to embed the geometric information. In addition, we propose a Locality-Constrained Cross Attention module to address the semantic noises caused by the direct fusion of these two features, where a geometric alignment graph is constructe… |
| https://openalex.org/W4386141641 | Generative AI in Medicine and Healthcare: Promises, Opportunities and Challenges | 2023 | Future Internet | article | 342 | yes | Peng Zhang, Maged N. Kamel Boulos | Generative grammar, Computer science, Domain (mathematical analysis), Narrative, Health care, Reliability (semiconductor), +13 more | https://doi.org/10.3390/fi15090286 | Generative AI (artificial intelligence) refers to algorithms and models, such as OpenAI’s ChatGPT, that can be prompted to generate various types of content. In this narrative review, we present a selection of representative examples of generative AI applications in medicine and healthcare. We then briefly discuss some associated issues, such as trust, veracity, clinical safety and reliability, privacy, copyrights, ownership, and opportunities, e.g., AI-driven conversational user interfaces for friendlier human-computer interaction. We conclude that generative AI will play an increasingly important role in medicine and healthcare as it further evolves and gets better tailored to the unique settings and requirements of the medical domain and as the laws, policies and regulatory frameworks surrounding its use start taking shape. |
| https://openalex.org/W3034188538 | Asking and Answering Questions to Evaluate the Factual Consistency of Summaries | 2020 |  | preprint | 307 | yes | Alex Wang, Kyunghyun Cho, Mike Lewis | Automatic summarization, Computer science, Interpretability, Intuition, USable, Consistency (knowledge bases), +10 more | https://doi.org/10.18653/v1/2020.acl-main.450 | Practical applications of abstractive summarization models are limited by frequent factual inconsistencies with respect to their input. Existing automatic evaluation metrics for summarization are largely insensitive to such errors. We propose QAGS (pronounced “kags”), an automatic evaluation protocol that is designed to identify factual inconsistencies in a generated summary. QAGS is based on the intuition that if we ask questions about a summary and its source, we will receive similar answers if the summary is factually consistent with the source. To evaluate QAGS, we collect human judgments of factual consistency on model-generated summaries for the CNN/DailyMail (Hermann et al., 2015) and XSUM (Narayan et al., 2018) summarization datasets. QAGS has substantially higher correlations with these judgments than other automatic evaluation metrics. Also, QAGS offers a natural form of inter… |
| https://openalex.org/W2991568321 | A transformer-based approach to irony and sarcasm detection | 2020 | Neural Computing and Applications | article | 251 | yes | Rolandos Alexandros Potamias, Georgios Siolas, Andreas Stafylopatis | Sarcasm, Computer science, Transformer, Irony, Artificial intelligence, Benchmark (surveying), +19 more | https://doi.org/10.1007/s00521-020-05102-3 |  |
| https://openalex.org/W4221055872 | Wordcraft: Story Writing With Large Language Models | 2022 |  | article | 307 | yes | Ann Yuan, Andy Coenen, Emily Reif, Daphne Ippolito | Computer science, Conversation, Generative grammar, Natural language, Natural language generation, Language model, +13 more | https://doi.org/10.1145/3490099.3511105 | The latest generation of large neural language models such as GPT-3 have achieved new levels of performance on benchmarks for language understanding and generation. These models have even demonstrated an ability to perform arbitrary tasks without explicit training. In this work, we sought to learn how people might use such models in the process of creative writing. We built Wordcraft, a text editor in which users collaborate with a generative language model to write a story. We evaluated Wordcraft with a user study in which participants wrote short stories with and without the tool. Our results show that large language models enable novel co-writing experiences. For example, the language model is able to engage in open-ended conversation about the story, respond to writers’ custom requests expressed in natural language (such as ”rewrite this text to be more Dickensian”), and generate su… |
| https://openalex.org/W4384264663 | Exploring AI Tool's Versatile Responses: An In-depth Analysis Across Different Industries and Its Performance Evaluation | 2023 | arXiv (Cornell University) | preprint | 3 | yes | Hitesh Mohapatra, Soumya Ranjan Mishra | Computer science, Leverage (statistics), Artificial intelligence, Transformer, Artificial neural network, Natural language, +7 more | http://arxiv.org/abs/2307.05909 | AI Tool is a large language model (LLM) designed to generate human-like responses in natural language conversations. It is trained on a massive corpus of text from the internet, which allows it to leverage a broad understanding of language, general knowledge, and various domains. AI Tool can provide information, engage in conversations, assist with tasks, and even offer creative suggestions. The underlying technology behind AI Tool is a transformer neural network. Transformers excel at capturing long-range dependencies in text, making them well-suited for language-related tasks. AI Tool has 175 billion parameters, making it one of the largest and most powerful LLMs to date. This work presents an overview of AI Tool's responses on various sectors of industry. Further, the responses of AI Tool have been cross-verified with human experts in the corresponding fields. To validate the perform… |
| https://openalex.org/W4380319195 | A transformer-based representation-learning model with unified processing of multimodal input for clinical diagnostics | 2023 | Nature Biomedical Engineering | article | 268 | yes | Hong-Yu Zhou, Yizhou Yu, Chengdi Wang, Shu Zhang, Yuanxu Gao, Jia Pan, Jun Shao, Guangming Lu, +2 more | Computer science, Transformer, Representation (politics), Artificial intelligence, Engineering, Voltage, +4 more | https://doi.org/10.1038/s41551-023-01045-x |  |
| https://openalex.org/W2983995706 | KagNet: Knowledge-Aware Graph Networks for Commonsense Reasoning | 2019 |  | article | 467 | yes | Bill Yuchen Lin, Xinyue Chen, Jamin Chen, Xiang Ren | Chen, Computer science, Knowledge graph, Commonsense reasoning, Commonsense knowledge, Graph, +8 more | https://doi.org/10.18653/v1/d19-1282 | Bill Yuchen Lin, Xinyue Chen, Jamin Chen, Xiang Ren. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019. |
| https://openalex.org/W3119385107 | Document Level NMT of Low-Resource Languages with Backtranslation | 2020 |  | article | 2 | yes | Sami Ul Haq, Sadaf Abdul-Rauf, Arsalan Shaukat, Abdullah Saeed | Computer science, Machine translation, Natural language processing, Transformer, Sentence, Artificial intelligence, +10 more | https://doi.org/10.18653/v1/2020.wmt-1.53 | This paper describes our system submission to WMT20 shared task on similar language translation. We examined the use of documentlevel neural machine translation (NMT) systems for low-resource, similar language pair Marathi−Hindi. Our system is an extension of state-of-the-art Transformer architecture with hierarchical attention networks to incorporate contextual information. Since, NMT requires large amount of parallel data which is not available for this task, our approach is focused on utilizing monolingual data with back translation to train our models. Our experiments reveal that document-level NMT can be a reasonable alternative to sentence-level NMT for improving translation quality of low resourced languages even when used with synthetic data. |
| https://openalex.org/W2995742898 | Transfer learning in hybrid classical-quantum neural networks | 2020 | Quantum | article | 342 | yes | Andrea Mari, Thomas R. Bromley, Josh Izaac, Maria Schuld, Nathan Killoran | Quantum machine learning, Computer science, Artificial neural network, Quantum, Artificial intelligence, Quantum circuit, +6 more | https://doi.org/10.22331/q-2020-10-09-340 | We extend the concept of transfer learning, widely applied in modern machine learning algorithms, to the emerging context of hybrid neural networks composed of classical and quantum elements. We propose different implementations of hybrid transfer learning, but we focus mainly on the paradigm in which a pre-trained classical network is modified and augmented by a final variational quantum circuit. This approach is particularly attractive in the current era of intermediate-scale quantum technology since it allows to optimally pre-process high dimensional data (e.g., images) with any state-of-the-art classical network and to embed a select set of highly informative features into a quantum processor. We present several proof-of-concept examples of the convenient application of quantum transfer learning for image recognition and quantum state classification. We use the cross-platform softwa… |
| https://openalex.org/W4313648826 | Survey of Explainable AI Techniques in Healthcare | 2023 | Sensors | review | 464 | yes | Ahmad Chaddad, Jihao Peng, Jian Xu, Ahmed Bouridane | Interpretability, Deep learning, Categorization, Health care, Computer science, Field (mathematics), +7 more | https://doi.org/10.3390/s23020634 | Artificial intelligence (AI) with deep learning models has been widely applied in numerous domains, including medical imaging and healthcare tasks. In the medical field, any judgment or decision is fraught with risk. A doctor will carefully judge whether a patient is sick before forming a reasonable explanation based on the patient’s symptoms and/or an examination. Therefore, to be a viable and accepted tool, AI needs to mimic human judgment and interpretation skills. Specifically, explainable AI (XAI) aims to explain the information behind the black-box model of deep learning that reveals how the decisions are made. This paper provides a survey of the most recent XAI techniques used in healthcare and related medical imaging applications. We summarize and categorize the XAI types, and highlight the algorithms used to increase interpretability in medical imaging topics. In addition, we f… |
| https://openalex.org/W4387925416 | Boon: A Neural Search Engine for Cross-Modal Information Retrieval | 2023 |  | article | 4 | yes | Yan Gong, Georgina Cosma | Computer science, Search engine, Information retrieval, Encoder, Modal, Full text search, +5 more | https://doi.org/10.1145/3606040.3617440 | Visual-Semantic Embedding (VSE) networks can help search engines understand the meaning behind visual content and associate it with relevant textual information, leading to accurate search results. VSE networks can be used in cross-modal search engines to embed image and textual descriptions in a shared space, enabling image-to-text and text-to-image retrieval tasks. However, the full potential of VSE networks for search engines has yet to be fully explored. This paper presents Boon, a novel cross-modal search engine that combines two state-of-the-art networks: the GPT-3.5-turbo large language model, and the VSE network VITR (VIsion Transformers with Relation-focused learning) to enhance the engine's capabilities in extracting and reasoning with regional relationships in images. VITR employs encoders from CLIP that were trained with 400 million image-description pairs and it was fine-tu… |
| https://openalex.org/W2971014768 | A Challenge Dataset and Effective Models for Aspect-Based Sentiment Analysis | 2019 |  | article | 290 | yes | Qingnan Jiang, Lei Chen, Ruifeng Xu, Xiang Ao, Min Yang | Computer science, Chen, Sentiment analysis, Natural language processing, Joint (building), Artificial intelligence, +4 more | https://doi.org/10.18653/v1/d19-1654 | Qingnan Jiang, Lei Chen, Ruifeng Xu, Xiang Ao, Min Yang. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019. |
| https://openalex.org/W2989743967 | SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization | 2019 |  | preprint | 310 | yes | Bogdan Gliwa, Iwona Mochol, Maciej Biesek, Aleksander Wawer | Automatic summarization, Computer science, Natural language processing, Artificial intelligence, Task (project management), Judgement, +8 more | https://doi.org/10.18653/v1/d19-5409 | This paper introduces the SAMSum Corpus, a new dataset with abstractive dialogue summaries. We investigate the challenges it poses for automated summarization by testing several models and comparing their results with those obtained on a corpus of news articles. We show that model-generated summaries of dialogues achieve higher ROUGE scores than the model-generated summaries of news -- in contrast with human evaluators' judgement. This suggests that a challenging task of abstractive dialogue summarization requires dedicated models and non-standard quality measures. To our knowledge, our study is the first attempt to introduce a high-quality chat-dialogues corpus, manually annotated with abstractive summarizations, which can be used by the research community for further studies. |
| https://openalex.org/W4210299703 | Fake news detection based on news content and social contexts: a transformer-based approach | 2022 | International Journal of Data Science and Analytics | article | 252 | yes | Shaina Raza, Chen Ding | Unavailability, Computer science, Exploit, Fake news, Economic shortage, Social media, +15 more | https://doi.org/10.1007/s41060-021-00302-z |  |
| https://openalex.org/W3043869244 | Context-Aware Attentive Knowledge Tracing | 2020 |  | article | 449 | yes | Aritra Ghosh, Neil T. Heffernan, Andrew Lan | Interpretability, Computer science, Artificial intelligence, Context (archaeology), Machine learning, Benchmark (surveying), +14 more | https://doi.org/10.1145/3394486.3403282 | Knowledge tracing (KT) refers to the problem of predicting future learner performance given their past performance in educational applications. Recent developments in KT using flexible deep neural network-based models excel at this task. However, these models often offer limited interpretability, thus making them insufficient for personalized learning, which requires using interpretable feedback and actionable recommendations to help learners achieve better learning outcomes. In this paper, we propose attentive knowledge tracing (AKT), which couples flexible attention-based neural network models with a series of novel, interpretable model components inspired by cognitive and psychometric models. AKT uses a novel monotonic attention mechanism that relates a learner's future responses to assessment questions to their past responses; attention weights are computed using exponential decay a… |
| https://openalex.org/W4296612997 | Explainable machine learning in materials science | 2022 | npj Computational Materials | article | 299 | yes | Xiaoting Zhong, Brian Gallagher, Shusen Liu, Bhavya Kailkhura, Anna M. Hiszpanski, T. Yong-Jin Han | Field (mathematics), Artificial intelligence, Computer science, Context (archaeology), Point (geometry), Artificial neural network, +10 more | https://doi.org/10.1038/s41524-022-00884-7 | Abstract Machine learning models are increasingly used in materials studies because of their exceptional accuracy. However, the most accurate machine learning models are usually difficult to explain. Remedies to this problem lie in explainable artificial intelligence (XAI), an emerging research field that addresses the explainability of complicated machine learning models like deep neural networks (DNNs). This article attempts to provide an entry point to XAI for materials scientists. Concepts are defined to clarify what explain means in the context of materials science. Example works are reviewed to show how XAI helps materials science research. Challenges and opportunities are also discussed. |
| https://openalex.org/W4308234200 | Efficient Fine-Tuning of Deep Neural Networks with Effective Parameter Allocation | 2022 | 2022 IEEE International Conference on Image Processing (ICIP) | article | 1 | no | Phillip Wallis, Xubo Song | Computer science, Artificial neural network, Deep neural networks, Fine-tuning, Artificial intelligence, Physics, +1 more | https://doi.org/10.1109/icip46576.2022.9897314 | It's commonplace in modern deep learning to achieve SOTA performance by fine-tuning a large, pretrained base model. Recent successes in natural language processing, attributed in part to knowledge transfer from large, pretrained, transformer-based language models, have sparked a similar revolution in computer vision via the introduction of Vision Transformers. As modern deep neural networks increase in performance, they also tend to increase in size. Key issues associated with fine-tuning such enormous models include storage overhead, as well as memory and / or latency requirements. Parameter efficient fine-tuning is a fairly recent paradigm which has been evolving alongside massive neural networks in part to address these issues. We showcase the effectiveness of parameter efficient fine-tuning of vision transformers, and introduce a simple yet effective method for learning a non-unifor… |
| https://openalex.org/W4385346108 | ChatGPT for shaping the future of dentistry: the potential of multi-modal large language model | 2023 | International Journal of Oral Science | review | 279 | yes | Hanyao Huang, Ou Zheng, Dongdong Wang, Jiayi Yin, Zijin Wang, Shengxuan Ding, Heng Yin, Chuan Xu, +3 more | Milestone, Computer science, Modal, Artificial intelligence, History, Chemistry, +2 more | https://doi.org/10.1038/s41368-023-00239-y |  |
| https://openalex.org/W2903721568 | Knowledge-Driven Encode, Retrieve, Paraphrase for Medical Image Report Generation | 2019 | Proceedings of the AAAI Conference on Artificial Intelligence | article | 292 | yes | Christy Y. Li, Xiaodan Liang, Zhiting Hu, Eric P. Xing | Computer science, ENCODE, Paraphrase, Natural language processing, Artificial intelligence, Bridging (networking), +13 more | https://doi.org/10.1609/aaai.v33i01.33016666 | Generating long and semantic-coherent reports to describe medical images poses great challenges towards bridging visual and linguistic modalities, incorporating medical domain knowledge, and generating realistic and accurate descriptions. We propose a novel Knowledge-driven Encode, Retrieve, Paraphrase (KERP) approach which reconciles traditional knowledge- and retrieval-based methods with modern learning-based methods for accurate and robust medical report generation. Specifically, KERP decomposes medical report generation into explicit medical abnormality graph learning and subsequent natural language modeling. KERP first employs an Encode module that transforms visual features into a structured abnormality graph by incorporating prior medical knowledge; then a Retrieve module that retrieves text templates based on the detected abnormalities; and lastly, a Paraphrase module that rewri… |
| https://openalex.org/W3125397025 | Incorporating external knowledge into machine learning algorithms for NLP applications | 2020 |  | dissertation | 1 | yes | Pengfei Li | Honor, Privilege (computing), Artificial intelligence, Machine learning, Computer science, Natural language processing, +3 more | https://doi.org/10.32657/10356/144577 | Natural Language Processing (NLP) is a sub-field of Artificial Intelligence (AI) that mainly uses machine learning algorithms to process and analyze large amounts of text data. It gives machines the ability to read, understand, derive meaning from human languages, and potentially generate human languages. The key issue in the modern statistical NLP is text representation learning that transforms unstructured text data into structured numerical representations. A good text representation shall capture important lexical, syntactic, and semantic information for certain NLP tasks, such as keywords and cue phrases, conceptual information, and long-distance dependencies. Traditional Bag-of-Words (BoW) model represents text as a fixed-length vector of words, where each dimension is a numerical value such as frequency or tf-idf weight. However, BoW simply looks at the surface form of words and… |
| https://openalex.org/W3205589518 | SuperShaper: Task-Agnostic Super Pre-training of BERT Models with Variable Hidden Dimensions | 2021 | arXiv (Cornell University) | preprint | 2 | yes | Vinod Ganesan, G. Ramesh, Pratyush Kumar | Computer science, Heuristics, Bottleneck, Transformer, Artificial neural network, Task (project management), +15 more | http://arxiv.org/abs/2110.04711 | Task-agnostic pre-training followed by task-specific fine-tuning is a default approach to train NLU models. Such models need to be deployed on devices across the cloud and the edge with varying resource and accuracy constraints. For a given task, repeating pre-training and fine-tuning across tens of devices is prohibitively expensive. We propose SuperShaper, a task agnostic pre-training approach which simultaneously pre-trains a large number of Transformer models by varying shapes, i.e., by varying the hidden dimensions across layers. This is enabled by a backbone network with linear bottleneck matrices around each Transformer layer which are sliced to generate differently shaped sub-networks. In spite of its simple design space and efficient implementation, SuperShaper discovers networks that effectively trade-off accuracy and model size: Discovered networks are more accurate than a ra… |
| https://openalex.org/W3034743747 | Span-based Localizing Network for Natural Language Video Localization | 2020 |  | article | 267 | yes | Hao Zhang, Aixin Sun, Jing Wei, Joey Tianyi Zhou | Computer science, Benchmark (surveying), Ranking (information retrieval), Matching (statistics), Span (engineering), Task (project management), +11 more | https://doi.org/10.18653/v1/2020.acl-main.585 | Given an untrimmed video and a text query, natural language video localization (NLVL) is to locate a matching span from the video that semantically corresponds to the query. Existing solutions formulate NLVL either as a ranking task and apply multimodal matching architecture, or as a regression task to directly regress the target video span. In this work, we address NLVL task with a span-based QA approach by treating the input video as text passage. We propose a video span localizing network (VSLNet), on top of the standard span-based QA framework, to address NLVL. The proposed VSLNet tackles the differences between NLVL and span-based QA through a simple and yet effective query-guided highlighting (QGH) strategy. The QGH guides VSLNet to search for matching video span within a highlighted region. Through extensive experiments on three benchmark datasets, we show that the proposed VSLNe… |
| https://openalex.org/W3118349318 | Prediction of chemical reaction yields using deep learning | 2021 | Machine Learning Science and Technology | article | 246 | yes | Philippe Schwaller, Alain C. Vaucher, Teodoro Laino, Jean‐Louis Reymond | Computer science, Categorical variable, Artificial intelligence, Machine learning | https://doi.org/10.1088/2632-2153/abc81d | Abstract Artificial intelligence is driving one of the most important revolutions in organic chemistry. Multiple platforms, including tools for reaction prediction and synthesis planning based on machine learning, have successfully become part of the organic chemists’ daily laboratory, assisting in domain-specific synthetic problems. Unlike reaction prediction and retrosynthetic models, the prediction of reaction yields has received less attention in spite of the enormous potential of accurately predicting reaction conversion rates. Reaction yields models, describing the percentage of the reactants converted to the desired products, could guide chemists and help them select high-yielding reactions and score synthesis routes, reducing the number of attempts. So far, yield predictions have been predominantly performed for high-throughput experiments using a categorical (one-hot) encoding… |
| https://openalex.org/W2952768212 | Attention Guided Graph Convolutional Networks for Relation Extraction | 2019 |  | preprint | 487 | yes | Zhijiang Guo, Yan Zhang, Wei Lu | Relationship extraction, Computer science, Leverage (statistics), Dependency (UML), Graph, Artificial intelligence, +12 more | https://doi.org/10.18653/v1/p19-1024 | Dependency trees convey rich structural information that is proven useful for extracting relations among entities in text. However, how to effectively make use of relevant information while ignoring irrelevant information from the dependency trees remains a challenging research question. Existing approaches employing rule based hard-pruning strategies for selecting relevant partial dependency structures may not always yield optimal results. In this work, we propose Attention Guided Graph Convolutional Networks (AGGCNs), a novel model which directly takes full dependency trees as inputs. Our model can be understood as a soft-pruning approach that automatically learns how to selectively attend to the relevant sub-structures useful for the relation extraction task. Extensive results on various tasks including cross-sentence n-ary relation extraction and large-scale sentence-level relation… |
| https://openalex.org/W3104128335 | Challenges in Deploying Machine Learning: A Survey of Case Studies | 2022 | ACM Computing Surveys | review | 495 | yes | Andrei Paleyes, Raoul-Gabriel Urma, Neil D. Lawrence | Software deployment, Workflow, Computer science, Artificial intelligence, Machine learning, Field (mathematics), +9 more | https://doi.org/10.1145/3533378 | In recent years, machine learning has transitioned from a field of academic research interest to a field capable of solving real-world business problems. However, the deployment of machine learning models in production systems can present a number of issues and concerns. This survey reviews published reports of deploying machine learning solutions in a variety of use cases, industries, and applications and extracts practical considerations corresponding to stages of the machine learning deployment workflow. By mapping found challenges to the steps of the machine learning deployment workflow, we show that practitioners face issues at each stage of the deployment process. The goal of this article is to lay out a research agenda to explore approaches addressing these challenges. |
| https://openalex.org/W4289538860 | A Transformer-Based Approach Combining Deep Learning Network and Spatial-Temporal Information for Raw EEG Classification | 2022 | IEEE Transactions on Neural Systems and Rehabilitation Engineering | article | 249 | yes | Jin Xie, J. X. Zhang, Jiayao Sun, Zheng Ma, Liuni Qin, Guanglin Li, Huihui Zhou, Yang Zhan | Computer science, Artificial intelligence, Transformer, Deep learning, Electroencephalography, Pattern recognition (psychology), +7 more | https://doi.org/10.1109/tnsre.2022.3194600 | The attention mechanism of the Transformer has the advantage of extracting feature correlation in the long-sequence data and visualizing the model. As time-series data, the spatial and temporal dependencies of the EEG signals between the time points and the different channels contain important information for accurate classification. So far, Transformer-based approaches have not been widely explored in motor-imagery EEG classification and visualization, especially lacking general models based on cross-individual validation. Taking advantage of the Transformer model and the spatial-temporal characteristics of the EEG signals, we designed Transformer-based models for classifications of motor imagery EEG based on the PhysioNet dataset. With 3s EEG data, our models obtained the best classification accuracy of 83.31%, 74.44%, and 64.22% on two-, three-, and four-class motor-imagery tasks in… |
| https://openalex.org/W4389359039 | Generative artificial intelligence | 2023 | Electronic Markets | article | 324 | yes | Leonardo Banh, Gero Strobel | Generative grammar, Computer science, Artificial intelligence, Field (mathematics), Generative model, Discriminative model, +6 more | https://doi.org/10.1007/s12525-023-00680-1 | Abstract Recent developments in the field of artificial intelligence (AI) have enabled new paradigms of machine processing, shifting from data-driven, discriminative AI tasks toward sophisticated, creative tasks through generative AI. Leveraging deep generative models, generative AI is capable of producing novel and realistic content across a broad spectrum (e.g., texts, images, or programming code) for various domains based on basic user prompts. In this article, we offer a comprehensive overview of the fundamentals of generative AI with its underpinning concepts and prospects. We provide a conceptual introduction to relevant terms and techniques, outline the inherent properties that constitute generative AI, and elaborate on the potentials and challenges. We underline the necessity for researchers and practitioners to comprehend the distinctive characteristics of generative artificial… |
| https://openalex.org/W2952594430 | Neural Architectures for Nested NER through Linearization | 2019 |  | article | 299 | yes | Jana Straková, Milan Straka, Jan Hajič | Computer science, Sequence (biology), Named-entity recognition, Artificial intelligence, Nested loop join, Word (group theory), +22 more | https://doi.org/10.18653/v1/p19-1527 | We propose two neural network architectures for nested named entity recognition (NER), a setting in which named entities may overlap and also be labeled with more than one label. We encode the nested labels using a linearized scheme. In our first proposed approach, the nested labels are modeled as multilabels corresponding to the Cartesian product of the nested labels in a standard LSTM-CRF architecture. In the second one, the nested NER is viewed as a sequence-to-sequence problem, in which the input sequence consists of the tokens and output sequence of the labels, using hard attention on the word whose label is being predicted. The proposed methods outperform the nested NER state of the art on four corpora: ACE-2004, ACE-2005, GENIA and Czech CNEC. We also enrich our architectures with the recently published contextual embeddings: ELMo, BERT and Flair, reaching further improvements fo… |
| https://openalex.org/W4388424695 | Large language models implicitly learn to straighten neural sentence trajectories to construct a predictive representation of natural language | 2023 |  | preprint | 1 | yes | Eghbal A. Hosseini, Evelina Fedorenko | Sentence, Computer science, Language model, Natural language processing, Artificial intelligence, Natural language understanding, +14 more | https://doi.org/10.1101/2023.11.05.564832 | Abstract Predicting upcoming events is critical to our ability to effectively interact with our environment and conspecifics. In natural language processing, transformer models, which are trained on next-word prediction, appear to construct a general-purpose representation of language that can support diverse downstream tasks. However, we still lack an understanding of how a predictive objective shapes such representations. Inspired by recent work in vision neuroscience Hénaff et al. (2019), here we test a hypothesis about predictive representations of autoregressive transformer models. In particular, we test whether the neural trajectory of a sequence of words in a sentence becomes progressively more straight as it passes through the layers of the network. The key insight behind this hypothesis is that straighter trajectories should facilitate prediction via linear extrapolation. We qu… |
| https://openalex.org/W4283701884 | Low-Precision Quantization Techniques for Hardware-Implementation-Friendly BERT Models | 2022 | 2022 23rd International Symposium on Quality Electronic Design (ISQED) | article | 3 | no | Xinpei Zhang, Yi Ding, Mingfei Yu, S. O’uchi, Masahiro Fujita | Quantization (signal processing), Computer science, Encoder, Convolutional neural network, Language model, Transformer, +12 more | https://doi.org/10.1109/isqed54688.2022.9806238 | Transformer-based deep learning models have been widely recognized as highly effective for NLP(natural language processing) tasks, among which BERT(Bidirectional Encoder Representations from Transformers) [1] is achieving outstanding performance on popular benchmarks. However, it is also noticed that the large amount of parameters, along with computational burden, are constraining its implementation on top of hardware platforms with limited computing and memory resources. In this work, having hardware implementation in mind, we introduce and evaluate 2 quantization techniques - clipping, and two piece-wise quantization, which have been implemented in convolutional neural network (CNN) models - to quantize this originally heavy model into the one requiring smaller number of bits. Our experimental results have revealed that, with the implementation of clipping and piece-wise quantization… |
| https://openalex.org/W2977526300 | exBAKE: Automatic Fake News Detection Model Based on Bidirectional Encoder Representations from Transformers (BERT) | 2019 | Applied Sciences | article | 263 | yes | Hee-Jung Jwa, Dongsuk Oh, Kinam Park, Jang Kang, Heuiseok Lim | Headline, Fake news, Computer science, Encoder, Transformer, Autoencoder, +9 more | https://doi.org/10.3390/app9194062 | News currently spreads rapidly through the internet. Because fake news stories are designed to attract readers, they tend to spread faster. For most readers, detecting fake news can be challenging and such readers usually end up believing that the fake news story is fact. Because fake news can be socially problematic, a model that automatically detects such fake news is required. In this paper, we focus on data-driven automatic fake news detection methods. We first apply the Bidirectional Encoder Representations from Transformers model (BERT) model to detect fake news by analyzing the relationship between the headline and the body text of news. To further improve performance, additional news data are gathered and used to pre-train this model. We determine that the deep-contextualizing nature of BERT is best suited for this task and improves the 0.14 F-score over older state-of-the-art m… |
| https://openalex.org/W3185695144 | Exploiting Multilingual Neural Linguistic Representation for Sentiment Classification of Political Tweets in Code-mix Language | 2021 |  | article | 3 | no | Rajkumar Kannan, Sridhar Swaminathan, Chutiporn Anutariya, Vaishnavi Saravanan | Computer science, Tamil, Social media, Sentiment analysis, Artificial intelligence, Natural language processing, +13 more | https://doi.org/10.1145/3468784.3470787 | Social media is more and more utilized by people around the world to express their feelings and opinions in the kind of short text messages. Twitter has been a rapidly growing microblogging social networking website where people express their opinions in a precise and simple manner of expressions. It has also become a platform for governmental, non-governmental and individual opinions and policy announcements. Detecting sentiments from tweets has a wide range of applications including identifying the anxiety or depression of individuals and measuring the well-being or mood of a community. In addition, the sentiment classification becomes complex when the tweet is written in codemix language which is a mix of two different languages. The main objective of this paper is to classify tweets written in mix of Tamil and English language into positive, negative, or neutral. This is achieved by… |
| https://openalex.org/W4391183470 | ABSX: The Chiplet Hyperscale AI Processing Unit for Energy-Efficient High-Performance AI Processing | 2023 |  | article | 3 | no | Young-Su Kwon | Computer science, Efficient energy use, Embedded system, Computer architecture, Energy consumption, Artificial neural network, +8 more | https://doi.org/10.1109/isocc59558.2023.10396520 | Recent advancement of Large Language Model necessitates high-performance AI processors with specialized architecture for Hyperscale Neural Network. A large amount of energy consumed by an array of AI processors not only degrades the effective performance but exacerbates energy shortage. The energy-efficient high-performance AI processing is becoming critical design factor of AI neural processors as it achieves high-performance by highly efficient energy consumption. Chiplet architecture is a viable E <sup xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">2</sup> HPA solution as it optimizes the memory access energy required for processing large matrices in transformers. We present the design of HPU(Hyperscale Processing Unit) on 2.5D chiplet architecture integrating dual Neural Processor chiplets and 8 HBM3 chiplets. The design aspects of HPU main… |
| https://openalex.org/W4360980513 | Welcome to the Era of ChatGPT et al. | 2023 | Business & Information Systems Engineering | article | 313 | yes | Timm Teubner, Christoph M. Flath, Christof Weinhardt, Wil M. P. van der Aalst, Oliver Hinz | Computer science, Transformer, World Wide Web, The Internet, Architecture, Artificial intelligence, +6 more | https://doi.org/10.1007/s12599-023-00795-x | The emergence of Large Language Models (LLMs) in combination with easy-to-use interfaces such as ChatGPT, Bing Chat, and Google's Bard represent both a Herculean task and a sublime opportunity for Business and Information Systems Engineering.The technology and its applications already have considerable impact in many domains directly related to the design, operation, and application of information systems.In this editorial, we seek to explore this new reality as researchers, practitioners, and legislators will -in some form or another -have to deal with it.Our goal is to provide insights and encourage research in this new, exciting, and rapidly developing field. From Foundational Technology towards Killer ApplicationChatGPT emerged as the hottest topic on the Internet at the end of 2022 and established itself as a ''cultural sensation'' (Thorp 2023).This spontaneous hype can be seen as… |
| https://openalex.org/W4378474365 | Winner-Take-All Column Row Sampling for Memory Efficient Adaptation of Language Model | 2023 | arXiv (Cornell University) | preprint | 2 | yes | Zirui Liu, Guanchu Wang, Shaochen Zhong, Zhaozhuo Xu, Daochen Zha, Ruixiang Tang, Zhimeng Jiang, Kaixiong Zhou, +3 more | Bottleneck, Computer science, Estimator, Artificial neural network, Gradient descent, Variance (accounting), +13 more | http://arxiv.org/abs/2305.15265 | With the rapid growth in model size, fine-tuning the large pre-trained language model has become increasingly difficult due to its extensive memory usage. Previous works usually focus on reducing the number of trainable parameters in the network. While the model parameters do contribute to memory usage, the primary memory bottleneck during training arises from storing feature maps, also known as activations, as they are crucial for gradient calculation. Notably, neural networks are usually trained using stochastic gradient descent. We argue that in stochastic optimization, models can handle noisy gradients as long as the gradient estimator is unbiased with reasonable variance. Following this motivation, we propose a new family of unbiased estimators called WTA-CRS, for matrix production with reduced variance, which only requires storing the sub-sampled activations for calculating the gr… |
| https://openalex.org/W3180608480 | A Comprehensive Survey of Privacy-preserving Federated Learning | 2021 | ACM Computing Surveys | review | 532 | yes | Xuefei Yin, Yanming Zhu, Jiankun Hu | Computer science, Federated learning, Information privacy, Data science, Taxonomy (biology), Privacy protection, +6 more | https://doi.org/10.1145/3460427 | The past four years have witnessed the rapid development of federated learning (FL). However, new privacy concerns have also emerged during the aggregation of the distributed intermediate results. The emerging privacy-preserving FL (PPFL) has been heralded as a solution to generic privacy-preserving machine learning. However, the challenge of protecting data privacy while maintaining the data utility through machine learning still remains. In this article, we present a comprehensive and systematic survey on the PPFL based on our proposed 5W-scenario-based taxonomy. We analyze the privacy leakage risks in the FL from five aspects, summarize existing methods, and identify future research directions. |
| https://openalex.org/W3035030897 | FastBERT: a Self-distilling BERT with Adaptive Inference Time | 2020 |  | article | 248 | yes | Weijie Liu, Peng Zhou, Zhiruo Wang, Zhe Zhao, Haotang Deng, Qi Ju | Speedup, Computer science, Inference, Language model, Range (aeronautics), Look-ahead, +9 more | https://doi.org/10.18653/v1/2020.acl-main.537 | Pre-trained language models like BERT have proven to be highly performant. However, they are often computationally expensive in many practical scenarios, for such heavy models can hardly be readily implemented with limited resources. To improve their efficiency with an assured model performance, we propose a novel speed-tunable FastBERT with adaptive inference time. The speed at inference can be flexibly adjusted under varying demands, while redundant calculation of samples is avoided. Moreover, this model adopts a unique self-distillation mechanism at fine-tuning, further enabling a greater computational efficacy with minimal loss in performance. Our model achieves promising results in twelve English and Chinese datasets. It is able to speed up by a wide range from 1 to 12 times than BERT if given different speedup thresholds to make a speed-performance tradeoff. |
| https://openalex.org/W4384115804 | Do Machine Learning Models Produce TypeScript Types That Type Check? | 2023 | DROPS (Schloss Dagstuhl – Leibniz Center for Informatics) | preprint | 4 | yes | Ming‐Ho Yee, Arjun Guha | TypeScript, Computer science, Programming language, JavaScript, Artificial intelligence, Natural language processing, +13 more | https://drops.dagstuhl.de/entities/document/10.4230/LIPIcs.ECOOP.2023.37 | Type migration is the process of adding types to untyped code to gain assurance at compile time. TypeScript and other gradual type systems facilitate type migration by allowing programmers to start with imprecise types and gradually strengthen them. However, adding types is a manual effort and several migrations on large, industry codebases have been reported to have taken several years. In the research community, there has been significant interest in using machine learning to automate TypeScript type migration. Existing machine learning models report a high degree of accuracy in predicting individual TypeScript type annotations. However, in this paper we argue that accuracy can be misleading, and we should address a different question: can an automatic type migration tool produce code that passes the TypeScript type checker? We present TypeWeaver, a TypeScript type migration tool that… |
| https://openalex.org/W4392851477 | Generative AI in healthcare: an implementation science informed translational path on application, integration and governance | 2024 | Implementation Science | review | 334 | yes | Sandeep Reddy | Health care, Health informatics, Generative grammar, Knowledge management, Health administration, Corporate governance, +10 more | https://doi.org/10.1186/s13012-024-01357-9 |  |
| https://openalex.org/W4387867479 | Theories and methods for large-scale brain-inspired neural networks | 2023 | Chinese Science Bulletin (Chinese Version) | article | 2 | yes | Zhengyu Ma, Yonghong Tian | Scale (ratio), Artificial neural network, Cognitive science, Computer science, Artificial intelligence, Neuroscience, +3 more | https://doi.org/10.1360/tb-2023-0775 | <p indent="0mm">The brain’s structure and functions have consistently inspired the development of intelligent technologies throughout human history. Brain-inspired spiking neural networks (SNNs) emulate neural dynamics by incorporating event-based computations to capture temporal information, thereby enabling brain-like and energy-efficient computations. Currently, SNNs have been successfully applied in various fields, such as image recognition and object detection, and have the potential to revolutionize the field of artificial intelligence. Similar to the brain, artificial models show emergent properties, such as high performance and intelligence, with sufficiently large size. However, the sizes of SNNs have been limited, which hinders potential performance improvements and thus their real-world application. Therefore, methods and theories for constructing large-scale SNNs must be dev… |
| https://openalex.org/W4385469325 | A Survey on ChatGPT: AI–Generated Contents, Challenges, and Solutions | 2023 | IEEE Open Journal of the Computer Society | article | 277 | yes | Yuntao Wang, Yanghe Pan, Miao Yan, Zhou Su, Tom H. Luan | Pace, Computer science, Key (lock), Data science, Generative grammar, Best practice, +6 more | https://doi.org/10.1109/ojcs.2023.3300321 | With the widespread use of large artificial intelligence (AI) models such as ChatGPT, AI-generated content (AIGC) has garnered increasing attention and is leading a paradigm shift in content creation and knowledge representation. AIGC uses generative large AI algorithms to assist or replace humans in creating massive, high-quality, and human-like content at a faster pace and lower cost, based on user-provided prompts. Despite the recent significant progress in AIGC, security, privacy, ethical, and legal challenges still need to be addressed. This paper presents an in-depth survey of working principles, security and privacy threats, state-of-the-art solutions, and future challenges of the AIGC paradigm. Specifically, we first explore the enabling technologies, general architecture of AIGC, and discuss its working modes and key characteristics. Then, we investigate the taxonomy of securit… |
| https://openalex.org/W3093031973 | Fake News Detection Using Machine Learning Ensemble Methods | 2020 | Complexity | article | 444 | yes | Iftikhar Ahmad, Muhammad Murtaza Yousaf, Suhail Yousaf, Muhammad Ovais Ahmad | Computer science, Misinformation, Machine learning, Social media, Disinformation, Ensemble learning, +14 more | https://doi.org/10.1155/2020/8885861 | The advent of the World Wide Web and the rapid adoption of social media platforms (such as Facebook and Twitter) paved the way for information dissemination that has never been witnessed in the human history before. With the current usage of social media platforms, consumers are creating and sharing more information than ever before, some of which are misleading with no relevance to reality. Automated classification of a text article as misinformation or disinformation is a challenging task. Even an expert in a particular domain has to explore multiple aspects before giving a verdict on the truthfulness of an article. In this work, we propose to use machine learning ensemble approach for automated classification of news articles. Our study explores different textual properties that can be used to distinguish fake contents from real. By using those properties, we train a combination of d… |
| https://openalex.org/W4296027312 | Expert-level detection of pathologies from unannotated chest X-ray images via self-supervised learning | 2022 | Nature Biomedical Engineering | article | 331 | yes | Ekin Tiu, Ellie Talius, Pujan R. Patel, Curtis P. Langlotz, Andrew Y. Ng, Pranav Rajpurkar | Computer science, Artificial intelligence, Machine learning, Medical imaging, Training set, Pattern recognition (psychology), +2 more | https://doi.org/10.1038/s41551-022-00936-9 |  |
| https://openalex.org/W3030978062 | Transformer-CNN: Swiss knife for QSAR modeling and interpretation | 2020 | Journal of Cheminformatics | article | 250 | yes | Pavel Karpov, Guillaume Godin, Igor V. Tetko | Quantitative structure–activity relationship, Computer science, Transformer, Inference, Artificial intelligence, Benchmark (surveying), +12 more | https://doi.org/10.1186/s13321-020-00423-w |  |
| https://openalex.org/W3005071803 | SSE-PT: Sequential Recommendation Via Personalized Transformer | 2020 |  | article | 239 | yes | Liwei Wu, Shuqing Li, Cho‐Jui Hsieh, James Sharpnack | Computer science, Personalization, Transformer, Regularization (linguistics), Artificial intelligence, Language model, +7 more | https://doi.org/10.1145/3383313.3412258 | Temporal information is crucial for recommendation problems because user preferences are naturally dynamic in the real world. Recent advances in deep learning, especially the discovery of various attention mechanisms and newer architectures in addition to widely used RNN and CNN in natural language processing, have allowed for better use of the temporal ordering of items that each user has engaged with. In particular, the SASRec model, inspired by the popular Transformer model in natural languages processing, has achieved state-of-the-art results. However, SASRec, just like the original Transformer model, is inherently an un-personalized model and does not include personalized user embeddings. To overcome this limitation, we propose a Personalized Transformer (SSE-PT) model, outperforming SASRec by almost 5% in terms of [email protected] on 5 real-world datasets. Furthermore, after exam… |
| https://openalex.org/W4285101952 | Utilizing Text-based Augmentation to Enhance Video Captioning | 2022 |  | article | 2 | no | Shanhao Li, Bang Yang, Yuexian Zou | Closed captioning, Computer science, Transformer, Machine translation, Language model, Visualization, +12 more | https://doi.org/10.1109/icaibd55127.2022.9820499 | Video captioning (VC) is a challenging cross-modality task that requires the model to capture the visual information in the video and to automatically generate the captions accordingly. Literature shows that Transformer-based deep neural networks (DNN) achieve the state-of-arts. Without exception, such DNN models are data-hungry, which hinders the development of the VC models since large-scale VC training datasets need to pay a much higher cost to build compared to the datasets for other tasks such as image recognition or neural machine translation. As a result, data augmentation is a valuable approach to improving the performance of VC models. In this work, we propose two text-based augmentation methods to enlarge the scale of VC datasets, so as to develop better VC models. Our basic ideas lie that when a video is given, a different person may give different descriptions, which leads t… |
| https://openalex.org/W4384517815 | Automatic ICD-10 Code Association: A Challenging Task on French Clinical Texts | 2023 |  | article | 2 | no | Yakini Tchouka, Jean-François Couchot, David Laiymani, Philippe Selles, Azzedine Rahmani | Computer science, Natural language processing, Artificial intelligence, Metric (unit), Code (set theory), Association (psychology), +17 more | http://dx.doi.org/10.1109/cbms58004.2023.00198 | Automatically associating ICD codes with electronic health data is a well-known NLP task in medical research. NLP has evolved significantly in recent years with the emergence of pre-trained language models based on Transformers architecture, mainly in the English language. This paper adapts these models to automatically associate the ICD codes. Several neural network architectures have been experimented with to address the challenges of dealing with a large set of both input tokens and labels to be guessed. In this paper, we propose a model that combines the latest advances in NLP and multi-label classification for ICD-10 code association. Fair experiments on a Clinical dataset in the French language show that our approach increases the <tex xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">$F_{1}$</tex> -score metric by more than 55% compared to… |
| https://openalex.org/W3126753370 | Predictive Attention Transformer: Improving Transformer with Attention Map Prediction | 2021 |  | article | 2 | no | Yujing Wang, Yaming Yang, Jiangang Bai, Mingliang Zhang, Jing Bai, Jing Yu, Ce Zhang, Yunhai Tong | Transformer, Computer science, ENCODE, Artificial intelligence, Convolutional neural network, Machine learning, +6 more |  | Transformer is a ubiquitous model for natural language processing and has also attracted wide attentions in other domains such as computer vision. The self-attention maps, learned independently for each layer, are indispensable for a transformer model to encode the dependencies among input tokens, however, learning them effectively is still a challenging problem. In this paper, we address this problem and propose a novel approach to improve self-attention through supplementary prediction modules. The underlying assumption is that the attention structures in the current layer should not be completely independent from those in the previous layer. Instead, we model their dependencies via a chain of prediction models that take previous attention maps as input to predict the attention maps of a new layer through convolutional neural networks. Specifically, we propose Predictive Attention Tra… |
| https://openalex.org/W3154435685 | On Interpretability of Artificial Neural Networks: A Survey | 2021 | IEEE Transactions on Radiation and Plasma Medical Sciences | article | 465 | yes | Fenglei Fan, Jinjun Xiong, Mengzhou Li, Ge Wang | Interpretability, Artificial intelligence, Artificial neural network, Deep learning, Computer science, Black box, +4 more | https://doi.org/10.1109/trpms.2021.3066428 | Deep learning as represented by the artificial deep neural networks (DNNs) has achieved great success recently in many important areas that deal with text, images, videos, graphs, and so on. However, the black-box nature of DNNs has become one of the primary obstacles for their wide adoption in mission-critical applications such as medical diagnosis and therapy. Because of the huge potentials of deep learning, increasing the interpretability of deep neural networks has recently attracted much research attention. In this paper, we propose a simple but comprehensive taxonomy for interpretability, systematically review recent studies in improving interpretability of neural networks, describe applications of interpretability in medicine, and discuss possible future research directions of interpretability, such as in relation to fuzzy logic and brain science. |
| https://openalex.org/W3082928416 | OPUS-MT – Building open translation services for the World | 2020 | Työväentutkimus Vuosikirja | article | 293 | yes | Jörg Tiedemann, Santhosh Thottingal | Machine translation, Computer science, Implementation, Opus, Open source, Translation (biology), +11 more | http://hdl.handle.net/10138/327852 | This paper presents OPUS-MT a project that focuses on the development of free resources and tools for machine translation. The current status is a repository of over 1,000 pre-trained neural machine translation models that are ready to be launched in on-line translation services. For this we also provide open source implementations of web applications that can run efficiently on average desktop hardware with a straightforward setup and installation. |
| https://openalex.org/W4399665770 | Prompting-to-Distill Semantic Knowledge for Few-Shot Learning | 2024 | IEEE Geoscience and Remote Sensing Letters | article | 3 | yes | Hong Ji, Zhi Gao, Jinchang Ren, Xing-ao Wang, Tianyi Gao, Wenbo Sun, Ping Ma | Computer science, Shot (pellet), One shot, Artificial intelligence, Natural language processing, Semantics (computer science), +6 more | https://rgu-repository.worktribe.com/output/2378315 | Recognizing visual patterns in low-data regime necessitates deep neural networks to glean generalized representations from limited training samples. In this paper, we propose a novel few-shot classification method, namely ProDFSL, leveraging multi-modal knowledge and attention mechanism. We are inspired by recent advances of large language models and the great potential they have shown across a wide range of downstream tasks, and tailor it to benefit the remote sensing community. We utilize ChatGPT to produce class-specific textual inputs for enabling CLIP with rich semantic information. To promote the adaptation of CLIP in remote sensing domain, we introduce a Cross-modal Knowledge Generation Module, which dynamically generates a group of soft prompts conditioned on the few-shot visual samples and further uses a shallow Transformer to model the dependencies between language sequences.… |
| https://openalex.org/W4312910656 | Efficient Acceleration of Deep Learning Inference on Resource-Constrained Edge Devices: A Review | 2022 | Proceedings of the IEEE | review | 272 | yes | Md Maruf Hossain Shuvo, Syed K. Islam, Jianlin Cheng, Bashir I. Morshed | Computer science, Edge device, Cloud computing, Edge computing, Software deployment, Deep learning, +9 more | https://doi.org/10.1109/jproc.2022.3226481 | © 1963-2012 IEEE. cc-by |
| https://openalex.org/W4388694364 | Illuminating protein space with a programmable generative model | 2023 | Nature | article | 342 | yes | John Ingraham, Max Baranov, Zak Costello, Karl W. Barber, Wujie Wang, Ahmed Ismail, Vincent Frappier, Dana M. Lord, +18 more | Generative model, Computer science, Protein crystallization, Synthetic biology, Protein engineering, Computation, +12 more | https://doi.org/10.1038/s41586-023-06728-8 | Abstract Three billion years of evolution has produced a tremendous diversity of protein molecules 1, but the full potential of proteins is likely to be much greater. Accessing this potential has been challenging for both computation and experiments because the space of possible protein molecules is much larger than the space of those likely to have functions. Here we introduce Chroma, a generative model for proteins and protein complexes that can directly sample novel protein structures and sequences, and that can be conditioned to steer the generative process towards desired properties and functions. To enable this, we introduce a diffusion process that respects the conformational statistics of polymer ensembles, an efficient neural architecture for molecular systems that enables long-range reasoning with sub-quadratic scaling, layers for efficiently synthesizing three-dimensional str… |
| https://openalex.org/W4413920635 | A review on deep learning for vision-based hand detection, hand segmentation and hand gesture recognition in human–robot interaction | 2025 | Robotics and Computer-Integrated Manufacturing | article | 3 | yes | Reza Jalayer, Masoud Jalayer, Carlotta Orsenigo, Masayoshi Tomizuka | Artificial intelligence, Segmentation, Computer science, Computer vision, Gesture, Deep learning, +4 more | https://doi.org/10.1016/j.rcim.2025.103110 |  |
| https://openalex.org/W4362706625 | Automatic ICD-10 Code Association: A Challenging Task on French Clinical Texts | 2023 | arXiv (Cornell University) | preprint | 1 | yes | Yakini Tchouka, Jean-François Couchot, David Laiymani, Philippe Selles, Azzedine Rahmani | Computer science, Natural language processing, Artificial intelligence, Transformer, Metric (unit), Association (psychology), +16 more | http://arxiv.org/abs/2304.02886 | Automatically associating ICD codes with electronic health data is a well-known NLP task in medical research. NLP has evolved significantly in recent years with the emergence of pre-trained language models based on Transformers architecture, mainly in the English language. This paper adapts these models to automatically associate the ICD codes. Several neural network architectures have been experimented with to address the challenges of dealing with a large set of both input tokens and labels to be guessed. In this paper, we propose a model that combines the latest advances in NLP and multi-label classification for ICD-10 code association. Fair experiments on a Clinical dataset in the French language show that our approach increases the $F_1$-score metric by more than 55\% compared to state-of-the-art results. |
| https://openalex.org/W4283315029 | Pre-training Enhanced Spatial-temporal Graph Neural Network for Multivariate Time Series Forecasting | 2022 | Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data M… | article | 234 | yes | Zezhi Shao, Zhao Zhang, Fei Wang, Yongjun Xu | Computer science, Time series, Scalability, Multivariate statistics, Graph, Series (stratigraphy), +9 more | https://doi.org/10.1145/3534678.3539396 | Multivariate Time Series (MTS) forecasting plays a vital role in a wide range\nof applications. Recently, Spatial-Temporal Graph Neural Networks (STGNNs) have\nbecome increasingly popular MTS forecasting methods. STGNNs jointly model the\nspatial and temporal patterns of MTS through graph neural networks and\nsequential models, significantly improving the prediction accuracy. But limited\nby model complexity, most STGNNs only consider short-term historical MTS data,\nsuch as data over the past one hour. However, the patterns of time series and\nthe dependencies between them (i.e., the temporal and spatial patterns) need to\nbe analyzed based on long-term historical MTS data. To address this issue, we\npropose a novel framework, in which STGNN is Enhanced by a scalable time series\nPre-training model (STEP). Specifically, we design a pre-training model to\nefficiently learn temporal patt… |
| https://openalex.org/W4309490745 | Deep learning in drug discovery: an integrative review and future challenges | 2022 | Artificial Intelligence Review | article | 330 | yes | Heba Askr, Enas Elgeldawi, Heba Aboul Ella, Yaseen A. M. M. Elshaier, Mamdouh M. Gomaa, Aboul Ella Hassanien | Drug discovery, Computer science, Drug, Data science, Benchmark (surveying), Drug repositioning, +7 more | https://doi.org/10.1007/s10462-022-10306-1 | Abstract Recently, using artificial intelligence (AI) in drug discovery has received much attention since it significantly shortens the time and cost of developing new drugs. Deep learning (DL)-based approaches are increasingly being used in all stages of drug development as DL technology advances, and drug-related data grows. Therefore, this paper presents a systematic Literature review (SLR) that integrates the recent DL technologies and applications in drug discovery Including, drug–target interactions (DTIs), drug–drug similarity interactions (DDIs), drug sensitivity and responsiveness, and drug-side effect predictions. We present a review of more than 300 articles between 2000 and 2022. The benchmark data sets, the databases, and the evaluation measures are also presented. In addition, this paper provides an overview of how explainable AI (XAI) supports drug discovery problems. The… |
| https://openalex.org/W3013437827 | Developing an online hate classifier for multiple social media platforms | 2020 | Human-centric Computing and Information Sciences | article | 282 | yes | Joni Salminen, Maximilian Hopf, Shammur Absar Chowdhury, Soon-Gyo Jung, Hind Almerekhi, Bernard J. Jansen | Computer science, Social media, Generalizability theory, Machine learning, Support vector machine, Classifier (UML), +7 more | https://doi.org/10.1186/s13673-019-0205-6 |  |
| https://openalex.org/W3175552668 | Directed Acyclic Graph Network for Conversational Emotion Recognition | 2021 |  | article | 257 | yes | Weizhou Shen, Siyue Wu, Yunyi Yang, Xiaojun Quan | Computer science, Directed acyclic graph, Natural language processing, Joint (building), Computational linguistics, Emotion recognition, +10 more | https://doi.org/10.18653/v1/2021.acl-long.123 | Weizhou Shen, Siyue Wu, Yunyi Yang, Xiaojun Quan. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021. |
| https://openalex.org/W3088999551 | Transfer learning enables the molecular transformer to predict regio- and stereoselective reactions on carbohydrates | 2020 | Nature Communications | article | 225 | yes | Giorgio Pesciullesi, Philippe Schwaller, Teodoro Laino, Jean‐Louis Reymond | Stereoselectivity, Regioselectivity, Transfer of learning, Deep learning, Chemistry, Transformer, +10 more | https://doi.org/10.1038/s41467-020-18671-7 | Abstract Organic synthesis methodology enables the synthesis of complex molecules and materials used in all fields of science and technology and represents a vast body of accumulated knowledge optimally suited for deep learning. While most organic reactions involve distinct functional groups and can readily be learned by deep learning models and chemists alike, regio- and stereoselective transformations are more challenging because their outcome also depends on functional group surroundings. Here, we challenge the Molecular Transformer model to predict reactions on carbohydrates where regio- and stereoselectivity are notoriously difficult to predict. We show that transfer learning of the general patent reaction model with a small set of carbohydrate reactions produces a specialized model returning predictions for carbohydrate reactions with remarkable accuracy. We validate these predict… |
| https://openalex.org/W4389518760 | GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints | 2023 |  | article | 254 | yes | Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, Sumit Sanghai | Computer science, Inference, Generalization, Query optimization, Query expansion, Artificial intelligence, +4 more | https://doi.org/10.18653/v1/2023.emnlp-main.298 | Multi-query attention (MQA), which only uses a single key-value head, drastically speeds up decoder inference. However, MQA can lead to quality degradation, and moreover it may not be desirable to train a separate model just for faster inference. We (1) propose a recipe for uptraining existing multi-head language model checkpoints into models with MQA using 5% of original pre-training compute, and (2) introduce grouped-query attention (GQA), a generalization of multi-query attention which uses an intermediate (more than one, less than number of query heads) number of key-value heads. We show that uptrained GQA achieves quality close to multi-head attention with comparable speed to MQA. |
| https://openalex.org/W4281702997 | Cross-modal Clinical Graph Transformer for Ophthalmic Report Generation | 2022 | arXiv (Cornell University) | preprint | 3 | yes | Mingjie Li, Wenjia Cai, Karin Verspoor, Shirui Pan, Xiaodan Liang, Xiaojun Chang | Computer science, Transformer, Knowledge base, Natural language processing, Artificial intelligence, Relationship extraction, +9 more | http://arxiv.org/abs/2206.01988 | Automatic generation of ophthalmic reports using data-driven neural networks has great potential in clinical practice. When writing a report, ophthalmologists make inferences with prior clinical knowledge. This knowledge has been neglected in prior medical report generation methods. To endow models with the capability of incorporating expert knowledge, we propose a Cross-modal clinical Graph Transformer (CGT) for ophthalmic report generation (ORG), in which clinical relation triples are injected into the visual features as prior knowledge to drive the decoding procedure. However, two major common Knowledge Noise (KN) issues may affect models' effectiveness. 1) Existing general biomedical knowledge bases such as the UMLS may not align meaningfully to the specific context and language of the report, limiting their utility for knowledge injection. 2) Incorporating too much knowledge may di… |
| https://openalex.org/W3092468804 | Improving Unsupervised Neural Machine Translation with Dependency Relationships | 2020 | Lecture notes in computer science | book-chapter | 2 | no | Jia Xu, Na Ye, Guangping Zhang | Computer science, Machine translation, Artificial intelligence, Natural language processing, Dependency grammar, Parsing, +7 more | https://doi.org/10.1007/978-3-030-60450-9_34 |  |
| https://openalex.org/W4391228459 | High-performing Multi-task Model of Urinary Tract Dilation (UTD) Classification for Neonatal Ultrasound Reports Through Natural Language Processing | 2024 |  | preprint | 4 | yes | Yining Hua, Anudeep Mukkamala, Carlos R. Estrada, Michael Lingzhi Li, Hsin‐Hsiao Scott Wang | Encoder, Computer science, Artificial intelligence, Medicine, Ultrasound, Task group, +11 more | https://doi.org/10.1101/2024.01.23.24301680 | ABSTRACT Objective The urinary tract dilation (UTD) classification system provides objective assessment relevant to hydronephrosis management for children. However, the lack of uniform language regarding UTD in radiology reports leads to significant difficulty in both clinical management and research. We seek to develop a unified multi-task/multi-class model that can effectively extract UTD components and classifications from early postnatal ultrasound (US) reports. Methods Radiology records from our institution were reviewed to identify infants aged 0-90 days undergoing early ultrasound for antenatal UTD. The report and images were reviewed by the study team to create the ground truth of UTD classification and components (primary outcome). Bio_ClinicalBERT, a variant of the Bidirectional Encoder Representations from Transformers (BERT) model, was used as the embedding layers of the cla… |
| https://openalex.org/W4391403992 | Generative artificial intelligence in supply chain and operations management: a capability-based framework for analysis and implementation | 2024 | International Journal of Production Research | article | 256 | yes | Ilya Jackson, Dmitry Ivanov, Alexandre Dolgui, Jafar Namdar | Supply chain, Context (archaeology), Supply chain management, Process management, Computer science, Transformative learning, +13 more | https://doi.org/10.1080/00207543.2024.2309309 | International audience |
| https://openalex.org/W4392669753 | A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity | 2023 |  | article | 573 | yes | Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, +5 more | Interactivity, Computer science, Natural language processing, Multimodal interaction, Multimodal therapy, Cognitive psychology, +5 more | https://doi.org/10.18653/v1/2023.ijcnlp-main.45 | This paper proposes a framework for quantitatively evaluating interactive LLMs such as ChatGPT using publicly available data sets, using 23 data sets covering 8 different common NLP application tasks. We extensively evaluate the multitask, multilingual, and multi-modal aspects of ChatGPT based on these data sets and a newly designed multimodal dataset. We find that ChatGPT outperforms LLMs with zero-shot learning on most tasks and even outperforms fine-tuned models on some tasks. We find that it is better at understanding non-Latin script languages than generating them. It is able to generate multimodal content from textual prompts via an intermediate code generation step. Moreover, we find that ChatGPT is 63.41% accurate on average in 10 different reasoning categories under logical reasoning, non-textual reasoning, and commonsense reasoning, hence making it an unreliable reasoner. Chat… |
| https://openalex.org/W4281643269 | Towards artificial general intelligence via a multimodal foundation model | 2022 | Nature Communications | article | 241 | yes | Nanyi Fei, Zhiwu Lu, Yizhao Gao, Guoxing Yang, Yuqi Huo, Jingyuan Wen, Haoyu Lu, Ruihua Song, +4 more | Computer science, Interpretability, Foundation (evidence), Artificial intelligence, Transformative learning, Cognition, +7 more | https://doi.org/10.1038/s41467-022-30761-2 | Abstract The fundamental goal of artificial intelligence (AI) is to mimic the core cognitive activities of human. Despite tremendous success in the AI research, most of existing methods have only single-cognitive ability. To overcome this limitation and take a solid step towards artificial general intelligence (AGI), we develop a foundation model pre-trained with huge multimodal data, which can be quickly adapted for various downstream cognitive tasks. To achieve this goal, we propose to pre-train our foundation model by self-supervised learning with weak semantic correlation data crawled from the Internet and show that promising results can be obtained on a wide range of downstream tasks. Particularly, with the developed model-interpretability tools, we demonstrate that strong imagination ability is now possessed by our foundation model. We believe that our work makes a transformative… |
| https://openalex.org/W2963823140 | Robust Neural Machine Translation with Doubly Adversarial Inputs | 2019 |  | preprint | 235 | yes | Yong Cheng, Lu Jiang, Wolfgang Macherey | Adversarial system, Machine translation, Computer science, Robustness (evolution), Transformer, Artificial intelligence, +9 more | https://doi.org/10.18653/v1/p19-1425 | Neural machine translation (NMT) often suffers from the vulnerability to noisy perturbations in the input. We propose an approach to improving the robustness of NMT models, which consists of two parts: (1) attack the translation model with adversarial source examples; (2) defend the translation model with adversarial target inputs to improve its robustness against the adversarial source inputs. For the generation of adversarial inputs, we propose a gradient-based method to craft adversarial examples informed by the translation loss over the clean inputs. Experimental results on Chinese-English and English-German translation tasks demonstrate that our approach achieves significant improvements (2.8 and 1.6 BLEU points) over Transformer on standard clean benchmarks as well as exhibiting higher robustness on noisy data. |
| https://openalex.org/W2970780738 | Cosmos QA: Machine Reading Comprehension with Contextual Commonsense Reasoning | 2019 |  | article | 276 | yes | Lifu Huang, Ronan Le Bras, Chandra Bhagavatula, Yejin Choi | Computer science, Natural language processing, Commonsense reasoning, Commonsense knowledge, Artificial intelligence, Question answering, +8 more | https://doi.org/10.18653/v1/d19-1243 | Lifu Huang, Ronan Le Bras, Chandra Bhagavatula, Yejin Choi. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019. |
| https://openalex.org/W4221153690 | LinkBERT: Pretraining Language Models with Document Links | 2022 | Proceedings of the 60th Annual Meeting of the Association for Computational Lin… | article | 272 | yes | Michihiro Yasunaga, Jure Leskovec, Percy Liang | Computer science, Hyperlink, Language model, Natural language processing, Artificial intelligence, Information retrieval, +10 more | https://doi.org/10.18653/v1/2022.acl-long.551 | Language model (LM) pretraining captures various knowledge from text corpora, helping downstream tasks. However, existing methods such as BERT model a single document, and do not capture dependencies or knowledge that span across documents. In this work, we propose LinkBERT, an LM pretraining method that leverages links between documents, e.g., hyperlinks. Given a text corpus, we view it as a graph of documents and create LM inputs by placing linked documents in the same context. We then pretrain the LM with two joint self-supervised objectives: masked language modeling and our new proposal, document relation prediction. We show that LinkBERT outperforms BERT on various downstream tasks across two domains: the general domain (pretrained on Wikipedia with hyperlinks) and biomedical domain (pretrained on PubMed with citation links). LinkBERT is especially effective for multi-hop reasoning… |
| https://openalex.org/W3134255974 | Split Computing and Early Exiting for Deep Learning Applications: Survey and Research Challenges | 2022 | ACM Computing Surveys | review | 238 | yes | Yoshitomo Matsubara, Marco Levorato, Francesco Restuccia | Computer science, Mobile device, Cloud computing, Inference, Deep learning, Edge device, +11 more | https://doi.org/10.1145/3527155 | Mobile devices such as smartphones and autonomous vehicles increasingly rely on deep neural networks (DNNs) to execute complex inference tasks such as image classification and speech recognition, among others. However, continuously executing the entire DNN on mobile devices can quickly deplete their battery. Although task offloading to cloud/edge servers may decrease the mobile device’s computational burden, erratic patterns in channel quality, network, and edge server load can lead to a significant delay in task execution. Recently, approaches based on split computing (SC) have been proposed, where the DNN is split into a head and a tail model, executed respectively on the mobile device and on the edge server. Ultimately, this may reduce bandwidth usage as well as energy consumption. Another approach, called early exiting (EE), trains models to embed multiple “exits” earlier in the arc… |
| https://openalex.org/W4323304388 | Uni-Mol: A Universal 3D Molecular Representation Learning Framework | 2023 |  | preprint | 228 | yes | Gengmo Zhou, Zhifeng Gao, Qiankun Ding, Hang Zheng, Hongteng Xu, Zhewei Wei, Linfeng Zhang, Guolin Ke | Computer science, Representation (politics), Limiting, Transformer, Machine learning, Artificial intelligence, +14 more | https://doi.org/10.26434/chemrxiv-2022-jjm0j-v4 | Molecular representation learning (MRL) has gained tremendous attention due to its critical role in learning from limited supervised data for applications like drug design. In most MRL methods, molecules are treated as 1D sequential tokens or 2D topology graphs, limiting their ability to incorporate 3D information for downstream tasks and, in particular, making it almost impossible for 3D geometry prediction/generation. In this paper, we propose a universal 3D MRL framework, called Uni-Mol, that significantly enlarges the representation ability and application scope of MRL schemes. Uni-Mol contains two pretrained models with the same SE(3) Transformer architecture: a molecular model pretrained by 209M molecular conformations; a pocket model pretrained by 3M candidate protein pocket data. Besides, Uni-Mol contains several finetuning strategies to apply the pretrained models to various do… |
| https://openalex.org/W2956530858 | Evaluating word embedding models: methods and experimental results | 2019 | APSIPA Transactions on Signal and Information Processing | article | 317 | yes | Bin Wang, Angela Yee‐Moon Wang, Fenxiao Chen, Yun-Cheng Wang, C.‐C. Jay Kuo | Computer science, Natural language processing, Word (group theory), Word embedding, Categorization, Task (project management), +15 more | https://doi.org/10.1017/atsip.2019.12 | Extensive evaluation on a large number of word embedding models for language\nprocessing applications is conducted in this work. First, we introduce popular\nword embedding models and discuss desired properties of word models and\nevaluation methods (or evaluators). Then, we categorize evaluators into\nintrinsic and extrinsic two types. Intrinsic evaluators test the quality of a\nrepresentation independent of specific natural language processing tasks while\nextrinsic evaluators use word embeddings as input features to a downstream task\nand measure changes in performance metrics specific to that task. We report\nexperimental results of intrinsic and extrinsic evaluators on six word\nembedding models. It is shown that different evaluators focus on different\naspects of word models, and some are more correlated with natural language\nprocessing tasks. Finally, we adopt correlation analys… |
| https://openalex.org/W4213070269 | A review of molecular representation in the age of machine learning | 2022 | Wiley Interdisciplinary Reviews Computational Molecular Science | review | 325 | yes | Daniel Wigh, Jonathan M. Goodman, Alexei A. Lapkin | Cheminformatics, Computer science, Representation (politics), Chemical space, Artificial intelligence, Identifier, +15 more | https://doi.org/10.1002/wcms.1603 | Abstract Research in chemistry increasingly requires interdisciplinary work prompted by, among other things, advances in computing, machine learning, and artificial intelligence. Everyone working with molecules, whether chemist or not, needs an understanding of the representation of molecules in a machine‐readable format, as this is central to computational chemistry. Four classes of representations are introduced: string, connection table, feature‐based, and computer‐learned representations. Three of the most significant representations are simplified molecular‐input line‐entry system (SMILES), International Chemical Identifier (InChI), and the MDL molfile, of which SMILES was the first to successfully be used in conjunction with a variational autoencoder (VAE) to yield a continuous representation of molecules. This is noteworthy because a continuous representation allows for efficient… |
| https://openalex.org/W3161335554 | Artificial intelligence and machine learning for medical imaging: A technology review | 2021 | Physica Medica | review | 394 | yes | Ana María Barragán Montero, Umair Javaid, Gilmer Valdés, Dan Nguyen, Paul Desbordes, Benoît Macq, Siri Willems, Liesbeth Vandewinckele, +6 more | Workflow, Artificial intelligence, Computer science, Applications of artificial intelligence, Mainstream, Medical imaging, +10 more | https://doi.org/10.1016/j.ejmp.2021.04.016 | Artificial intelligence (AI) has recently become a very popular buzzword, as a consequence of disruptive technical advances and impressive experimental results, notably in the field of image analysis and processing. In medicine, specialties where images are central, like radiology, pathology or oncology, have seized the opportunity and considerable efforts in research and development have been deployed to transfer the potential of AI to clinical applications. With AI becoming a more mainstream tool for typical medical imaging analysis tasks, such as diagnosis, segmentation, or classification, the key for a safe and efficient use of clinical AI applications relies, in part, on informed practitioners. The aim of this review is to present the basic technological pillars of AI, together with the state-of-the-art machine learning methods and their application to medical imaging. In addition,… |
| https://openalex.org/W4412803458 | Large Language Model AI Text Generation Detection based on Transformer Deep Fast Quantum Convolutional Neural Networks | 2025 |  | article | 0 | no | M Sumithra, G Sweety Prasanna kiruba, Jagan Srinivasan, M. Aishwarya, S. Sathya, K. Geetha | Computer science, Convolutional neural network, Transformer, Artificial intelligence, Language model, Deep learning, +8 more | https://doi.org/10.1109/icirca65293.2025.11089509 |  |
| https://openalex.org/W4382877880 | Polyp-PVT: Polyp Segmentation with Pyramid Vision Transformers | 2023 | CAAI Artificial Intelligence Research | article | 287 | yes | Bo Dong, Wenhai Wang, Deng-Ping Fan, Jinpeng Li, Huazhu Fu, Ling Shao | Computer science, Encoder, Artificial intelligence, Segmentation, Transformer, Convolutional neural network, +8 more | https://doi.org/10.26599/air.2023.9150015 | Most polyp segmentation methods use convolutional neural networks (CNNs) as their backbone, leading to two key issues when exchanging information between the encoder and decoder: (1) taking into account the differences in contribution between different-level features, and (2) designing an effective mechanism for fusing these features. Unlike existing CNN-based methods, we adopt a transformer encoder, which learns more powerful and robust representations. In addition, considering the image acquisition influence and elusive properties of polyps, we introduce three standard modules, including a cascaded fusion module (CFM), a camouflage identification module (CIM), and a similarity aggregation module (SAM). Among these, the CFM is used to collect the semantic and location information of polyps from high-level features; the CIM is applied to capture polyp information disguised in low-level… |
| https://openalex.org/W4284670866 | A deep learning based approach for automated plant disease classification using vision transformer | 2022 | Scientific Reports | article | 236 | yes | Yasamin Borhani, Javad Khoramdel, Esmaeil Najafi | Computer science, Convolutional neural network, Artificial intelligence, Plant disease, Deep learning, Machine learning, +7 more | https://doi.org/10.1038/s41598-022-15163-0 |  |
| https://openalex.org/W2970583420 | Syntax-Aware Aspect Level Sentiment Classification with Graph Attention Networks | 2019 |  | article | 292 | yes | Binxuan Huang, Kathleen M. Carley | Computer science, Syntax, Graph, Artificial intelligence, Natural language processing, Theoretical computer science | https://doi.org/10.18653/v1/d19-1549 | Binxuan Huang, Kathleen Carley. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019. |
| https://openalex.org/W3104597568 | Event Extraction as Machine Reading Comprehension | 2020 |  | article | 296 | yes | Jian Liu, Yubo Chen, Kang Liu, Wei Bi, Xiaojiang Liu | Computer science, Artificial intelligence, Event (particle physics), Schema (genetic algorithms), Natural language processing, Information extraction, +12 more | https://doi.org/10.18653/v1/2020.emnlp-main.128 | Event extraction (EE) is a crucial information extraction task that aims to extract event information in texts. Previous methods for EE typically model it as a classification task, which are usually prone to the data scarcity problem. In this paper, we propose a new learning paradigm of EE, by explicitly casting it as a machine reading comprehension problem (MRC). Our approach includes an unsupervised question generation process, which can transfer event schema into a set of natural questions, followed by a BERT-based question-answering process to retrieve answers as EE results. This learning paradigm enables us to strengthen the reasoning process of EE, by introducing sophisticated models in MRC, and relieve the data scarcity problem, by introducing the large-scale datasets in MRC. The empirical results show that: i) our approach attains state-of-the-art performance by considerable mar… |
| https://openalex.org/W4321276815 | Prompt Tuning of Deep Neural Networks for Speaker-adaptive Visual Speech Recognition | 2023 | arXiv (Cornell University) | preprint | 3 | yes | Minsu Kim, Hyung-Il Kim, Yong Man Ro | Computer science, Speech recognition, Transformer, Concatenation (mathematics), Sentence, Hidden Markov model, +12 more | http://arxiv.org/abs/2302.08102 | Visual Speech Recognition (VSR) aims to infer speech into text depending on lip movements alone. As it focuses on visual information to model the speech, its performance is inherently sensitive to personal lip appearances and movements, and this makes the VSR models show degraded performance when they are applied to unseen speakers. In this paper, to remedy the performance degradation of the VSR model on unseen speakers, we propose prompt tuning methods of Deep Neural Networks (DNNs) for speaker-adaptive VSR. Specifically, motivated by recent advances in Natural Language Processing (NLP), we finetune prompts on adaptation data of target speakers instead of modifying the pre-trained model parameters. Different from the previous prompt tuning methods mainly limited to Transformer variant architecture, we explore different types of prompts, the addition, the padding, and the concatenation… |
| https://openalex.org/W4220949944 | Shared computational principles for language processing in humans and deep language models | 2022 | Nature Neuroscience | article | 409 | yes | Ariel Goldstein, Zaid Zada, Eliav Buchnik, Mariano Schain, Amy Price, Bobbi Aubrey, Samuel A. Nastase, Amir Feder, +24 more | Surprise, Computer science, Autoregressive model, Artificial intelligence, Context (archaeology), Computational model, +19 more | https://doi.org/10.1038/s41593-022-01026-4 |  |
| https://openalex.org/W4379260118 | Identifying Risk Factors Associated With Lower Back Pain in Electronic Medical Record Free Text: Deep Learning Approach Using Clinical Note Annotations | 2023 | JMIR Medical Informatics | article | 4 | yes | Aman Jaiswal, Alan Katz, Marcello Nesca, Evangelos Milios | Artificial intelligence, Deep learning, Computer science, Machine learning, Natural language processing, Population, +2 more | https://doi.org/10.2196/45105 | Abstract Background Lower back pain is a common weakening condition that affects a large population. It is a leading cause of disability and lost productivity, and the associated medical costs and lost wages place a substantial burden on individuals and society. Recent advances in artificial intelligence and natural language processing have opened new opportunities for the identification and management of risk factors for lower back pain. In this paper, we propose and train a deep learning model on a data set of clinical notes that have been annotated with relevant risk factors, and we evaluate the model’s performance in identifying risk factors in new clinical notes. Objective The primary objective is to develop a novel deep learning approach to detect risk factors for underlying disease in patients presenting with lower back pain in clinical encounter notes. The secondary objective is… |
| https://openalex.org/W4285227013 | RTIDS: A Robust Transformer-Based Approach for Intrusion Detection System | 2022 | IEEE Access | article | 234 | yes | Zihan Wu, Hong Zhang, Penghai Wang, Zhibo Sun | Computer science, Intrusion detection system, Artificial intelligence, Recurrent neural network, Data mining, Artificial neural network, +9 more | https://doi.org/10.1109/access.2022.3182333 | Due to the rapid growth in network traffic and increasing security threats, Intrusion Detection Systems (IDS) have become increasingly critical in the field of cyber security for providing secure communications against cyber adversaries. However, there exist many challenges for designing a robust, efficient and accurate IDS, especially when dealing with high-dimensional anomaly data with unforeseen and unpredictable attacks. In this paper, we propose a Robust Transformer-based Intrusion Detection System (RTIDS) reconstructing feature representations to make a trade-off between dimensionality reduction and feature retention in imbalanced datasets. The proposed method utilizes positional embedding technique to associate sequential information between features, then a variant stacked encoder-decoder neural network is used to learn low-dimensional feature representations from high-dimension… |
| https://openalex.org/W4400118952 | When large language models meet personalization: perspectives of challenges and opportunities | 2024 | World Wide Web | article | 236 | yes | Jing Chen, Zheng Liu, Xu Huang, Chenwang Wu, Qi Liu, Gangwei Jiang, Yuanhao Pu, Yuxuan Lei, +5 more | Personalization, Computer science, Function (biology), Scope (computer science), Language model, Service (business), +13 more | https://doi.org/10.1007/s11280-024-01276-1 |  |
| https://openalex.org/W4387390282 | Use Your INSTINCT: INSTruction optimization for LLMs usIng Neural bandits Coupled with Transformers | 2023 | arXiv (Cornell University) | preprint | 2 | yes | Xiaoqiang Lin, Zhaoxuan Wu, Zhongxiang Dai, Wenyang Hu, Yao Shu, See-Kiong Ng, Patrick Jaillet, Bryan Kian Hsiang Low | Computer science, Transformer, Artificial intelligence, Artificial neural network, Bayesian optimization, Machine learning, +8 more | http://arxiv.org/abs/2310.02905 | Large language models (LLMs) have shown remarkable instruction-following capabilities and achieved impressive performances in various applications. However, the performances of LLMs depend heavily on the instructions given to them, which are typically manually tuned with substantial human efforts. Recent work has used the query-efficient Bayesian optimization (BO) algorithm to automatically optimize the instructions given to black-box LLMs. However, BO usually falls short when optimizing highly sophisticated (e.g., high-dimensional) objective functions, such as the functions mapping an instruction to the performance of an LLM. This is mainly due to the limited expressive power of the Gaussian process (GP) which is used by BO as a surrogate to model the objective function. Meanwhile, it has been repeatedly shown that neural networks (NNs), especially pre-trained transformers, possess str… |
| https://openalex.org/W4226108798 | Explainable AI Methods - A Brief Overview | 2022 | Lecture notes in computer science | book-chapter | 335 | yes | Andreas Holzinger, Anna Saranti, Christoph Molnar, Przemysław Biecek, Wojciech Samek | Computer science, Variety (cybernetics), Field (mathematics), Artificial intelligence, Artificial neural network, Data science, +3 more | https://doi.org/10.1007/978-3-031-04083-2_2 | Abstract Explainable Artificial Intelligence (xAI) is an established field with a vibrant community that has developed a variety of very successful approaches to explain and interpret predictions of complex machine learning models such as deep neural networks. In this article, we briefly introduce a few selected methods and discuss them in a short, clear and concise way. The goal of this article is to give beginners, especially application engineers and data scientists, a quick overview of the state of the art in this current topic. The following 17 methods are covered in this chapter: LIME, Anchors, GraphLIME, LRP, DTD, PDA, TCAV, XGNN, SHAP, ASV, Break-Down, Shapley Flow, Textual Explanations of Visual Models, Integrated Gradients, Causal Models, Meaningful Perturbations, and X-NeSyL. |
| https://openalex.org/W4391514872 | Autoencoders and their applications in machine learning: a survey | 2024 | Artificial Intelligence Review | article | 398 | yes | Kamal Berahmand, Fatemeh Daneshfar, Elaheh Sadat Salehi, Yuefeng Li, Yue Xu | Autoencoder, Computer science, Artificial intelligence, Machine learning, Dimensionality reduction, Process (computing), +7 more | https://doi.org/10.1007/s10462-023-10662-6 | Abstract Autoencoders have become a hot researched topic in unsupervised learning due to their ability to learn data features and act as a dimensionality reduction method. With rapid evolution of autoencoder methods, there has yet to be a complete study that provides a full autoencoders roadmap for both stimulating technical improvements and orienting research newbies to autoencoders. In this paper, we present a comprehensive survey of autoencoders, starting with an explanation of the principle of conventional autoencoder and their primary development process. We then provide a taxonomy of autoencoders based on their structures and principles and thoroughly analyze and discuss the related models. Furthermore, we review the applications of autoencoders in various fields, including machine vision, natural language processing, complex network, recommender system, speech process, anomaly de… |
| https://openalex.org/W4407348966 | Introducing TEC-LncMir for prediction of lncRNA-miRNA interactions through deep learning of RNA sequences | 2024 | Briefings in Bioinformatics | article | 4 | yes | Tingpeng Yang, Yonghong He, Yu Wang | TEC, Computer science, microRNA, Competing endogenous RNA, Encoder, Computational biology, +10 more | https://doi.org/10.1093/bib/bbaf046 | Abstract The interactions between long noncoding RNA (lncRNA) and microRNA (miRNA) play critical roles in life processes, highlighting the necessity to enhance the performance of state-of-the-art models. Here, we introduced TEC-LncMir, a novel approach for predicting lncRNA-miRNA interaction using Transformer Encoder and convolutional neural networks (CNNs). TEC-LncMir treats lncRNA and miRNA sequences as natural languages, encodes them using the Transformer Encoder, and combines representations of a pair of microRNA and lncRNA into a contact tensor (a three-dimensional array). Afterward, TEC-LncMir treats the contact tensor as a multi-channel image, utilizes a four-layer CNN to extract the contact tensor’s features, and then uses these features to predict the interaction between the pair of lncRNA and miRNA. We applied a series of comparative experiments to demonstrate that TEC-LncMir… |
| https://openalex.org/W4399912714 | VisualRWKV: Exploring Recurrent Neural Networks for Visual Language Models | 2024 | arXiv (Cornell University) | preprint | 2 | yes | Haowen Hou, Peigen Zeng, Fei Ma, F. Richard Yu | Computer science, Artificial intelligence, Recurrent neural network, Visual language, Natural language processing, Artificial neural network, +4 more | http://arxiv.org/abs/2406.13362 | Visual Language Models (VLMs) have rapidly progressed with the recent success of large language models. However, there have been few attempts to incorporate efficient linear Recurrent Neural Networks (RNNs) architectures into VLMs. In this study, we introduce VisualRWKV, the first application of a linear RNN model to multimodal learning tasks, leveraging the pre-trained RWKV language model. We propose a data-dependent recurrence and sandwich prompts to enhance our modeling capabilities, along with a 2D image scanning mechanism to enrich the processing of visual sequences. Extensive experiments demonstrate that VisualRWKV achieves competitive performance compared to Transformer-based models like LLaVA-1.5 on various benchmarks. Compared to LLaVA-1.5, VisualRWKV has a speed advantage of 3.98 times and can save 54% of GPU memory when reaching an inference length of 24K tokens. To facilitat… |
| https://openalex.org/W4212926655 | A Survey on Text Classification Algorithms: From Text to Predictions | 2022 | Information | article | 218 | yes | Andrea Gasparetto, Matteo Marcuzzo, Alessandro Zangari, Andrea Albarelli | Computer science, Artificial intelligence, Leverage (statistics), Machine learning, Preprocessor, Natural language processing, +4 more | https://doi.org/10.3390/info13020083 | In recent years, the exponential growth of digital documents has been met by rapid progress in text classification techniques. Newly proposed machine learning algorithms leverage the latest advancements in deep learning methods, allowing for the automatic extraction of expressive features. The swift development of these methods has led to a plethora of strategies to encode natural language into machine-interpretable data. The latest language modelling algorithms are used in conjunction with ad hoc preprocessing procedures, of which the description is often omitted in favour of a more detailed explanation of the classification step. This paper offers a concise review of recent text classification models, with emphasis on the flow of data, from raw text to output labels. We highlight the differences between earlier methods and more recent, deep learning-based methods in both their functio… |
| https://openalex.org/W4368276316 | Context Aware Automatic Subjective and Objective Question Generation using Fast Text to Text Transfer Learning | 2023 | International Journal of Advanced Computer Science and Applications | article | 3 | yes | Arpit Agrawal, Pragya Shukla | Computer science, Artificial intelligence, Natural language processing, Sentence, Transfer of learning, Deep learning, +1 more | https://doi.org/10.14569/ijacsa.2023.0140451 | Online learning has gained a tremendous popularity in the last decade due to the facility to learn anytime, anything, anywhere from the ocean of web resources available. Especially the lockdown all over the world due to the Covid-19 pandemic has brought an enormous attention towards the online learning for value addition and skills development not only for the school/college students, but also to the working professionals. This massive growth in online learning has made the task of assessment very tedious and demands training, experience and resources. Automatic Question generation (AQG) techniques have been introduced to resolve this problem by deriving a question bank from the text documents. However, the performance of conventional AQG techniques is subject to the availability of large labelled training dataset. The requirement of deep linguistic knowledge for the generation of heuri… |
| https://openalex.org/W4364363895 | Exploring the Boundaries of Reality: Investigating the Phenomenon of Artificial Intelligence Hallucination in Scientific Writing Through ChatGPT References | 2023 | Cureus | article | 342 | yes | Sai Anirudh Athaluri, Sandeep Varma Manthena, V S R Krishna Manoj Kesapragada, Vineel Yarlagadda, Tirth Dave, Rama Tulasi Siri Duddumpudi | Computer science, Natural language processing, Artificial intelligence, Chatbot, Data science | https://doi.org/10.7759/cureus.37432 | Background Chatbots are computer programs that use artificial intelligence (AI) and natural language processing (NLP) to simulate conversations with humans. One such chatbot is ChatGPT, which uses the third-generation generative pre-trained transformer (GPT-3) developed by OpenAI. ChatGPT has been praised for its ability to generate text, but concerns have been raised about its accuracy and precision in generating data, as well as legal issues related to references. This study aims to investigate the frequency of AI hallucination in research proposals entirely drafted by ChatGPT. Methodology An analytical design was employed to investigate AI hallucination by ChatGPT. A total of 178 references listed by ChatGPT were verified for inclusion in the study. Statistical analysis was performed by five researchers who entered their data into a Google Form, and the final results were represented… |
| https://openalex.org/W3010387158 | ProGen: Language Modeling for Protein Generation | 2020 |  | preprint | 236 | yes | Ali Madani, Bryan McCann, Nikhil Naik, Nitish Shirish Keskar, Namrata Anand, Raphael R. Eguchi, Po‐Ssu Huang, Richard Socher | Leverage (statistics), Generative grammar, Computer science, Sequence (biology), Protein engineering, Artificial intelligence, +10 more | https://doi.org/10.1101/2020.03.07.982272 | Abstract Generative modeling for protein engineering is key to solving fundamental problems in synthetic biology, medicine, and material science. We pose protein engineering as an unsupervised sequence generation problem in order to leverage the exponentially growing set of proteins that lack costly, structural annotations. We train a 1.2B-parameter language model, ProGen, on ∼280M protein sequences conditioned on taxonomic and keyword tags such as molecular function and cellular component. This provides ProGen with an unprecedented range of evolutionary sequence diversity and allows it to generate with fine-grained control as demonstrated by metrics based on primary sequence similarity, secondary structure accuracy, and conformational energy. |
| https://openalex.org/W2982875310 | Sentence-Level BERT and Multi-Task Learning of Age and Gender in Social Media | 2019 | arXiv (Cornell University) | preprint | 2 | yes | Muhammad Abdul-Mageed, Chiyu Zhang, Arun Kumar Rajendran, AbdelRahim Elmadany, Michael Przystupa, Lyle Ungar | Computer science, Encoder, Artificial intelligence, Sentence, Task (project management), Margin (machine learning), +19 more | http://arxiv.org/abs/1911.00637 | Social media currently provide a window on our lives, making it possible to learn how people from different places, with different backgrounds, ages, and genders use language. In this work we exploit a newly-created Arabic dataset with ground truth age and gender labels to learn these attributes both individually and in a multi-task setting at the sentence level. Our models are based on variations of deep bidirectional neural networks. More specifically, we build models with gated recurrent units and bidirectional encoder representations from transformers (BERT). We show the utility of multi-task learning (MTL) on the two tasks and identify task-specific attention as a superior choice in this context. We also find that a single-task BERT model outperform our best MTL models on the two tasks. We report tweet-level accuracy of 51.43% for the age task (three-way) and 65.30% on the gender t… |
| https://openalex.org/W4393932061 | A multimodal machine learning approach to generate news articles from geo-tagged images | 2024 | International Journal of Power Electronics and Drive Systems/International Jour… | article | 2 | yes | Abhay R. Gotmare, Gandharva Thite, Laxmi Bewoor | Computer science, Closed captioning, Artificial intelligence, Machine learning, Convolutional neural network, Headline, +5 more | http://dx.doi.org/10.11591/ijece.v14i3.pp3434-3442 | Classical machine learning algorithms typically operate on unimodal data and hence it can analyze and make predictions based on data from a single source (modality). Whereas multimodal machine learning algorithm, learns from information across multiple modalities, such as text, images, audio, and sensor data. The paper leverages the functionalities of multimodal machine learning (ML) application for generating text from images. The proposed work presents an innovative multimodal algorithm that automates the creation of news articles from geo-tagged images by leveraging cutting-edge developments in machine learning, image captioning, and advanced text generation technologies. Employing a multimodal approach that integrates machine learning and transformer algorithms, such as visual geometry group network16 (VGGNet16), convolutional neural network (CNN) and a long short-term memory (LSTM)… |
| https://openalex.org/W4285787895 | A Review of Generalized Zero-Shot Learning Methods | 2022 | IEEE Transactions on Pattern Analysis and Machine Intelligence | review | 368 | yes | Farhad Pourpanah, Moloud Abdar, Yuxuan Luo, Xinlei Zhou, Ran Wang, Chee Peng Lim, Xizhao Wang, Q. M. Jonathan Wu | Computer science, Artificial intelligence, Benchmark (surveying), Categorization, Machine learning, Task (project management), +8 more | https://doi.org/10.1109/tpami.2022.3191696 | Generalized zero-shot learning (GZSL) aims to train a model for classifying data samples under the condition that some output classes are unknown during supervised learning. To address this challenging task, GZSL leverages semantic information of the seen (source) and unseen (target) classes to bridge the gap between both seen and unseen classes. Since its introduction, many GZSL models have been formulated. In this review paper, we present a comprehensive review on GZSL. First, we provide an overview of GZSL including the problems and challenges. Then, we introduce a hierarchical categorization for the GZSL methods and discuss the representative methods in each category. In addition, we discuss the available benchmark data sets and applications of GZSL, along with a discussion on the research gaps and directions for future investigations. |
| https://openalex.org/W3175225269 | A Unified Generative Framework for Various NER Subtasks | 2021 |  | article | 247 | yes | Hang Yan, Tao Gui, Junqi Dai, Qipeng Guo, Zheng Zhang, Xipeng Qiu | Generative grammar, Computer science, Zhàng, Natural language processing, Joint (building), Linguistics, +14 more | https://doi.org/10.18653/v1/2021.acl-long.451 | Hang Yan, Tao Gui, Junqi Dai, Qipeng Guo, Zheng Zhang, Xipeng Qiu. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021. |
| https://openalex.org/W3119308075 | VoxPopuli: A Large-Scale Multilingual Speech Corpus for Representation Learning, Semi-Supervised Learning and Interpretation | 2021 |  | preprint | 288 | yes | Changhan Wang, Morgane Rivière, Ann Lee, Anne Wu, Chaitanya Talnikar, Daniel Haziza, Mary Williamson, Juan Pino, +1 more | Computer science, Natural language processing, Interpretation (philosophy), Representation (politics), Artificial intelligence, Computational linguistics, +19 more | https://doi.org/10.18653/v1/2021.acl-long.80 | International audience |
| https://openalex.org/W4405799498 | Scientific article summarization model with unbounded input length | 2024 | Information, computing and intelligent systems | article | 2 | yes | Oleksandr Steblianko, Volodymyr Shymkovych, P.I. Kravets, A.O. Novatskyi, Lyubov Shymkovych | Automatic summarization, Computer science, Transformer, Encoder, Artificial intelligence, Natural language processing, +5 more | https://doi.org/10.20535/2786-8729.5.2024.314724 | In recent years, the exponential growth of scientific literature has made it increasingly difficult for researchers and practitioners to keep up with new discoveries and developments in their fields. Thanks to this, text summarization has become one of the primary tasks of natural language processing. Abstractive summarization of long documents, such as scientific articles, requires large neural networks with high memory and computation requirements. Therefore, it is all the more important to find ways to increase the efficiency of long document summarization models. The objects of this research are long document summarization transformer models and the Unlimiformer cross-attention modification. The article reviews the basic principles of transformer attention, which constitutes the primary computational expense in transformer models. More efficient self-attention approaches used for lo… |
| https://openalex.org/W4288425181 | Pre-training Models Based Knowledge Graph Multi-hop Reasoning for Smart Grid Technology | 2022 | Lecture notes in electrical engineering | book-chapter | 3 | no | Ruilin Chen, Yu Wang, Gen Li, Dapeng Yan, Hui Cao | Computer science, Grid, Smart grid, Artificial intelligence, Graph, Transformer, +8 more | https://doi.org/10.1007/978-981-19-3998-3_173 |  |
| https://openalex.org/W4360995088 | eSwin-UNet: A Collaborative Model for Industrial Surface Defect Detection | 2023 |  | article | 2 | no | Helei Cui, Tao Xing, Jiaju Ren, Yaxing Chen, Zhiwen Yu, Bin Guo, Xiaobing Guo | Jaccard index, Computer science, Convolutional neural network, Artificial intelligence, Segmentation, Transformer, +6 more | https://doi.org/10.1109/icpads56603.2022.00056 | Surface inspection of industrial equipment defection plays a vital role in real production. Traditional inspection routines require a large number of inspection workers, which not only affects production efficiency but also leads to unreliable results. Computer vision-based detection approaches, e.g., using the deep learning method, have shown great potential in this trend. Specifically, the semantic segmentation algorithm based on Convolutional Neural Network (CNN) can extract relatively complete feature information. And the Transformer, which emerged from the field of Natural Language Processing (NLP), also performs well in maintaining and transmitting semantic information. In light of these, we propose to design a segmentation model called eSwin-UNet, i.e., enhanced Swin-UNet, that leverages the advantages of the CNN and Transformer. It uses multi-scale information fusion to better i… |
| https://openalex.org/W3153758184 | Exploring Neural Language Models via Analysis of Local and Global Self-Attention Spaces | 2021 | Edinburgh Research Explorer (University of Edinburgh) | article | 1 | yes | Blaž Škrlj, Shane Sheehan, Nika Eržen, Marko Robnik‐Šikonja, Saturnino Luz, Senja Pollak | Computer science, Interpretability, Transformer, Artificial intelligence, Machine translation, Language model, +11 more | https://www.research.ed.ac.uk/en/publications/458a8ec9-ae67-4a67-9c36-26a7b402f485 | Large pretrained language models using the transformer neural network architecture are becoming a dominant methodology for many natural language processing tasks, such as question answering, text classification, word sense disambiguation, text completion and machine translation. Commonly comprising hundreds of millions of parameters, these models offer state-of-the-art performance, but at the expense of interpretability. The attention mechanism is the main component of transformer networks. We present AttViz, a method for exploration of self-attention in transformer networks, which can help in explanation and debugging of the trained models by showing associations between text tokens in an input sequence. We show that existing deep learning pipelines can be explored with AttViz, which offers novel visualizations of the attention heads and their aggregations. We implemented the proposed… |
| https://openalex.org/W3035448883 | Improving Multimodal Named Entity Recognition via Entity Span Detection with Unified Multimodal Transformer | 2020 |  | article | 236 | yes | Jianfei Yu, Jing Jiang, Yang Li, Rui Xia | Computer science, Transformer, Leverage (statistics), Artificial intelligence, Natural language processing, Named-entity recognition, +7 more | https://doi.org/10.18653/v1/2020.acl-main.306 | In this paper, we study Multimodal Named Entity Recognition (MNER) for social media posts. Existing approaches for MNER mainly suffer from two drawbacks: (1) despite generating word-aware visual representations, their word representations are insensitive to the visual context; (2) most of them ignore the bias brought by the visual context. To tackle the first issue, we propose a multimodal interaction module to obtain both image-aware word representations and word-aware visual representations. To alleviate the visual bias, we further propose to leverage purely text-based entity span detection as an auxiliary module, and design a Unified Multimodal Transformer to guide the final predictions with the entity span predictions. Experiments show that our unified approach achieves the new state-of-the-art performance on two benchmark datasets. |
| https://openalex.org/W4291288469 | The transformational role of GPU computing and deep learning in drug discovery | 2022 | Nature Machine Intelligence | article | 242 | yes | Mohit Pandey, Michael Fernández, Francesco Gentile, Olexandr Isayev, Alexander Tropsha, Abraham C. Stern, Artem Cherkasov | Drug discovery, Computer science, Deep learning, Data science, Artificial intelligence, Field (mathematics), +10 more | https://doi.org/10.1038/s42256-022-00463-x |  |
| https://openalex.org/W4399038319 | The Impact of Activation Patterns in the Explainability of Large Language Models – A Survey of recent advances | 2024 |  | article | 1 | yes | Mateus R. Figênio, André Santanchè, Luiz Gomes-Jr | Computer science, Transformer, Language model, Artificial neural network, Natural language, Artificial intelligence, +5 more | http://dx.doi.org/10.5753/erbd.2024.238868 | The performance benchmarks of Natural Language Processing (NLP) tasks have been overwhelmed by Large Language Models (LLMs), with their capabilities outshining many previous approaches to language modeling. But, despite the success in these tasks and the more ample and pervasive use of these models in many daily and specialized fields of application, little is known of how or why they reach the outputs they do. This study reviews the development of Language Models (LMs), the advances in their explainability approaches, and focuses on assessing methods to interpret and explain the neural network portion of LMs (specially of Transformer models) as means of better understanding them. |
| https://openalex.org/W3166791908 | Semantic Communication Systems for Speech Transmission | 2021 | IEEE Journal on Selected Areas in Communications | article | 537 | yes | Zhenzi Weng, Zhijin Qin | Computer science, Speech recognition, Transmission (telecommunications), Channel (broadcasting), Communications system, Distortion (music), +8 more | https://qmro.qmul.ac.uk/xmlui/handle/123456789/72821 | Semantic communications could improve the transmission efficiency significantly by exploring the semantic information. In this paper, we make an effort to recover the transmitted speech signals in the semantic communication systems, which minimizes the error at the semantic level rather than the bit or symbol level. Particularly, we design a deep learning (DL)-enabled semantic communication system for speech signals, named DeepSC-S. In order to improve the recovery accuracy of speech signals, especially for the essential information, DeepSC-S is developed based on an attention mechanism by utilizing a squeeze-and-excitation (SE) network. The motivation behind the attention mechanism is to identify the essential speech information by providing higher weights to them when training the neural network. Moreover, in order to facilitate the proposed DeepSC-S for dynamic channel environments,… |
| https://openalex.org/W3162922479 | What Disease Does This Patient Have? A Large-Scale Open Domain Question Answering Dataset from Medical Exams | 2021 | Applied Sciences | article | 381 | yes | Di Jin, Eileen Pan, Nassim Oufattole, Wei‐Hung Weng, Hanyi Fang, Peter Szolovits | Question answering, Computer science, Natural language processing, Artificial intelligence, Domain (mathematical analysis), Open domain, +11 more | https://doi.org/10.3390/app11146421 | Open domain question answering (OpenQA) tasks have been recently attracting more and more attention from the natural language processing (NLP) community. In this work, we present the first free-form multiple-choice OpenQA dataset for solving medical problems, MedQA, collected from the professional medical board exams. It covers three languages: English, simplified Chinese, and traditional Chinese, and contains 12,723, 34,251, and 14,123 questions for the three languages, respectively. We implement both rule-based and popular neural methods by sequentially combining a document retriever and a machine comprehension model. Through experiments, we find that even the current best method can only achieve 36.7%, 42.0%, and 70.1% of test accuracy on the English, traditional Chinese, and simplified Chinese questions, respectively. We expect MedQA to present great challenges to existing OpenQA sy… |
| https://openalex.org/W4403601655 | ILMCNet: A Deep Neural Network Model That Uses PLM to Process Features and Employs CRF to Predict Protein Secondary Structure | 2024 | Genes | article | 2 | yes | Benzhi Dong, Hui Su, Dali Xu, Hou Chang, Zheng Liu, Na Niu, Guohua Wang | Artificial neural network, Task (project management), Computational biology, Computer science, Sequence (biology), Protein secondary structure, +13 more | https://doi.org/10.3390/genes15101350 | Background: Protein secondary structure prediction (PSSP) is a critical task in computational biology, pivotal for understanding protein function and advancing medical diagnostics. Recently, approaches that integrate multiple amino acid sequence features have gained significant attention in PSSP research. Objectives: We aim to automatically extract additional features represented by evolutionary information from a large number of sequences while simultaneously incorporating positional information for more comprehensive sequence features. Additionally, we consider the interdependence between secondary structures during the prediction stage. Methods: To this end, we propose a deep neural network model, ILMCNet, which utilizes a language model and Conditional Random Field (CRF). Protein language models (PLMs) pre-trained on sequences from multiple large databases can provide sequence featu… |
| https://openalex.org/W3177224328 | UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning | 2021 |  | article | 229 | yes | Wei Li, Can Gao, Guocheng Niu, Xinyan Xiao, Hao Liu, Jiachen Liu, Hua Wu, Haifeng Wang | Modal, Computer science, Natural language processing, Linguistics, Artificial intelligence, Computational linguistics, +9 more | https://doi.org/10.18653/v1/2021.acl-long.202 | Wei Li, Can Gao, Guocheng Niu, Xinyan Xiao, Hao Liu, Jiachen Liu, Hua Wu, Haifeng Wang. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021. |
| https://openalex.org/W3205586691 | Point Transformer | 2021 | IEEE Access | article | 255 | yes | Nico Engel, Vasileios Belagiannis, Klaus Dietmayer | Computer science, Transformer, Segmentation, Artificial intelligence, Pattern recognition (psychology), Artificial neural network, +4 more | https://doi.org/10.1109/access.2021.3116304 | In this work, we present Point Transformer, a deep neural network that\noperates directly on unordered and unstructured point sets. We design Point\nTransformer to extract local and global features and relate both\nrepresentations by introducing the local-global attention mechanism, which aims\nto capture spatial point relations and shape information. For that purpose, we\npropose SortNet, as part of the Point Transformer, which induces input\npermutation invariance by selecting points based on a learned score. The output\nof Point Transformer is a sorted and permutation invariant feature list that\ncan directly be incorporated into common computer vision applications. We\nevaluate our approach on standard classification and part segmentation\nbenchmarks to demonstrate competitive results compared to the prior work. Code\nis publicly available at: https://github.com/engelnico/point-tran… |
| https://openalex.org/W4323536358 | Leadership is needed for ethical ChatGPT: Character, assessment, and learning using artificial intelligence (AI) | 2023 | Journal of University Teaching and Learning Practice | article | 483 | yes | Joseph Crawford, Michael Cowling, Kelly‐Ann Allen | Character education, Psychology, Curriculum, Artificial intelligence, Higher education, Pedagogy, +8 more | https://doi.org/10.53761/1.20.3.02 | The OpenAI’s ChatGPT-3, or Chat Generative Pre-Trained Transformer was released in November 2022 without significant warning, and has taken higher education by storm since. The artificial intelligence (AI)-powered chatbot has caused alarm for practitioners seeking to detect authenticity of student work. Whereas some educational doomsayers predict the end of education in its current form, we propose an alternate early view. We identify in this commentary a position where educators can leverage AI like ChatGPT to build supportive learning environments for students who have cultivated good character. Such students know how to use ChatGPT for good, and can engage effectively with the ChatGPT application. In building our ChatGPT argument, we acknowledge the existing literature on plagiarism and academic integrity, and consider leadership as a root support mechanism, character development as… |
| https://openalex.org/W4309374880 | Training Effective Neural Sentence Encoders from Automatically Mined Paraphrases | 2022 | 2022 IEEE International Conference on Systems, Man, and Cybernetics (SMC) | article | 2 | no | Sławomir Dadas | Computer science, Sentence, Natural language processing, Artificial intelligence, Encoder, Paraphrase, +6 more | https://doi.org/10.1109/smc53654.2022.9945218 | Sentence embeddings are commonly used in text clustering and semantic retrieval tasks. State-of-the-art sentence representation methods are based on artificial neural networks fine-tuned on large collections of manually labeled sentence pairs. Sufficient amount of annotated data is available for high-resource languages such as English or Chinese. In less popular languages, multilingual models have to be used, which offer lower performance. In this publication, we address this problem by proposing a method for training effective language-specific sentence encoders without manually labeled data. Our approach is to automatically construct a dataset of paraphrase pairs from sentence-aligned bilingual text corpora. We then use the collected data to fine-tune a Transformer language model with an additional recurrent pooling layer. Our sentence encoder can be trained in less than a day on a si… |
| https://openalex.org/W3035367371 | Weight Poisoning Attacks on Pretrained Models | 2020 |  | preprint | 302 | yes | Keita Kurita, Paul Michel, Graham Neubig | Computer science, Initialization, Upload, Regularization (linguistics), Embedding, Construct (python library), +9 more | https://doi.org/10.18653/v1/2020.acl-main.249 | Recently, NLP has seen a surge in the usage of large pre-trained models. Users download weights of models pre-trained on large datasets, then fine-tune the weights on a task of their choice. This raises the question of whether downloading untrusted pre-trained weights can pose a security threat. In this paper, we show that it is possible to construct “weight poisoning” attacks where pre-trained weights are injected with vulnerabilities that expose “backdoors” after fine-tuning, enabling the attacker to manipulate the model prediction simply by injecting an arbitrary keyword. We show that by applying a regularization method which we call RIPPLe and an initialization procedure we call Embedding Surgery, such attacks are possible even with limited knowledge of the dataset and fine-tuning procedure. Our experiments on sentiment classification, toxicity detection, and spam detection show tha… |
| https://openalex.org/W4221161239 | RuMedBench: A Russian Medical Language Understanding Benchmark | 2022 | Lecture notes in computer science | book-chapter | 2 | yes | Pavel Blinov, Arina Reshetnikova, А. Г. Нестеров, Galina Zubkova, Vladimir Kokh | Computer science, Benchmark (surveying), Inference, Artificial intelligence, Metric (unit), Machine learning, +16 more | http://arxiv.org/abs/2201.06499 |  |
| https://openalex.org/W4284890590 | Vision transformer and explainable transfer learning models for auto detection of kidney cyst, stone and tumor from CT-radiography | 2022 | Scientific Reports | article | 242 | yes | Md Nazmul Islam, Mehedi Hasan, Md Kabir Hossain, Md. Golam Rabiul Alam, Md. Zia Uddin, Ahmet Soylu | Deep learning, Artificial intelligence, Computer science, Transfer of learning, Radiography, Ray casting, +4 more | https://doi.org/10.1038/s41598-022-15634-4 |  |
| https://openalex.org/W3116295307 | IndoLEM and IndoBERT: A Benchmark Dataset and Pre-trained Language Model for Indonesian NLP | 2020 |  | article | 244 | yes | Fajri Koto, Afshin Rahimi, Jey Han Lau, Timothy Baldwin | Indonesian, Computer science, Natural language processing, Syntax, Benchmark (surveying), Artificial intelligence, +15 more | https://doi.org/10.18653/v1/2020.coling-main.66 | Although the Indonesian language is spoken by almost 200 million people and the 10th most spoken language in the world, it is under-represented in NLP research. Previous work on Indonesian has been hampered by a lack of annotated datasets, a sparsity of language resources, and a lack of resource standardization. In this work, we release the IndoLEM dataset comprising seven tasks for the Indonesian language, spanning morpho-syntax, semantics, and discourse. We additionally release IndoBERT, a new pre-trained language model for Indonesian, and evaluate it over IndoLEM, in addition to benchmarking it against existing resources. Our experiments show that IndoBERT achieves state-of-the-art performance over most of the tasks in IndoLEM. |
| https://openalex.org/W2997150500 | Channel Attention Is All You Need for Video Frame Interpolation | 2020 | Proceedings of the AAAI Conference on Artificial Intelligence | article | 325 | yes | Myungsub Choi, Heewon Kim, Bohyung Han, Ning Xu, Kyoung Mu Lee | Computer science, Optical flow, Interpolation (computer graphics), Channel (broadcasting), Motion interpolation, Frame (networking), +20 more | https://doi.org/10.1609/aaai.v34i07.6693 | Prevailing video frame interpolation techniques rely heavily on optical flow estimation and require additional model complexity and computational cost; it is also susceptible to error propagation in challenging scenarios with large motion and heavy occlusion. To alleviate the limitation, we propose a simple but effective deep neural network for video frame interpolation, which is end-to-end trainable and is free from a motion estimation network component. Our algorithm employs a special feature reshaping operation, referred to as PixelShuffle, with a channel attention, which replaces the optical flow computation module. The main idea behind the design is to distribute the information in a feature map into multiple channels and extract motion information by attending the channels for pixel-level frame synthesis. The model given by this principle turns out to be effective in the presence… |
| https://openalex.org/W3176038554 | A Unified Generative Framework for Aspect-based Sentiment Analysis | 2021 |  | article | 271 | yes | Hang Yan, Junqi Dai, Tuo Ji, Xipeng Qiu, Zheng Zhang | Generative grammar, Computer science, Natural language processing, Zhàng, Joint (building), Artificial intelligence, +12 more | https://doi.org/10.18653/v1/2021.acl-long.188 | Hang Yan, Junqi Dai, Tuo Ji, Xipeng Qiu, Zheng Zhang. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021. |
| https://openalex.org/W3213708992 | Improving Neural Language Processing with Named Entities | 2021 |  | article | 1 | yes | Kyoumoto Matsushita, Takuya Makino, Tomoya Iwakura | Computer science, Artificial intelligence, Natural language processing, Parsing, Headline, Sentence, +14 more | https://doi.org/10.26615/978-954-452-072-4_107 | Pretraining-based neural network models have demonstrated state-of-the-art (SOTA) performances on natural language processing (NLP) tasks.The most frequently used sentence representation for neural-based NLP methods is a sequence of subwords that is different from the sentence representation of non-neural methods that are created using basic NLP technologies, such as part-of-speech (POS) tagging, named entity (NE) recognition, and parsing.Most neural-based NLP models receive only vectors encoded from a sequence of subwords obtained from an input text.However, basic NLP information, such as POS tags, NEs, parsing results, etc, cannot be obtained explicitly from only the large unlabeled text used in pretraining-based models.This paper explores use of NEs on two Japanese tasks; document classification and headline generation using Transformer-based models, to reveal the effectiveness of ba… |
| https://openalex.org/W3210237961 | Node Feature Extraction by Self-Supervised Multi-scale Neighborhood Prediction | 2021 | arXiv (Cornell University) | preprint | 6 | yes | Eli Chien, Wei-Cheng Chang, Cho‐Jui Hsieh, Hsiang‐Fu Yu, Jiong Zhang, Olgica Milenković, Inderjit S. Dhillon | Computer science, Graph, Artificial intelligence, Feature extraction, Feature learning, Data mining, +2 more | https://arxiv.org/pdf/2111.00064 | Learning on graphs has attracted significant attention in the learning community due to numerous real-world applications. In particular, graph neural networks (GNNs), which take numerical node features and graph structure as inputs, have been shown to achieve state-of-the-art performance on various graph-related learning tasks. Recent works exploring the correlation between numerical node features and graph structure via self-supervised learning have paved the way for further performance improvements of GNNs. However, methods used for extracting numerical node features from raw data are still graph-agnostic within standard GNN pipelines. This practice is sub-optimal as it prevents one from fully utilizing potential correlations between graph topology and node attributes. To mitigate this issue, we propose a new self-supervised learning framework, Graph Information Aided Node feature exT… |
